<!DOCTYPE html>

<html>
<head>
<title>NIPS 2009 collected by Wang</title>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body>
<p id="section"><div id="pid">Paperid:<span id="pid">1</span></div><div id="author">Authors:<span id="author">Marc Lanctot, Kevin Waugh, Martin Zinkevich, Michael Bowling</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/00411460f7c92d2124a67ea0f4cb5f85-Paper.pdf">Monte Carlo Sampling for Regret Minimization in Extensive Games</a></div><div id="abs">Abstract: <span id="abs">Sequential decision-making with multiple agents and imperfect information is commonly modeled as an extensive game.  One efficient method for computing Nash equilibria in large, zero-sum, imperfect information games is counterfactual regret minimization (CFR). In the domain of poker, CFR has proven effective, particularly when using a domain-specific augmentation involving chance outcome sampling.  In this paper, we describe a general family of domain independent CFR sample-based algorithms called Monte Carlo counterfactual regret minimization (MCCFR) of which the original and poker-specific versions are special cases. We start by showing that MCCFR performs the same regret updates as CFR on expectation. Then, we introduce two sampling schemes: {\it outcome sampling} and {\it external sampling}, showing that both have bounded overall regret with high  probability. Thus, they can compute an approximate equilibrium using self-play. Finally, we prove a new tighter bound on the regret for the original CFR algorithm and relate this new bound to MCCFRs bounds. We show empirically that, although the sample-based algorithms require more iterations, their lower cost per iteration can lead to dramatically faster convergence in various games.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">101</span></div><div id="author">Authors:<span id="author">Daniel J. Hsu, Sham M. Kakade, John Langford, Tong Zhang</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/67974233917cea0e42a49a2fb7eb4cf4-Paper.pdf">Multi-Label Prediction via Compressed Sensing</a></div><div id="abs">Abstract: <span id="abs">We consider multi-label prediction problems with large output spaces under the assumption of output sparsity – that the target (label) vectors have small support. We develop a general theory for a variant of the popular error correcting output code scheme, using ideas from compressed sensing for exploiting this sparsity. The method can be regarded as a simple reduction from multi-label regression problems to binary regression problems. We show that the number of subprob- lems need only be logarithmic in the total number of possible labels, making this approach radically more efﬁcient than others. We also state and prove robustness guarantees for this method in the form of regret transform bounds (in general), and also provide a more detailed analysis for the linear prediction setting.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">45</span></div><div id="author">Authors:<span id="author">Pascal Germain, Alexandre Lacasse, Mario Marchand, Sara Shanian, François Laviolette</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf">From PAC-Bayes Bounds to KL Regularization</a></div><div id="abs">Abstract: <span id="abs">We show that convex KL-regularized objective functions are obtained from a PAC-Bayes risk bound when using convex loss functions for the stochastic Gibbs classifier that upper-bound the standard zero-one loss used for the weighted majority vote. By restricting ourselves to a class of posteriors, that we call quasi uniform, we propose a simple coordinate descent learning algorithm to minimize the proposed KL-regularized cost function. We show that standard ellp-regularized objective functions currently used, such as ridge regression and ellp-regularized boosting, are obtained from a relaxation of the KL divergence between the quasi uniform posterior and the uniform prior. We present numerical experiments where the proposed learning algorithm generally outperforms ridge regression and AdaBoost.</span></div>ables. Specifically, we assess the impact of different noise correlations structures on coding accuracy in long versus short decoding time windows. That is, for long time window we use the common Gaussian noise approximation. To address the case of short time windows we analyze the Ising model with identical noise correlation structure. In this way, we provide a new rigorous framework for assessing the functional consequences of noise correlation structures for the representational accuracy of neural population codes that is in particular applicable to short-time population coding.</p><p id="section"><div id="pid">Paperid:<span id="pid">48</span></div><div id="author">Authors:<span id="author">Jake Bouvrie, Lorenzo Rosasco, Tomaso Poggio</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/2ab56412b1163ee131e1246da0955bd1-Paper.pdf">On Invariance in Hierarchical Models</a></div><div id="abs">Abstract: <span id="abs">A goal of central importance in the study of hierarchical models for object recognition -- and indeed the visual cortex -- is that of understanding quantitatively the trade-off between invariance and selectivity, and how invariance and discrimination properties contribute towards providing an improved representation useful for learning from data. In this work we provide a general group-theoretic framework for characterizing and understanding invariance in a family of hierarchical models.  We show that by taking an algebraic perspective, one can provide a concise set of conditions which must be met to establish invariance, as well as a constructive prescription for meeting those conditions. Analyses in specific cases of particular relevance to computer vision and text processing are given, yielding insight into how and when invariance can be achieved. We find that the minimal sets of transformations intrinsic to the hierarchical model needed to support a particular invariance can be clearly described, thereby encouraging efficient computational implementations.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">7</span></div><div id="author">Authors:<span id="author">Yoshua Bengio, James S. Bergstra</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/043c3d7e489c69b48737cc0c92d0f3a2-Paper.pdf">Slow, Decorrelated Features for Pretraining Complex Cell-like Networks</a></div><div id="abs">Abstract: <span id="abs">We introduce a new type of neural network activation function based on recent physiological rate models for complex cells in visual area V1.  A single-hidden-layer neural network of this kind of model achieves 1.5% error on MNIST.  We also introduce an existing criterion for learning slow, decorrelated features as a pretraining strategy for image models.  This pretraining strategy results in orientation-selective features, similar to the receptive fields of complex cells.  With this pretraining, the same single-hidden-layer model achieves better generalization error, even though the pretraining sample distribution is very different from the fine-tuning distribution.  To implement this pretraining strategy, we derive a fast algorithm for online learning of decorrelated features such that each iteration of the algorithm runs in linear time with respect to the number of features.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">51</span></div><div id="author">Authors:<span id="author">Pietro Berkes, Ben White, Jozsef Fiser</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/2b24d495052a8ce66358eb576b8912c8-Paper.pdf">No evidence for active sparsification in the visual cortex</a></div><div id="abs">Abstract: <span id="abs">The proposal that cortical activity in the visual cortex is optimized for sparse neural activity is one of the most established ideas in computational neuroscience. However, direct experimental evidence for optimal sparse coding remains inconclusive, mostly due to the lack of reference values on which to judge the measured sparseness. Here we analyze neural responses to natural movies in the primary visual cortex of ferrets at different stages of development, and of rats while awake and under different levels of anesthesia. In contrast with prediction from a sparse coding model, our data shows that population and lifetime sparseness decrease with visual experience, and increase from the awake to anesthetized state. These results suggest that the representation in the primary visual cortex is not actively optimized to maximize sparseness.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">32</span></div><div id="author">Authors:<span id="author">Cosmin Bejan, Matthew Titsworth, Andrew Hickl, Sanda Harabagiu</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/185e65bc40581880c4f2c82958de8cfe-Paper.pdf">Nonparametric Bayesian Models for Unsupervised Event Coreference Resolution</a></div><div id="abs">Abstract: <span id="abs">We present a sequence of unsupervised, nonparametric Bayesian models for clustering  complex linguistic objects. In this approach, we consider a potentially infinite number of features and categorical outcomes. We evaluate these models for the task  of within- and cross-document event coreference on two corpora.  All the models we investigated show significant improvements when compared against an existing baseline for this task.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">59</span></div><div id="author">Authors:<span id="author">Samory Kpotufe</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/31b3b31a1c2f8a370206f111127c0dbd-Paper.pdf">Fast, smooth and adaptive regression in metric spaces</a></div><div id="abs">Abstract: <span id="abs">It was recently shown that certain nonparametric regressors can escape the curse of dimensionality in the sense that their convergence rates adapt to the intrinsic dimension of data (\cite{BL:65, SK:77}). We prove some stronger results in more general settings. In particular, we consider a regressor which, by combining aspects of both tree-based regression and kernel regression, operates on a general metric space, yields a smooth function, and evaluates in time $O(\log n)$. We derive a tight convergence rate of the form $n^{-2/(2+d)}$ where $d$ is the Assouad dimension of the input space.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">65</span></div><div id="author">Authors:<span id="author">Joel Veness, David Silver, Alan Blair, William Uther</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/389bc7bb1e1c2a5e7e147703232a88f6-Paper.pdf">Bootstrapping from Game Tree Search</a></div><div id="abs">Abstract: <span id="abs">In this paper we introduce a new algorithm for updating the parameters of a heuristic evaluation function, by updating the heuristic towards the values computed by an alpha-beta search. Our algorithm differs from previous approaches to learning from search, such as Samuels checkers player and the TD-Leaf algorithm, in two key ways. First, we update all nodes in the search tree, rather than a single node. Second, we use the outcome of a deep search, instead of the outcome of a subsequent search, as the training signal for the evaluation function. We implemented our algorithm in a chess program Meep, using a linear heuristic function. After initialising its weight vector to small random values, Meep was able to learn high quality weights from self-play alone. When tested online against human opponents, Meep played at a master level, the best performance of any chess program with a heuristic learned entirely from self-play.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">44</span></div><div id="author">Authors:<span id="author">Parikshit Ram, Dongryeol Lee, William March, Alexander G. Gray</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/2421fcb1263b9530df88f7f002e78ea5-Paper.pdf">Linear-time Algorithms for Pairwise Statistical Problems</a></div><div id="abs">Abstract: <span id="abs">Several key computational bottlenecks in machine learning involve pairwise distance computations, including all-nearest-neighbors (finding the nearest neighbor(s) for each point, e.g.  in manifold learning) and kernel summations (e.g. in kernel density estimation or kernel machines).  We consider the general, bichromatic case for these problems, in addition to the scientific problem of N-body potential calculation.  In this paper we show for the first time O(N) worst case runtimes for practical algorithms for these problems based on the cover tree data structure (Beygelzimer, Kakade, Langford, 2006).</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">102</span></div><div id="author">Authors:<span id="author">Kenji Fukumizu, Arthur Gretton, Gert R. Lanckriet, Bernhard Schölkopf, Bharath K. Sriperumbudur</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/685ac8cadc1be5ac98da9556bc1c8d9e-Paper.pdf">Kernel Choice and Classifiability for RKHS Embeddings of Probability Distributions</a></div><div id="abs">Abstract: <span id="abs">Embeddings of probability measures into reproducing kernel Hilbert spaces have been proposed as a straightforward and practical means of representing and comparing probabilities. In particular, the distance between embeddings (the maximum mean discrepancy, or MMD) has several key advantages over many classical metrics on distributions, namely easy computability, fast convergence and low bias of finite sample estimates. An important requirement of the embedding RKHS is that it be characteristic: in this case, the MMD between two distributions is zero if and only if the distributions coincide. Three new results on the MMD are introduced in the present study. First, it is established that MMD corresponds to the optimal risk of a kernel classifier, thus forming a natural link between the distance between distributions and their ease of classification. An important consequence is that a kernel must be characteristic to guarantee classifiability between distributions in the RKHS. Second, the class of characteristic kernels is broadened to incorporate all strictly positive definite kernels: these include non-translation invariant kernels and kernels on non-compact domains. Third, a generalization of the MMD is proposed for families of kernels, as the supremum over MMDs on a class of kernels (for instance the Gaussian kernels with different bandwidths). This extension is necessary to obtain a single distance measure if a large selection or class of characteristic kernels is potentially appropriate. This generalization is reasonable, given that it corresponds to the problem of learning the kernel by minimizing the risk of the corresponding kernel classifier. The generalized MMD is shown to have consistent finite sample estimates, and its performance is demonstrated on a homogeneity testing example.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">43</span></div><div id="author">Authors:<span id="author">Alekh Agarwal, Martin J. Wainwright, Peter L. Bartlett, Pradeep K. Ravikumar</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/2387337ba1e0b0249ba90f55b2ba2521-Paper.pdf">Information-theoretic lower bounds on the oracle complexity of convex optimization</a></div><div id="abs">Abstract: <span id="abs">Despite the large amount of literature on upper bounds on complexity of convex analysis, surprisingly little is known about the fundamental hardness of these problems. The extensive use of convex optimization in machine learning and statistics makes such an understanding critical to understand fundamental computational limits of learning and estimation. In this paper, we study the complexity of stochastic convex optimization in an oracle model of computation. We improve upon known results and obtain tight minimax complexity estimates for some function classes. We also discuss implications of these results to the understanding the inherent complexity of large-scale learning and estimation problems.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">69</span></div><div id="author">Authors:<span id="author">Samy Bengio, Fernando Pereira, Yoram Singer, Dennis Strelow</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/3b3dbaf68507998acd6a5a5254ab2d76-Paper.pdf">Group Sparse Coding</a></div><div id="abs">Abstract: <span id="abs">Bag-of-words document representations are often used in text, image and video processing. While it is relatively easy to determine a suitable word dictionary for text documents, there is no simple mapping from raw images or videos to dictionary terms. The classical approach builds a dictionary using vector quantization over a large set of useful visual descriptors extracted from a training set, and uses a nearest-neighbor algorithm to count the number of occurrences of each dictionary word in documents to be encoded. More robust approaches have been proposed recently that represent each visual descriptor as a sparse weighted combination of dictionary words. While favoring a sparse representation at the level of visual descriptors, those methods however do not ensure that images have sparse representation. In this work, we use mixed-norm regularization to achieve sparsity at the image level as well as a small overall dictionary. This approach can also be used to encourage using the same dictionary words for all the images in a class, providing a discriminative signal in the construction of image representations. Experimental results on a benchmark image classification dataset show that when compact image or dictionary representations are needed for computational efficiency, the proposed approach yields better mean average precision in classification.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">29</span></div><div id="author">Authors:<span id="author">Pierre-arnaud Coquelin, Romain Deguest, Rémi Munos</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/16a5cdae362b8d27a1d8f8c7b78b4330-Paper.pdf">Sensitivity analysis in HMMs with application to likelihood maximization</a></div><div id="abs">Abstract: <span id="abs">This paper considers a sensitivity analysis in Hidden Markov Models with continuous state and observation spaces. We propose an Infinitesimal Perturbation Analysis (IPA) on the filtering distribution with respect to some parameters of the model. We describe a methodology for using any algorithm that estimates the filtering density, such as Sequential Monte Carlo methods, to design an algorithm that estimates its gradient. The resulting IPA estimator is proven to be asymptotically unbiased, consistent and has computational complexity linear in the number of particles. We consider an application of this analysis to the problem of identifying unknown parameters of the model given a sequence of observations. We derive an IPA estimator for the gradient of the log-likelihood, which may be used in a gradient method for the purpose of likelihood maximization. We illustrate the method with several numerical experiments.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">24</span></div><div id="author">Authors:<span id="author">Jarno Vanhatalo, Pasi Jylänki, Aki Vehtari</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/13fe9d84310e77f13a6d184dbf1232f3-Paper.pdf">Gaussian process regression with Student-t likelihood</a></div><div id="abs">Abstract: <span id="abs">In the Gaussian process regression the observation model is commonly assumed to be Gaussian, which is convenient in computational perspective. However, the drawback is that the predictive accuracy of the model can be significantly compromised if the observations are contaminated by outliers. A robust observation model, such as the Student-t distribution, reduces the influence of outlying observations and improves the predictions. The problem, however, is the analytically intractable inference. In this work, we discuss the properties of a Gaussian process regression model with the Student-t likelihood and utilize the Laplace approximation for approximate inference. We compare our approach to a variational approximation and a Markov chain Monte Carlo scheme, which utilize the commonly used scale mixture representation of the Student-t distribution.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">18</span></div><div id="author">Authors:<span id="author">Hanna M. Wallach, David M. Mimno, Andrew McCallum</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/0d0871f0806eae32d30983b62252da50-Paper.pdf">Rethinking LDA: Why Priors Matter</a></div><div id="abs">Abstract: <span id="abs">Implementations of topic models typically use symmetric Dirichlet priors with fixed concentration parameters, with the implicit assumption that such smoothing parameters" have little practical effect. In this paper, we explore several classes of structured priors for topic models. We find that an asymmetric Dirichlet prior over the document-topic distributions has substantial advantages over a symmetric prior, while an asymmetric prior over the topic-word distributions provides no real benefit. Approximation of this prior structure through simple, efficient hyperparameter optimization steps is sufficient to achieve these performance gains. The prior structure we advocate substantially increases the robustness of topic models to variations in the number of topics and to the highly skewed word frequency distributions common in natural language. Since this prior structure can be implemented using efficient algorithms that add negligible cost beyond standard inference techniques, we recommend it as a new standard for topic modeling."</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">42</span></div><div id="author">Authors:<span id="author">Andrea Montanari, Jose A. Pereira</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/22fb0cee7e1f3bde58293de743871417-Paper.pdf">Which graphical models are difficult to learn?</a></div><div id="abs">Abstract: <span id="abs">We consider the problem of learning the structure of Ising models (pairwise binary Markov random fields) from i.i.d. samples. While several methods have been proposed to accomplish this task, their relative merits and limitations remain somewhat obscure. By analyzing a number of concrete examples, we show that low-complexity  algorithms systematically fail when the Markov random field  develops long-range correlations. More precisely, this phenomenon  appears to be related to the Ising model phase transition  (although it does not coincide with it).</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">10</span></div><div id="author">Authors:<span id="author">Vinayak Rao, Yee W. Teh</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/0537fb40a68c18da59a35c2bfe1ca554-Paper.pdf">Spatial Normalized Gamma Processes</a></div><div id="abs">Abstract: <span id="abs">Dependent Dirichlet processes (DPs) are dependent sets of random measures, each being marginally Dirichlet process distributed. They are used in Bayesian nonparametric models when the usual exchangebility assumption does not hold. We propose a simple and general framework to construct dependent DPs by marginalizing and normalizing a single gamma process over an extended space. The result is a set of DPs, each located at a point in a space such that  neighboring DPs are more dependent. We describe Markov chain Monte Carlo inference, involving the typical Gibbs sampling and three different Metropolis-Hastings proposals to speed up convergence. We report an empirical study of convergence speeds on a synthetic dataset and demonstrate an application of the model to topic modeling through time.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">98</span></div><div id="author">Authors:<span id="author">Yoram Singer, John C. Duchi</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/621bf66ddb7c962aa0d22ac97d69b793-Paper.pdf">Efficient Learning using Forward-Backward Splitting</a></div><div id="abs">Abstract: <span id="abs">We describe, analyze, and experiment with a new framework for empirical loss minimization with regularization. Our algorithmic framework alternates between two phases. On each iteration we first perform an {\em unconstrained} gradient descent step. We then cast and solve an instantaneous optimization problem that trades off minimization of a regularization term while keeping close proximity to the result of the first phase. This yields a simple yet effective algorithm for both batch penalized risk minimization and online learning. Furthermore, the two phase approach enables sparse solutions when used in conjunction with regularization functions that promote sparsity, such as $\ell_1$. We derive concrete and very simple algorithms for minimization of loss functions with $\ell_1$, $\ell_2$, $\ell_2^2$, and $\ell_\infty$ regularization. We also show how to construct efficient algorithms for mixed-norm $\ell_1/\ell_q$ regularization. We further extend the algorithms and give efficient implementations for very high-dimensional data with sparsity. We demonstrate the potential of the proposed framework in experiments with synthetic and natural datasets.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">22</span></div><div id="author">Authors:<span id="author">Shuai Huang, Jing Li, Liang Sun, Jun Liu, Teresa Wu, Kewei Chen, Adam Fleisher, Eric Reiman, Jieping Ye</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/1141938ba2c2b13f5505d7c424ebae5f-Paper.pdf">Learning Brain Connectivity of Alzheimer's Disease from Neuroimaging Data</a></div><div id="abs">Abstract: <span id="abs">Recent advances in neuroimaging techniques provide great potentials for effective diagnosis of Alzheimer’s disease (AD), the most common form of dementia. Previous studies have shown that AD is closely related to alternation in the functional brain network, i.e., the functional connectivity among different brain regions. In this paper, we consider the problem of learning functional brain connectivity from neuroimaging, which holds great promise for identifying image-based markers used to distinguish Normal Controls (NC), patients with Mild Cognitive Impairment (MCI), and patients with AD.  More specifically, we study sparse inverse covariance estimation (SICE), also known as exploratory Gaussian graphical models, for brain connectivity modeling. In particular, we apply SICE to learn and analyze functional brain connectivity patterns from different subject groups, based on a key property of SICE, called the “monotone property” we established in this paper. Our experimental results on neuroimaging PET data of 42 AD, 116 MCI, and 67 NC subjects reveal several interesting connectivity patterns consistent with literature findings, and also some new patterns that can help the knowledge discovery of AD.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">70</span></div><div id="author">Authors:<span id="author">Chong Wang, David M. Blei</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/3b8a614226a953a8cd9526fca6fe9ba5-Paper.pdf">Decoupling Sparsity and Smoothness in the Discrete Hierarchical Dirichlet Process</a></div><div id="abs">Abstract: <span id="abs">We present a nonparametric hierarchical Bayesian model of document collections that decouples sparsity and smoothness in the component distributions (i.e., the ``topics). In the sparse topic model (STM), each topic is represented by a bank of selector variables that determine which terms appear in the topic. Thus each topic is associated with a subset of the vocabulary, and topic smoothness is modeled on this subset. We develop an efficient Gibbs sampler for the STM that includes a general-purpose method for sampling from a Dirichlet mixture with a combinatorial number of components. We demonstrate the STM on four real-world datasets. Compared to traditional approaches, the empirical results show that STMs give better predictive performance with simpler inferred models.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">57</span></div><div id="author">Authors:<span id="author">Roy Anati, Kostas Daniilidis</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf">Constructing Topological Maps using Markov Random Fields and Loop-Closure Detection</a></div><div id="abs">Abstract: <span id="abs">We present a system which constructs a topological map of an environment given a sequence of images. This system includes a novel image similarity score which uses dynamic programming to match images using both the appearance and relative positions of local features simultaneously. Additionally an MRF is constructed to model the probability of loop-closures. A locally optimal labeling is found using Loopy-BP. Finally we outline a method to generate a topological map from loop closure data. Results are presented on four urban sequences and one indoor sequence.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">73</span></div><div id="author">Authors:<span id="author">Nicolas Vayatis, Marine Depecker, Stéphan J. Clémençcon</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/40008b9a5380fcacce3976bf7c08af5b-Paper.pdf">AUC optimization and the two-sample problem</a></div><div id="abs">Abstract: <span id="abs">The purpose of the paper is to explore the connection between multivariate homogeneity tests and $\auc$ optimization. The latter problem has recently received much attention in the statistical learning literature. From the elementary observation that, in the two-sample problem setup, the null assumption corresponds to the situation where the area under the optimal ROC curve is equal to 1/2, we propose a two-stage testing method based on data splitting. A nearly optimal scoring function in the AUC sense is first learnt from one of the two half-samples. Data from the remaining half-sample are then projected onto the real line and eventually ranked according to the scoring function computed at the first stage. The last step amounts to performing a standard Mann-Whitney Wilcoxon  test in the one-dimensional framework. We show that the learning step of the procedure does not affect the consistency of the test as well as its properties in terms of power, provided the ranking produced is accurate enough in the AUC sense. The results of a numerical experiment are eventually displayed in order to show the efficiency of the method.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">99</span></div><div id="author">Authors:<span id="author">Arnak Dalalyan, Renaud Keriven</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/65ded5353c5ee48d0b7d48c591b8f430-Paper.pdf">$L_1$-Penalized Robust Estimation for a Class of Inverse Problems Arising in Multiview Geometry</a></div><div id="abs">Abstract: <span id="abs">We propose a new approach to the problem of robust estimation in multiview geometry. Inspired by recent  advances in the sparse recovery problem of statistics, our estimator is defined as a Bayesian maximum a posteriori  with multivariate Laplace prior on the vector describing the outliers. This leads to an estimator in which  the fidelity to the data is measured by the $L_\infty$-norm while the regularization is done by the $L_1$-norm.  The proposed procedure is fairly fast since the outlier removal is done by solving one linear program (LP).  An important difference compared to existing algorithms is that for our estimator it is not necessary  to specify neither the number nor the proportion of the outliers. The theoretical results, as well as  the numerical example reported in this work, confirm the efficiency of the proposed approach.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">62</span></div><div id="author">Authors:<span id="author">Jean Honorio, Dimitris Samaras, Nikos Paragios, Rita Goldstein, Luis E. Ortiz</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/37693cfc748049e45d87b8c7d8b9aacd-Paper.pdf">Sparse and Locally Constant Gaussian Graphical Models</a></div><div id="abs">Abstract: <span id="abs">Locality information is crucial in datasets where each variable corresponds to a measurement in a manifold (silhouettes, motion trajectories, 2D and 3D images). Although these datasets are typically under-sampled and high-dimensional, they often need to be represented with low-complexity statistical models, which are comprised of only the important probabilistic dependencies in the datasets. Most methods attempt to reduce model complexity by enforcing structure sparseness. However, sparseness cannot describe inherent regularities in the structure. Hence, in this paper we first propose a new class of Gaussian graphical models which, together with sparseness, imposes local constancy through ${\ell}_1$-norm penalization. Second, we propose an efficient algorithm which decomposes the strictly convex maximum likelihood estimation into a sequence of problems with closed form solutions. Through synthetic experiments, we evaluate the closeness of the recovered models to the ground truth. We also test the generalization performance of our method in a wide range of complex real-world datasets and demonstrate that it can capture useful structures such as the rotation and shrinking of a beating heart, motion correlations between body parts during walking and functional interactions of brain regions. Our method outperforms the state-of-the-art structure learning techniques for Gaussian graphical models both for small and large datasets.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">2</span></div><div id="author">Authors:<span id="author">Peilin Zhao, Steven C. Hoi, Rong Jin</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/013d407166ec4fa56eb1e1f8cbe183b9-Paper.pdf">DUOL: A Double Updating Approach for Online Learning</a></div><div id="abs">Abstract: <span id="abs">In most online learning algorithms, the weights assigned to the misclassified examples (or support vectors) remain unchanged during the entire learning process. This is clearly insufficient since when a new misclassified example is added to the pool of support vectors, we generally expect it to affect the weights for the existing support vectors. In this paper, we propose a new online learning method, termed Double Updating Online Learning", or "DUOL" for short. Instead of only assigning a fixed weight to the misclassified example received in current trial, the proposed online learning algorithm also tries to update the weight for one of the existing support vectors. We show that the mistake bound can be significantly improved by the proposed online learning method. Encouraging experimental results show that the proposed technique is in general considerably more effective than the state-of-the-art online learning algorithms."</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">85</span></div><div id="author">Authors:<span id="author">Arno Onken, Steffen Grünewälder, Klaus Obermayer</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/4ea06fbc83cdd0a06020c35d50e1e89a-Paper.pdf">Correlation Coefficients are Insufficient for Analyzing Spike Count Dependencies</a></div><div id="abs">Abstract: <span id="abs">The linear correlation coefficient is typically used to characterize and analyze dependencies of neural spike counts. Here, we show that the correlation coefficient is in general insufficient to characterize these dependencies. We construct two neuron spike count models with Poisson-like marginals and vary their dependence structure using copulas. To this end, we construct a copula that allows to keep the spike counts uncorrelated while varying their dependence strength. Moreover, we employ a network of leaky integrate-and-fire neurons to investigate whether weakly correlated spike counts with strong dependencies are likely to occur in real networks. We find that the entropy of uncorrelated but dependent spike count distributions can deviate from the corresponding distribution with independent components by more than 25% and that weakly correlated but strongly dependent spike counts are very likely to occur in biological networks. Finally, we introduce a test for deciding whether the dependence structure of distributions with Poisson-like marginals is well characterized by the linear correlation coefficient and verify it for different copula-based models.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">54</span></div><div id="author">Authors:<span id="author">Kamalika Chaudhuri, Yoav Freund, Daniel J. Hsu</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/2cbca44843a864533ec05b321ae1f9d1-Paper.pdf">A Parameter-free Hedging Algorithm</a></div><div id="abs">Abstract: <span id="abs">We study the problem of decision-theoretic online learning (DTOL). Motivated by practical applications, we focus on DTOL when the number of actions is very large. Previous algorithms for learning in this framework have a tunable learning rate parameter, and a major barrier to using online-learning in practical applications is that it is not understood how to set this parameter optimally, particularly when the number of actions is large. In this paper, we offer a clean solution by proposing a novel and completely parameter-free algorithm for DTOL.   In addition, we introduce a new notion of regret, which is more natural for applications with a large number of actions. We show that our algorithm achieves good performance with respect to this new notion of regret; in addition, it also achieves performance close to that of the best bounds achieved by previous algorithms with optimally-tuned parameters, according to previous notions of regret.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">55</span></div><div id="author">Authors:<span id="author">Lei Wu, Rong Jin, Steven C. Hoi, Jianke Zhu, Nenghai Yu</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/2f2b265625d76a6704b08093c652fd79-Paper.pdf">Learning Bregman Distance Functions and Its Application for Semi-Supervised Clustering</a></div><div id="abs">Abstract: <span id="abs">Learning distance functions with side information plays a key role in many machine learning and data mining applications. Conventional approaches often assume a Mahalanobis distance function. These approaches are limited in two aspects: (i) they are computationally expensive (even infeasible) for high dimensional data because the size of the metric is in the square of dimensionality; (ii) they assume a fixed metric for the entire input space and therefore are unable to handle heterogeneous data. In this paper, we propose a novel scheme that learns nonlinear Bregman distance functions from side information using a non-parametric approach that is similar to support vector machines. The proposed scheme avoids the assumption of fixed metric because its local distance metric is implicitly derived from the Hessian matrix of a convex function that is used to generate the Bregman distance function. We present an efficient learning algorithm for the proposed scheme for distance function learning. The extensive experiments with semi-supervised clustering show the proposed technique (i) outperforms the state-of-the-art approaches for distance function learning, and (ii) is computationally efficient for high dimensional data.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">5</span></div><div id="author">Authors:<span id="author">Joseph Schlecht, Kobus Barnard</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/024d7f84fff11dd7e8d9c510137a2381-Paper.pdf">Learning models of object structure</a></div><div id="abs">Abstract: <span id="abs">We present an approach for learning stochastic geometric models of object categories from single view images. We focus here on models expressible as a spatially contiguous assemblage of blocks. Model topologies are learned across groups of images, and one or more such topologies is linked to an object category (e.g.  chairs). Fitting learned topologies to an image can be used to identify the object class, as well as detail its geometry. The latter goes beyond labeling objects, as it provides the geometric structure of particular instances.  We learn the models using joint statistical inference over structure parameters, camera parameters, and instance parameters. These produce an image likelihood through a statistical imaging model. We use trans-dimensional sampling to explore topology hypotheses, and alternate between Metropolis-Hastings and stochastic dynamics to explore instance parameters. Experiments on images of furniture objects such as tables and chairs suggest that this is an effective approach for learning models that encode simple representations of category geometry and the statistics thereof, and support inferring both category and geometry on held out single view images.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">38</span></div><div id="author">Authors:<span id="author">Finale Doshi-velez, Shakir Mohamed, Zoubin Ghahramani, David A. Knowles</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/1f50893f80d6830d62765ffad7721742-Paper.pdf">Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process</a></div><div id="abs">Abstract: <span id="abs">Nonparametric Bayesian models provide a framework for flexible probabilistic modelling of complex datasets. Unfortunately, Bayesian inference methods often require high-dimensional averages and can be slow to compute, especially with the potentially unbounded representations associated with nonparametric models. We address the challenge of scaling nonparametric Bayesian inference to the increasingly large datasets found in real-world applications, focusing on the case of parallelising inference in the Indian Buffet Process (IBP).  Our approach divides a large data set between multiple processors.  The processors use message passing to compute likelihoods in an asynchronous, distributed fashion and to propagate statistics about the global Bayesian posterior.  This novel MCMC sampler is the first parallel inference scheme for IBP-based models, scaling to datasets orders of magnitude larger than had previously been possible.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">100</span></div><div id="author">Authors:<span id="author">Brian Kulis, Trevor Darrell</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/6602294be910b1e3c4571bd98c4d5484-Paper.pdf">Learning to Hash with Binary Reconstructive Embeddings</a></div><div id="abs">Abstract: <span id="abs">Fast retrieval methods are increasingly critical for many large-scale analysis tasks, and there have been several recent methods that attempt to learn hash functions for fast and accurate nearest neighbor searches.  In this paper, we develop an algorithm for learning hash functions based on explicitly minimizing the reconstruction error between the original distances and the Hamming distances of the corresponding binary embeddings.  We develop a scalable coordinate-descent algorithm for our proposed hashing objective that is able to efficiently learn hash functions in a variety of settings.  Unlike existing methods such as semantic hashing and spectral hashing, our method is easily kernelized and does not require restrictive assumptions about the underlying distribution of the data.  We present results over several domains to demonstrate that our method outperforms existing state-of-the-art techniques.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">67</span></div><div id="author">Authors:<span id="author">Tomoharu Iwata, Takeshi Yamada, Naonori Ueda</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/39059724f73a9969845dfe4146c5660e-Paper.pdf">Modeling Social Annotation Data with Content Relevance using a Topic Model</a></div><div id="abs">Abstract: <span id="abs">We propose a probabilistic topic model for analyzing and extracting content-related annotations from noisy annotated discrete data such as web pages stored in social bookmarking services. In these services, since users can attach annotations freely, some annotations do not describe the semantics of the content, thus they are noisy,  i.e.  not content-related. The extraction of content-related annotations can be used as a preprocessing step in machine learning tasks such as text classification and image recognition, or can improve information retrieval performance. The proposed model is a generative model for content and annotations, in which the annotations are assumed to originate either from topics that generated the content or from a general distribution unrelated to the content. We demonstrate the effectiveness of the proposed method by using synthetic data and real social annotation data for text and images.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">4</span></div><div id="author">Authors:<span id="author">Marco Cuturi, Jean-philippe Vert, Alexandre D'aspremont</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/0188e8b8b014829e2fa0f430f0a95961-Paper.pdf">White Functionals for Anomaly Detection in Dynamical Systems</a></div><div id="abs">Abstract: <span id="abs">We propose new methodologies to detect anomalies in discrete-time processes taking values in a set. The method is based on the inference of functionals whose evaluations on successive states visited by the process have low autocorrelations. Deviations from this behavior are used to flag anomalies. The candidate functionals are estimated in a subset of a reproducing kernel Hilbert space associated with the set where the process takes values. We provide experimental results which show that these techniques compare favorably with other algorithms.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">14</span></div><div id="author">Authors:<span id="author">Piyush Rai, Hal Daume</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/09c6c3783b4a70054da74f2538ed47c6-Paper.pdf">Multi-Label Prediction via Sparse Infinite CCA</a></div><div id="abs">Abstract: <span id="abs">Canonical Correlation Analysis (CCA) is a useful technique for modeling dependencies between two (or more) sets of variables. Building upon the recently suggested probabilistic interpretation of CCA, we propose a nonparametric, fully Bayesian framework that can automatically select the number of correlation components, and effectively capture the sparsity underlying the projections. In addition, given (partially) labeled data, our algorithm can also be used as a (semi)supervised dimensionality reduction technique, and can be applied to learn useful predictive features in the context of learning a set of related tasks. Experimental results demonstrate the efficacy of the proposed approach for both CCA as a stand-alone problem, and when applied to multi-label prediction.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">46</span></div><div id="author">Authors:<span id="author">Elad Hazan, Satyen Kale</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/26337353b7962f533d78c762373b3318-Paper.pdf">On Stochastic and Worst-case Models for Investing</a></div><div id="abs">Abstract: <span id="abs">In practice, most investing is done assuming a probabilistic model of stock price returns known as the Geometric Brownian Motion (GBM). While it is often an acceptable approximation, the GBM model is not always valid empirically. This motivates a worst-case approach to investing, called universal portfolio management, where the objective is to maximize wealth relative to the wealth earned by the best fixed portfolio in hindsight.  In this paper we tie the two approaches, and design an investment strategy which is universal in the worst-case, and yet capable of exploiting the mostly valid GBM model. Our method is based on new and improved regret bounds for online convex optimization with exp-concave loss functions.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">21</span></div><div id="author">Authors:<span id="author">Marco Grzegorczyk, Dirk Husmeier</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/0ff39bbbf981ac0151d340c9aa40e63e-Paper.pdf">Non-stationary continuous dynamic Bayesian networks</a></div><div id="abs">Abstract: <span id="abs">Dynamic Bayesian networks have been applied widely to reconstruct the structure of regulatory processes from time series data. The standard approach is based on the assumption of a homogeneous Markov chain, which is not valid in many real-world scenarios. Recent research efforts addressing this shortcoming have considered undirected graphs, directed graphs for discretized data, or over-flexible models that lack any information sharing between time series segments. In the present article, we propose a non-stationary dynamic Bayesian network for continuous data, in which parameters are allowed to vary between segments, and in which a common network structure provides essential information sharing across segments.  Our model is based on a Bayesian change-point process, and we apply a variant of the allocation sampler of Nobile and Fearnside to infer the number and location of the change-points.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">28</span></div><div id="author">Authors:<span id="author">Rob Fergus, Yair Weiss, Antonio Torralba</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/1651cf0d2f737d7adeab84d339dbabd3-Paper.pdf">Semi-Supervised Learning in Gigantic Image Collections</a></div><div id="abs">Abstract: <span id="abs">With the advent of the Internet it is now possible to collect   hundreds of millions of images. These images come with varying   degrees of label information.Clean labels can be manually obtained on a   small fraction,noisy labels may be extracted automatically from  surrounding text, while for most images there are no labels at all. Semi-supervised learning is a principled framework for combining these different label sources. However, it scales polynomially with the  number of images, making it impractical for use on gigantic  collections with hundreds of millions of images and thousands of classes.   In this paper we show how to utilize recent results in machine   learning to obtain highly efficient approximations for   semi-supervised learning that are linear in the number of images.  Specifically, we use the convergence of the eigenvectors of the normalized graph Laplacian to eigenfunctions of weighted Laplace-Beltrami operators. We combine this with a label sharing framework obtained from Wordnet to propagate label information to classes lacking manual annotations. Our algorithm enables us to apply semi-supervised learning to a  database of 80 million images with 74 thousand classes.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">96</span></div><div id="author">Authors:<span id="author">Miguel Lázaro-Gredilla, Aníbal Figueiras-Vidal</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/5ea1649a31336092c05438df996a3e59-Paper.pdf">Inter-domain Gaussian Processes for Sparse Inference using Inducing Features</a></div><div id="abs">Abstract: <span id="abs">We present a general inference framework for inter-domain Gaussian Processes (GPs), focusing on its usefulness to build sparse GP models. The state-of-the-art sparse GP model introduced by Snelson and Ghahramani in [1] relies on finding a small, representative pseudo data set of m elements (from the same domain as the n available data elements) which is able to explain existing data well, and then uses it to perform inference. This reduces inference and model selection computation time from O(n^3) to O(m^2n), where m &lt;&lt; n. Inter-domain GPs can be used to find a (possibly more compact) representative set of features lying in a different domain, at the same computational cost. Being able to specify a different domain for the representative features allows to incorporate prior knowledge about relevant characteristics of data and detaches the functional form of the covariance and basis functions. We will show how previously existing models fit into this framework and will use it to develop two new sparse GP models. Tests on large, representative regression data sets suggest that significant improvement can be achieved, while retaining computational efficiency.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">68</span></div><div id="author">Authors:<span id="author">Hamid R. Maei, Csaba Szepesvári, Shalabh Bhatnagar, Doina Precup, David Silver, Richard S. Sutton</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/3a15c7d0bbe60300a39f76f8a5ba6896-Paper.pdf">Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation</a></div><div id="abs">Abstract: <span id="abs">We introduce the first  temporal-difference  learning algorithms that converge  with smooth value function approximators, such as neural networks. Conventional temporal-difference (TD) methods, such as TD($\lambda$), Q-learning and Sarsa have been used successfully with function approximation in many applications.  However, it is well known that off-policy sampling, as well as nonlinear function approximation, can cause these algorithms to become unstable (i.e., the parameters of the approximator may diverge). Sutton et al (2009a,b) solved the problem of off-policy learning with linear TD algorithms by introducing a new objective function, related to the Bellman-error, and algorithms that perform stochastic gradient-descent on this function. In this paper, we generalize their work to nonlinear function approximation. We present a Bellman error objective function and two gradient-descent TD algorithms that optimize it. We prove the  asymptotic almost-sure convergence  of both algorithms for any finite Markov decision process and any smooth value function approximator, under usual stochastic approximation conditions. The computational complexity per iteration scales linearly with the number of parameters of the approximator. The algorithms are incremental and are guaranteed to converge to locally optimal solutions.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">33</span></div><div id="author">Authors:<span id="author">Peter Carbonetto, Matthew King, Firas Hamze</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/19ca14e7ea6328a42e0eb13d585e4c22-Paper.pdf">A Stochastic approximation method for inference in probabilistic graphical models</a></div><div id="abs">Abstract: <span id="abs">We describe a new algorithmic framework for inference in probabilistic models, and apply it to inference for latent Dirichlet allocation. Our framework adopts the methodology of variational inference, but unlike existing variational methods such as mean field and expectation propagation it is not restricted to tractable classes of approximating distributions. Our approach can also be viewed as a sequential Monte Carlo (SMC) method, but unlike existing SMC methods there is no need to design the artificial sequence of distributions. Notably, our framework offers a principled means to exchange the variance of an importance sampling estimate for the bias incurred through variational approximation. Experiments on a challenging inference problem in population genetics demonstrate improvements in stability and accuracy over existing methods, and at a comparable cost.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">40</span></div><div id="author">Authors:<span id="author">Kate Saenko, Trevor Darrell</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/208e43f0e45c4c78cafadb83d2888cb6-Paper.pdf">Filtering Abstract Senses From Image Search Results</a></div><div id="abs">Abstract: <span id="abs">We propose an unsupervised method that, given a word, automatically selects non-abstract senses of that word from an online ontology and generates images depicting the corresponding entities. When faced with the task of learning a visual model based only on the name of an object, a common approach is to find images on the web that are associated with the object name, and then train a visual classifier from the search result. As words are generally polysemous, this approach can lead to relatively noisy models if many examples due to outlier senses are added to the model. We argue that images associated with an abstract word sense should be excluded when training a visual classifier to learn a model of a physical object. While image clustering can group together visually coherent sets of returned images, it can be difficult to distinguish whether an image cluster relates to a desired object or to an abstract sense of the word. We propose a method that uses both image features and the text associated with the images to relate latent topics to particular senses. Our model does not require any human supervision, and takes as input only the name of an object category. We show results of retrieving concrete-sense images in two available multimodal, multi-sense databases, as well as experiment with object classifiers trained on concrete-sense images returned by our method for a set of ten common office objects.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">17</span></div><div id="author">Authors:<span id="author">Han Liu, Xi Chen</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/0cb929eae7a499e50248a3a78f7acfc7-Paper.pdf">Nonparametric Greedy Algorithms for the Sparse Learning Problem</a></div><div id="abs">Abstract: <span id="abs">This paper studies the forward greedy strategy in sparse nonparametric regression. For additive models, we propose an algorithm called additive forward regression; for general multivariate regression, we propose an algorithm called generalized forward regression. Both of them simultaneously conduct estimation and variable selection in nonparametric settings for the high dimensional sparse learning problem. Our main emphasis is empirical: on both simulated and real data, these two simple greedy methods can clearly outperform several state-of-the-art competitors, including the LASSO, a nonparametric version of the LASSO called the sparse additive model (SpAM) and a recently proposed adaptive parametric forward-backward algorithm called the Foba. Some theoretical justifications are also provided.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">76</span></div><div id="author">Authors:<span id="author">Percy Liang, Guillaume Bouchard, Francis R. Bach, Michael I. Jordan</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/43fa7f58b7eac7ac872209342e62e8f1-Paper.pdf">Asymptotically Optimal Regularization in Smooth Parametric Models</a></div><div id="abs">Abstract: <span id="abs">Many types of regularization schemes have been employed in statistical learning, each one motivated by some assumption about the problem domain.  In this paper, we present a unified asymptotic analysis of smooth regularizers, which allows us to see how the validity of these assumptions impacts the success of a particular regularizer.  In addition, our analysis motivates an algorithm for optimizing regularization parameters, which in turn can be analyzed within our framework.  We apply our analysis to several examples, including hybrid generative-discriminative learning and multi-task learning.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">92</span></div><div id="author">Authors:<span id="author">Youngmin Cho, Lawrence K. Saul</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/5751ec3e9a4feab575962e78e006250d-Paper.pdf">Kernel Methods for Deep Learning</a></div><div id="abs">Abstract: <span id="abs">We introduce a new family of positive-definite kernel functions that mimic the computation in large, multilayer neural nets.  These kernel functions can be used in shallow architectures, such as support vector machines (SVMs), or in deep kernel-based architectures that we call multilayer kernel machines (MKMs).  We evaluate SVMs and MKMs with these kernel functions on problems designed to illustrate the advantages of deep architectures.  On several problems, we obtain better results than previous, leading benchmarks from both SVMs with Gaussian kernels as well as deep belief nets.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">75</span></div><div id="author">Authors:<span id="author">Kurt Miller, Michael I. Jordan, Thomas L. Griffiths</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/437d7d1d97917cd627a34a6a0fb41136-Paper.pdf">Nonparametric Latent Feature Models for Link Prediction</a></div><div id="abs">Abstract: <span id="abs">As the availability and importance of relational data -- such as the friendships summarized on a social networking website -- increases, it becomes increasingly important to have good models for such data.  The kinds of latent structure that have been considered for use in predicting links in such networks have been relatively limited. In particular, the machine learning community has focused on latent class models, adapting nonparametric Bayesian methods to jointly infer how many latent classes there are while learning which entities belong to each class. We pursue a similar approach with a richer kind of latent variable -- latent features -- using a nonparametric Bayesian technique to simultaneously infer the number of features at the same time we learn which entities have each feature. The greater expressiveness of this approach allows us to improve link prediction on three datasets.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">88</span></div><div id="author">Authors:<span id="author">Matthew Wilder, Matt Jones, Michael Mozer</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/522a9ae9a99880d39e5daec35375e999-Paper.pdf">Sequential effects reflect parallel learning of multiple environmental regularities</a></div><div id="abs">Abstract: <span id="abs">Across a wide range of cognitive tasks, recent experience inﬂuences behavior. For example, when individuals repeatedly perform a simple two-alternative forced-choice task (2AFC), response latencies vary dramatically based on the immediately preceding trial sequence. These sequential effects have been interpreted as adaptation to the statistical structure of an uncertain, changing environment (e.g. Jones &amp; Sieck, 2003; Mozer, Kinoshita, &amp; Shettel, 2007; Yu &amp; Cohen, 2008). The Dynamic Belief Model (DBM) (Yu &amp; Cohen, 2008) explains sequential effects in 2AFC tasks as a rational consequence of a dynamic internal representation that tracks second-order statistics of the trial sequence (repetition rates) and predicts whether the upcoming trial will be a repetition or an alternation of the previous trial. Experimental results suggest that ﬁrst-order statistics (base rates) also inﬂuence sequential effects. We propose a model that learns both ﬁrst- and second-order sequence properties, each according to the basic principles of the DBM but under a uniﬁed inferential framework. This model, the Dynamic Belief Mixture Model (DBM2), obtains precise, parsimonious ﬁts to data. Furthermore, the model predicts dissociations in behavioral (Maloney, Dal Martello, Sahm, &amp; Spillmann, 2005) and electrophysiological studies (Jentzsch &amp; Sommer, 2002), supporting the psychological and neurobiological reality of its two components.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">71</span></div><div id="author">Authors:<span id="author">Dilip Krishnan, Rob Fergus</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/3dd48ab31d016ffcbf3314df2b3cb9ce-Paper.pdf">Fast Image Deconvolution using Hyper-Laplacian Priors</a></div><div id="abs">Abstract: <span id="abs">The heavy-tailed distribution of gradients in natural scenes have proven effective priors for a range of problems such as denoising, deblurring and super-resolution. However, the use of sparse distributions makes the problem non-convex and impractically slow to solve for multi-megapixel images. In this paper we describe a deconvolution approach that is several orders of magnitude faster than existing techniques that use hyper-Laplacian priors. We adopt an alternating minimization scheme where one of the two phases is a non-convex problem that is separable over pixels. This per-pixel sub-problem may be solved with a lookup table (LUT). Alternatively, for two specific values of α, 1/2 and 2/3 an analytic solution can be found, by finding the roots of a cubic and quartic polynomial, respectively. Our approach (using either LUTs or analytic formulae) is able to deconvolve a 1 megapixel image in less than ∼3 seconds, achieving comparable quality to existing methods such as iteratively reweighted least squares (IRLS) that take ∼20 minutes. Furthermore, our method is quite general and can easily be extended to related image processing problems, beyond the deconvolution application demonstrated.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">35</span></div><div id="author">Authors:<span id="author">David P. Wipf, Srikantan S. Nagarajan</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/1ce927f875864094e3906a4a0b5ece68-Paper.pdf">Sparse Estimation Using General Likelihoods and Non-Factorial Priors</a></div><div id="abs">Abstract: <span id="abs">Finding maximally sparse representations from overcomplete feature dictionaries frequently involves minimizing a cost function composed of a likelihood (or data fit) term and a prior (or penalty function) that favors sparsity.  While typically the prior is factorial, here we examine non-factorial alternatives that have a number of desirable properties relevant to sparse estimation and are easily implemented using an efficient, globally-convergent reweighted $\ell_1$ minimization procedure.  The first method under consideration arises from the sparse Bayesian learning (SBL) framework.  Although based on a highly non-convex underlying cost function, in the context of canonical sparse estimation problems, we prove uniform superiority of this method over the Lasso in that, (i) it can never do worse, and (ii) for any dictionary and sparsity profile, there will always exist cases where it does better.  These results challenge the prevailing reliance on strictly convex penalty functions for finding sparse solutions.  We then derive a new non-factorial variant with similar properties that exhibits further performance improvements in empirical tests.  For both of these methods, as well as traditional factorial analogs, we demonstrate the effectiveness of reweighted $\ell_1$-norm algorithms in handling more general sparse estimation problems involving classification, group feature selection, and non-negativity constraints.  As a byproduct of this development, a rigorous reformulation of sparse Bayesian classification (e.g., the relevance vector machine) is derived that, unlike the original, involves no approximation steps and descends a well-defined objective function.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">93</span></div><div id="author">Authors:<span id="author">Romain Brasselet, Roland Johansson, Angelo Arleo</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/57aeee35c98205091e18d1140e9f38cf-Paper.pdf">Optimal context separation of spiking haptic signals by second-order somatosensory neurons</a></div><div id="abs">Abstract: <span id="abs">We study an encoding/decoding mechanism accounting for the relative spike timing of the signals propagating from peripheral nerve fibers to second-order somatosensory neurons in the cuneate nucleus (CN). The CN is modeled as a population of spiking neurons receiving as inputs the spatiotemporal responses of real mechanoreceptors obtained via microneurography recordings in humans. The efficiency of the haptic discrimination process is quantified by a novel definition of entropy that takes into full account the metrical properties of the spike train space. This measure proves to be a suitable decoding scheme for generalizing the classical Shannon entropy to spike-based neural codes. It permits an assessment of neurotransmission in the presence of a large output space (i.e. hundreds of spike trains) with 1 ms temporal precision. It is shown that the CN population code performs a complete discrimination of 81 distinct stimuli already within 35 ms of the first afferent spike, whereas a partial discrimination (80% of the maximum information transmission) is possible as rapidly as 15 ms. This study suggests that the CN may not constitute a mere synaptic relay along the somatosensory pathway but, rather, it may convey optimal contextual accounts (in terms of fast and reliable information transfer) of peripheral tactile inputs to downstream structures of the central nervous system.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">84</span></div><div id="author">Authors:<span id="author">Francois Caron, Arnaud Doucet</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/4e4b5fbbbb602b6d35bea8460aa8f8e5-Paper.pdf">Bayesian Nonparametric Models on Decomposable Graphs</a></div><div id="abs">Abstract: <span id="abs">Over recent years Dirichlet processes and the associated Chinese restaurant process (CRP) have found many applications in clustering while the Indian buffet process (IBP) is increasingly used to describe latent feature models. In the clustering case, we associate to each data point a latent allocation variable. These latent variables can share the same value and this induces a partition of the data set. The CRP is a prior distribution on such partitions.  In latent feature models, we associate to each data point a potentially infinite number of binary latent variables indicating the possession of some features and the IBP is a prior distribution on the associated infinite binary matrix. These prior distributions are attractive because they ensure exchangeability (over samples). We propose here extensions of these models to decomposable graphs. These models have appealing properties and can be easily learned using Monte Carlo techniques.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">52</span></div><div id="author">Authors:<span id="author">Novi Quadrianto, James Petterson, Alex J. Smola</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/2bb232c0b13c774965ef8558f0fbd615-Paper.pdf">Distribution Matching for Transduction</a></div><div id="abs">Abstract: <span id="abs">Many transductive inference algorithms assume that distributions over training and test estimates should be related, e.g. by providing a large margin of separation on both sets. We use this idea to design a transduction algorithm which can be used without modification for classification, regression, and structured estimation. At its heart we exploit the fact that for a good learner the distributions over the outputs on training and test sets should match. This is a classical two-sample problem which can be solved efficiently in its most general form by using distance measures in Hilbert Space. It turns out that a number of existing heuristics can be viewed as special cases of our approach.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">39</span></div><div id="author">Authors:<span id="author">Raghu Meka, Prateek Jain, Inderjit S. Dhillon</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/1fc214004c9481e4c8073e85323bfd4b-Paper.pdf">Matrix Completion from Power-Law Distributed Samples</a></div><div id="abs">Abstract: <span id="abs">The low-rank matrix completion problem is a fundamental problem with many important applications. Recently, Candes &amp; Recht, Keshavan et al. and Candes &amp; Tao obtained the first non-trivial theoretical results for the problem assuming that the observed entries are sampled uniformly at random. Unfortunately, most real-world datasets do not satisfy this assumption, but instead exhibit power-law distributed samples. In this paper, we propose a graph theoretic approach to matrix completion that solves the problem for more realistic sampling models. Our method is easier to analyze than previous methods with the analysis reducing to computing the threshold for complete cascades in random graphs, a problem of independent interest. By analyzing the graph theoretic problem, we show that our method achieves exact recovery when the observed entries are sampled from the Chung-Lu-Vu model, which can generate power-law distributed graphs. We also hypothesize that our algorithm solves the matrix completion problem from an optimal number of entries for the popular preferential attachment model and provide strong empirical evidence for the claim. Furthermore, our method is easier to implement and is substantially faster than existing methods. We demonstrate the effectiveness of our method on examples when the low-rank matrix is sampled according to the prevalent random graph models for complex networks and also on the Netflix challenge dataset.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">49</span></div><div id="author">Authors:<span id="author">Arkadas Ozakin, Alexander G. Gray</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/2ac2406e835bd49c70469acae337d292-Paper.pdf">Submanifold density estimation</a></div><div id="abs">Abstract: <span id="abs">Kernel density estimation is the most widely-used practical method for accurate nonparametric density estimation. However, long-standing worst-case theoretical results showing that its performance worsens exponentially with the dimension of the data have quashed its application to modern high-dimensional datasets for decades. In practice, it has been recognized that often such data have a much lower-dimensional intrinsic structure. We propose a small modification to kernel density estimation for estimating probability density functions on Riemannian submanifolds of Euclidean space. Using ideas from Riemannian geometry, we prove the consistency of this modified estimator and show that the convergence rate is determined by the intrinsic dimension of the submanifold. We conclude with empirical results demonstrating the behavior predicted by our theory.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">77</span></div><div id="author">Authors:<span id="author">Jaakko Luttinen, Alexander Ilin</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/4a47d2983c8bd392b120b627e0e1cab4-Paper.pdf">Variational Gaussian-process factor analysis for modeling spatio-temporal data</a></div><div id="abs">Abstract: <span id="abs">We present a probabilistic latent factor model which can be used for studying spatio-temporal datasets. The spatial and temporal structure is modeled by using Gaussian process priors both for the loading matrix and the factors. The posterior distributions are approximated using the variational Bayesian framework. High computational cost of Gaussian process modeling is reduced by using sparse approximations. The model is used to compute the reconstructions of the global sea surface temperatures from a historical dataset. The results suggest that the proposed model can outperform the state-of-the-art reconstruction systems.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">90</span></div><div id="author">Authors:<span id="author">Liam M. Dermed, Charles L. Isbell</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/5680522b8e2bb01943234bce7bf84534-Paper.pdf">Solving Stochastic Games</a></div><div id="abs">Abstract: <span id="abs">Solving multi-agent reinforcement learning problems has proven difficult because of the lack of tractable algorithms.  We provide the first approximation algorithm which solves stochastic games to within $\epsilon$ relative error of the optimal game-theoretic solution, in time polynomial in $1/\epsilon$. Our algorithm extends Murrays and Gordon’s (2007) modified Bellman equation which determines the \emph{set} of all possible achievable utilities; this provides us a truly general framework for multi-agent learning. Further, we empirically validate our algorithm and find the computational cost to be orders of magnitude less than what the theory predicts.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">58</span></div><div id="author">Authors:<span id="author">Geoffrey E. Hinton, Ruslan Salakhutdinov</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/31839b036f63806cba3f47b93af8ccb5-Paper.pdf">Replicated Softmax: an Undirected Topic Model</a></div><div id="abs">Abstract: <span id="abs">We show how to model documents as bags of words using family of two-layer, undirected graphical models. Each member of the family has the same number of binary hidden units but a different number of ``softmax visible units. All of the softmax units in all of the models in the family share the same weights to the binary hidden units. We describe efficient inference and learning procedures for such a family. Each member of the family models the probability distribution of documents of a specific length as a product of topic-specific distributions rather than as a mixture and this gives much better generalization than Latent Dirichlet Allocation for modeling the log probabilities of held-out documents. The low-dimensional topic vectors learned by the undirected family are also much better than LDA topic vectors for retrieving documents that are similar to a query document. The learned topics are more general than those found by LDA because precision is achieved by intersecting many general topics rather than by selecting a single precise topic to generate each word.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">80</span></div><div id="author">Authors:<span id="author">Mark Steyvers, Brent Miller, Pernille Hemmer, Michael D. Lee</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/4c27cea8526af8cfee3be5e183ac9605-Paper.pdf">The Wisdom of Crowds in the Recollection of Order Information</a></div><div id="abs">Abstract: <span id="abs">When individuals independently recollect events or retrieve facts from memory, how can we aggregate these retrieved memories to reconstruct the actual set of events or facts? In this research, we report the performance of individuals in a series of general knowledge tasks, where the goal is to reconstruct from memory the order of historic events, or the order of items along some physical dimension. We introduce two Bayesian models for aggregating order information based on a Thurstonian approach and Mallows model. Both models assume that each individuals reconstruction is based on either a random permutation of the unobserved ground truth, or by a pure guessing strategy. We apply MCMC to make inferences about the underlying truth and the strategies employed by individuals. The models demonstrate a wisdom of crowds" effect, where the aggregated orderings are closer to the true ordering than the orderings of the best individual."</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">72</span></div><div id="author">Authors:<span id="author">Emanuel Todorov</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/3eb71f6293a2a31f3569e10af6552658-Paper.pdf">Compositionality of optimal control laws</a></div><div id="abs">Abstract: <span id="abs">We present a theory of compositionality in stochastic optimal control, showing how task-optimal controllers can be constructed from certain primitives. The primitives are themselves feedback controllers pursuing their own agendas. They are mixed in proportion to how much progress they are making towards their agendas and how compatible their agendas are with the present task. The resulting composite control law is provably optimal when the problem belongs to a certain class. This class is rather general and yet has a number of unique properties - one of which is that the Bellman equation can be made linear even for non-linear or discrete dynamics. This gives rise to the compositionality developed here. In the special case of linear dynamics and Gaussian noise our framework yields analytical solutions (i.e. non-linear mixtures of linear-quadratic regulators) without requiring the final cost to be quadratic. More generally, a natural set of control primitives can be constructed by applying SVD to Greens function of the Bellman equation. We illustrate the theory in the context of human arm movements. The ideas of optimality and compositionality are both very prominent in the field of motor control, yet they are hard to reconcile. Our work makes this possible.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">78</span></div><div id="author">Authors:<span id="author">Eric Garcia, Maya Gupta</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/4b0250793549726d5c1ea3906726ebfe-Paper.pdf">Lattice Regression</a></div><div id="abs">Abstract: <span id="abs">We present a new empirical risk minimization framework for approximating functions from training samples for low-dimensional regression applications where a lattice (look-up table) is stored and interpolated at run-time for an efficient hardware implementation. Rather than evaluating a fitted function at the lattice nodes without regard to the fact that samples will be interpolated, the proposed lattice regression approach estimates the lattice to minimize the interpolation error on the given training samples. Experiments show that lattice regression can reduce mean test error  compared to Gaussian process regression for digital color management of printers, an application for which linearly interpolating a look-up table (LUT) is standard. Simulations confirm that lattice regression performs consistently better than the naive approach to learning the lattice, particularly when the density of training samples is low.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">94</span></div><div id="author">Authors:<span id="author">Xiaolin Yang, Seyoung Kim, Eric P. Xing</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/58c54802a9fb9526cd0923353a34a7ae-Paper.pdf">Heterogeneous multitask learning with joint sparsity constraints</a></div><div id="abs">Abstract: <span id="abs">Multitask learning addressed the problem of learning related tasks whose information can be shared each other. Traditional problem usually deal with homogeneous tasks such as regression, classification individually. In this paper we consider the problem learning multiple related tasks where tasks consist of both continuous and discrete outputs from a common set of input variables that lie in a high-dimensional space. All of the tasks are related in the sense that they share the same set of relevant input variables, but the amount of influence of each input on different outputs may vary. We formulate this problem as a combination of linear regression and logistic regression and model the joint sparsity as L1/Linf and L1/L2-norm of the model parameters. Among several possible applications, our approach addresses an important open problem in genetic association mapping, where we are interested in discovering genetic markers that influence multiple correlated traits jointly. In our experiments, we demonstrate our method in the scenario of association mapping, using simulated and asthma data, and show that the algorithm can effectively recover the relevant inputs with respect to all of the tasks.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">97</span></div><div id="author">Authors:<span id="author">Andrew Frank, Padhraic Smyth, Alexander T. Ihler</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/5ec91aac30eae62f4140325d09b9afd0-Paper.pdf">Particle-based Variational Inference for Continuous Systems</a></div><div id="abs">Abstract: <span id="abs">Since the development of loopy belief propagation, there has been considerable work on advancing the state of the art for approximate inference over distributions defined on discrete random variables. Improvements include guarantees of convergence, approximations that are provably more accurate, and bounds on the results of exact inference.  However, extending these methods  to continuous-valued systems has lagged behind.  While several methods have been developed to use belief propagation on systems with continuous values, they have not as yet incorporated the recent advances for discrete variables. In this context we extend a recently proposed particle-based belief propagation algorithm to provide a general framework for adapting discrete message-passing algorithms to perform inference in continuous systems.  The resulting algorithms behave similarly to their purely discrete counterparts, extending the benefits of these more advanced inference techniques to the continuous domain.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">86</span></div><div id="author">Authors:<span id="author">Nir Ailon, Ragesh Jaiswal, Claire Monteleoni</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/4f16c818875d9fcb6867c7bdc89be7eb-Paper.pdf">Streaming k-means approximation</a></div><div id="abs">Abstract: <span id="abs">We provide a clustering algorithm that approximately optimizes the k-means objective, in the one-pass streaming setting. We make no assumptions about the data, and  our algorithm is very light-weight in terms of memory, and computation. This setting is applicable to unsupervised learning on massive data sets, or resource-constrained devices. The two main ingredients of our theoretical work are:  a derivation of an extremely simple pseudo-approximation batch algorithm for k-means, in which the algorithm is allowed to output more than k centers (based on the recent k-means++"), and a streaming clustering algorithm in which batch clustering algorithms are performed on small inputs (fitting in memory) and combined in a hierarchical manner.  Empirical evaluations on real and simulated data reveal the practical utility of our method."</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">60</span></div><div id="author">Authors:<span id="author">Alan Jern, Kai-min Chang, Charles Kemp</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/3435c378bb76d4357324dd7e69f3cd18-Paper.pdf">Bayesian Belief Polarization</a></div><div id="abs">Abstract: <span id="abs">Situations in which people with opposing prior beliefs observe the same evidence and then strengthen those existing beliefs are frequently offered as evidence of human irrationality. This phenomenon, termed belief polarization, is typically assumed to be non-normative. We demonstrate, however, that a variety of cases of belief polarization are consistent with a Bayesian approach to belief revision. Simulation results indicate that belief polarization is not only possible but relatively common within the class of Bayesian models that we consider.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">103</span></div><div id="author">Authors:<span id="author">Boaz Nadler, Nathan Srebro, Xueyuan Zhou</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/68ce199ec2c5517597ce0a4d89620f55-Paper.pdf">Statistical Analysis of Semi-Supervised Learning: The Limit of Infinite Unlabelled Data</a></div><div id="abs">Abstract: <span id="abs">We study the behavior of the popular Laplacian Regularization method for Semi-Supervised Learning at the regime of a fixed number of labeled points but a large number of unlabeled points.  We show that in $\R^d$, $d \geq 2$, the method is actually not well-posed, and as the number of unlabeled points increases the solution degenerates to a noninformative function.  We also contrast the method with the Laplacian Eigenvector method, and discuss the ``smoothness assumptions associated with this alternate method.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">66</span></div><div id="author">Authors:<span id="author">Charles Kemp, Alan Jern, Fei Xu</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/38af86134b65d0f10fe33d30dd76442e-Paper.pdf">Individuation, Identification and Object Discovery</a></div><div id="abs">Abstract: <span id="abs">Humans are typically able to infer how many objects their environment contains and to recognize when the same object is encountered twice.  We present a simple statistical model that helps to explain these abilities and evaluate it in three behavioral experiments.  Our first experiment suggests that humans rely on prior knowledge when deciding whether an object token has been previously encountered. Our second and third experiments suggest that humans can infer how many objects they have seen and can learn about categories and their properties even when they are uncertain about which tokens are instances of the same object.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">89</span></div><div id="author">Authors:<span id="author">Robert Nowak</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/556f391937dfd4398cbac35e050a2177-Paper.pdf">Noisy Generalized Binary Search</a></div><div id="abs">Abstract: <span id="abs">This paper addresses the problem of noisy Generalized Binary Search (GBS).  GBS is a well-known greedy algorithm for determining a binary-valued hypothesis through a sequence of strategically selected queries.  At each step, a query is selected that most evenly splits the hypotheses under consideration into two disjoint subsets, a natural generalization of the idea underlying classic binary search.  GBS is used in many applications, including fault testing, machine diagnostics, disease diagnosis, job scheduling, image processing, computer vision, and active learning. In most of these cases, the responses to queries can be noisy.  Past work has provided a partial characterization of GBS, but existing noise-tolerant versions of GBS are suboptimal in terms of sample complexity.  This paper presents the first optimal algorithm for noisy GBS and demonstrates its application to learning multidimensional threshold functions.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">64</span></div><div id="author">Authors:<span id="author">Daniel Zoran, Yair Weiss</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/37f0e884fbad9667e38940169d0a3c95-Paper.pdf">The 'tree-dependent components' of natural scenes are edge filters</a></div><div id="abs">Abstract: <span id="abs">We propose a new model for natural image statistics. Instead of minimizing dependency between components of natural images, we maximize a simple form of dependency in the form of tree-dependency. By learning filters and tree structures which are best suited for natural images we observe that the resulting filters are edge filters, similar to the famous ICA on natural images results. Calculating the likelihood of the model requires estimating the squared output of pairs of filters connected in the tree. We observe that after learning, these pairs of filters are predominantly of similar orientations but different phases, so their joint energy resembles models of complex cells.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">81</span></div><div id="author">Authors:<span id="author">Liang Sun, Jun Liu, Jianhui Chen, Jieping Ye</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/4ca82782c5372a547c104929f03fe7a9-Paper.pdf">Efficient Recovery of Jointly Sparse Vectors</a></div><div id="abs">Abstract: <span id="abs">We consider the reconstruction of sparse signals in the multiple measurement vector (MMV) model,in which the signal, represented as a matrix, consists of a set of jointly sparse vectors. MMV is an extension of the single measurement vector (SMV) model employed in standard compressive sensing (CS). Recent theoretical studies focus on the convex relaxation of the MMV problem based on the $(2,1)$-norm minimization, which is an extension of the well-known $1$-norm minimization employed in SMV. However, the resulting convex optimization problem in MMV is significantly much more difficult to solve than the one in SMV. Existing algorithms reformulate it as a second-order cone programming (SOCP) or semidefinite programming (SDP), which is computationally expensive to solve for problems of moderate size. In this paper, we propose a new (dual) reformulation of the convex optimization problem in MMV and develop an efficient algorithm based on the prox-method. Interestingly, our theoretical analysis reveals the close connection between the proposed reformulation and multiple kernel learning. Our simulation studies demonstrate the scalability of the proposed algorithm.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">79</span></div><div id="author">Authors:<span id="author">Emily B. Fox, Michael I. Jordan, Erik B. Sudderth, Alan S. Willsky</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/4b04a686b0ad13dce35fa99fa4161c65-Paper.pdf">Sharing Features among Dynamical Systems with Beta Processes</a></div><div id="abs">Abstract: <span id="abs">We propose a Bayesian nonparametric approach to relating multiple time series via a set of latent, dynamical behaviors.  Using a beta process prior, we allow data-driven selection of the size of this set, as well as the pattern with which behaviors are shared among time series.  Via the Indian buffet process representation of the beta process predictive distributions, we develop an exact Markov chain Monte Carlo inference method.  In particular, our approach uses the sum-product algorithm to efficiently compute Metropolis-Hastings acceptance probabilities, and explores new dynamical behaviors via birth/death proposals.  We validate our sampling algorithm using several synthetic datasets, and also demonstrate promising unsupervised segmentation of visual motion capture data.</span></div> distribution with shape parameter q is compressible only in restricted cases since the expected decay rate of its N-sample iid realizations decreases with N as 1/[q log(N/q)]. We use compressible priors as a scaffold to build new iterative sparse signal recovery algorithms based on Bayesian inference arguments. We show how tuning of these algorithms explicitly depends on the parameters of the compressible prior of the signal, and how to learn the parameters of the signal’s compressible prior on the fly during recovery.</p><p id="section"><div id="pid">Paperid:<span id="pid">50</span></div><div id="author">Authors:<span id="author">Kai Yu, Tong Zhang, Yihong Gong</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/2afe4567e1bf64d32a5527244d104cea-Paper.pdf">Nonlinear Learning using Local Coordinate Coding</a></div><div id="abs">Abstract: <span id="abs">This paper introduces a new method for semi-supervised learning on high dimensional nonlinear manifolds, which includes a phase of unsupervised basis learning and a phase of supervised function learning. The learned bases provide a set of anchor points to form a local coordinate system, such that each data point x on the manifold can be locally approximated by a linear combination of its nearby anchor points, and the linear weights become its local coordinate coding. We show that a high dimensional nonlinear function can be approximated by a global linear function with respect to this coding scheme, and the approximation quality is ensured by the locality of such coding. The method turns a difficult nonlinear learning problem into a simple global linear learning problem, which overcomes some drawbacks of traditional local learning methods.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">56</span></div><div id="author">Authors:<span id="author">Wei Chen, Tie-yan Liu, Yanyan Lan, Zhi-ming Ma, Hang Li</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/2f55707d4193dc27118a0f19a1985716-Paper.pdf">Ranking Measures and Loss Functions in Learning to Rank</a></div><div id="abs">Abstract: <span id="abs">Learning to rank has become an important research topic in machine learning. While most learning-to-rank methods learn the ranking function by minimizing the loss functions, it is the ranking measures (such as NDCG and MAP) that are used to evaluate the performance of the learned ranking function. In this work, we reveal the relationship between ranking measures and loss functions in learning-to-rank methods, such as Ranking SVM, RankBoost, RankNet, and ListMLE. We show that these loss functions are upper bounds of the measure-based ranking errors. As a result, the minimization of these loss functions will lead to the maximization of the ranking measures. The key to obtaining this result is to model ranking as a sequence of classification tasks, and define a so-called essential loss as the weighted sum of the classification errors of individual tasks in the sequence. We have proved that the essential loss is both an upper bound of the measure-based ranking errors, and a lower bound of the loss functions in the aforementioned methods. Our proof technique also suggests a way to modify existing loss functions to make them tighter bounds of the measure-based ranking errors. Experimental results on benchmark datasets show that the modifications can lead to better ranking performance, demonstrating the correctness of our analysis.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">37</span></div><div id="author">Authors:<span id="author">Douglas Eck, Yoshua Bengio, Aaron C. Courville</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/1e6e0a04d20f50967c64dac2d639a577-Paper.pdf">An Infinite Factor Model Hierarchy Via a Noisy-Or Mechanism</a></div><div id="abs">Abstract: <span id="abs">The Indian Buffet Process is a Bayesian nonparametric approach that models objects as arising from an infinite number of latent factors. Here we extend the latent factor model framework to two or more unbounded layers of latent factors. From a generative perspective, each layer defines a conditional \emph{factorial} prior distribution over the binary latent variables of the layer below via a noisy-or mechanism. We explore the properties of the model with two empirical studies, one digit recognition task and one music tag data experiment.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">25</span></div><div id="author">Authors:<span id="author">Hamed Pirsiavash, Deva Ramanan, Charless C. Fowlkes</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/147702db07145348245dc5a2f2fe5683-Paper.pdf">Bilinear classifiers for visual recognition</a></div><div id="abs">Abstract: <span id="abs">We describe an algorithm for learning bilinear SVMs. Bilinear classifiers are a discriminative variant of bilinear models, which capture the dependence of data on multiple factors. Such models are particularly appropriate for visual data that is better represented as a matrix or tensor, rather than a vector. Matrix encodings allow for more natural regularization through rank restriction. For example, a rank-one scanning-window classifier yields a separable filter. Low-rank models have fewer parameters and so are easier to regularize and faster to score at run-time. We learn low-rank models with bilinear classifiers. We also use bilinear classifiers for transfer learning by sharing linear factors between different classification tasks. Bilinear classifiers are trained with biconvex programs. Such programs are optimized with coordinate descent, where each coordinate step requires solving a convex program - in our case, we use a standard off-the-shelf SVM solver. We demonstrate bilinear SVMs on difficult problems of people detection in video sequences and action classification of video sequences, achieving state-of-the-art results in both.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">12</span></div><div id="author">Authors:<span id="author">Stefan Klampfl, Wolfgang Maass</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/08c5433a60135c32e34f46a71175850c-Paper.pdf">Replacing supervised classification learning by Slow Feature Analysis in spiking neural networks</a></div><div id="abs">Abstract: <span id="abs">Many models for computations in recurrent networks of neurons assume that the network state moves from some initial state to some fixed point attractor or limit cycle that represents the output of the computation. However experimental data show that in response to a sensory stimulus the network state moves from its initial state through a trajectory of network states and eventually returns to the initial state, without reaching an attractor or limit cycle in between. This type of network response, where salient information about external stimuli is encoded in characteristic trajectories of continuously varying network states, raises the question how a neural system could compute with such code, and arrive for example at a temporally stable classification of the external stimulus. We show that a known unsupervised learning algorithm, Slow Feature Analysis (SFA), could be an important ingredient for extracting stable information from these network trajectories. In fact, if sensory stimuli are more often followed by another stimulus from the same class than by a stimulus from another class, SFA approaches the classification capability of Fishers Linear Discriminant (FLD), a powerful algorithm for supervised learning. We apply this principle to simulated cortical microcircuits, and show that it enables readout neurons to learn discrimination of spoken digits and detection of repeating firing patterns within a stream of spike trains with the same firing statistics, without requiring any supervision for learning.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">82</span></div><div id="author">Authors:<span id="author">Liefeng Bo, Cristian Sminchisescu</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/4daa3db355ef2b0e64b472968cb70f0d-Paper.pdf">Efficient Match Kernel between Sets of Features for Visual Recognition</a></div><div id="abs">Abstract: <span id="abs">In visual recognition, the images are frequently modeled as sets of local features (bags). We show that bag of words, a common method to handle such cases, can be viewed as a special match kernel, which counts 1 if two local features fall into the same regions partitioned by visual words and 0 otherwise. Despite its simplicity, this quantization is too coarse. It is, therefore, appealing to design match kernels that more accurately measure the similarity between local features. However, it is impractical to use such kernels on large datasets due to their significant computational cost. To address this problem, we propose an efficient match kernel (EMK), which maps local features to a low dimensional feature space, average the resulting feature vectors to form a set-level feature, then apply a linear classifier. The local feature maps are learned so that their inner products preserve, to the best possible, the values of the specified kernel function. EMK is linear both in the number of images and in the number of local features. We demonstrate that EMK is extremely efficient and achieves the current state of the art performance on three difficult real world datasets: Scene-15, Caltech-101 and Caltech-256.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">8</span></div><div id="author">Authors:<span id="author">Henning Sprekeler, Guillaume Hennequin, Wulfram Gerstner</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/04ecb1fa28506ccb6f72b12c0245ddbc-Paper.pdf">Code-specific policy gradient rules for spiking neurons</a></div><div id="abs">Abstract: <span id="abs">Although it is widely believed that reinforcement learning is a suitable tool for describing behavioral learning, the mechanisms by which it can be implemented in networks of spiking neurons are not fully understood. Here, we show that different learning rules emerge from a policy gradient approach depending on which features of the spike trains are assumed to influence the reward signals, i.e., depending on which neural code is in effect. We use the framework of Williams (1992) to derive learning rules for arbitrary neural codes. For illustration, we present policy-gradient rules for  three different example codes - a spike count code, a spike timing code and the most general ``full spike train code - and test them on simple model problems. In addition to classical synaptic learning, we derive learning rules for intrinsic parameters that control the excitability of the neuron. The spike count learning rule has structural similarities with established Bienenstock-Cooper-Munro rules. If the distribution of the relevant spike train features  belongs to the natural exponential family, the learning rules have a characteristic shape that raises interesting prediction problems.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">47</span></div><div id="author">Authors:<span id="author">Andrea Vedaldi, Andrew Zisserman</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/2a38a4a9316c49e5a833517c45d31070-Paper.pdf">Structured output regression for detection with partial truncation</a></div><div id="abs">Abstract: <span id="abs">We develop a structured output model for object category detection that explicitly accounts for alignment, multiple aspects and partial truncation in both training and inference. The model is formulated as large margin learning with latent variables and slack rescaling, and both training and inference are computationally efficient. We make the following contributions: (i) we note that extending the Structured Output Regression formulation of Blaschko and Lampert (ECCV 2008) to include a bias term significantly improves performance; (ii) that alignment (to account for small rotations and anisotropic scalings) can be included as a latent variable and efficiently determined and implemented; (iii) that the latent variable extends to multiple aspects (e.g. left facing, right facing, front) with the same formulation; and (iv), most significantly for performance, that truncated and truncated instances can be included in both training and inference with an explicit truncation mask. We demonstrate the method by training and testing on the PASCAL VOC 2007 data set -- training includes the truncated examples, and in testing object instances are detected at multiple scales, alignments, and with significant truncations.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">106</span></div><div id="author">Authors:<span id="author">Wu-jun Li, Dit-Yan Yeung, Zhihua Zhang</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/69adc1e107f7f7d035d7baf04342e1ca-Paper.pdf">Probabilistic Relational PCA</a></div><div id="abs">Abstract: <span id="abs">One crucial assumption made by both principal component analysis (PCA) and probabilistic PCA (PPCA) is that the instances are independent and identically distributed (i.i.d.). However, this common i.i.d. assumption is unreasonable for relational data. In this paper, by explicitly modeling covariance between instances as derived from the relational information, we propose a novel probabilistic dimensionality reduction method, called probabilistic relational PCA (PRPCA), for relational data analysis. Although the i.i.d. assumption is no longer adopted in PRPCA, the learning algorithms for PRPCA can still be devised easily like those for PPCA which makes explicit use of the i.i.d. assumption. Experiments on real-world data sets show that PRPCA can effectively utilize the relational information to dramatically outperform PCA and achieve state-of-the-art performance.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">31</span></div><div id="author">Authors:<span id="author">Benjamin V. Durme, Ashwin Lall</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/185c29dc24325934ee377cfda20e414c-Paper.pdf">Streaming Pointwise Mutual Information</a></div><div id="abs">Abstract: <span id="abs">Recent work has led to the ability to perform space efﬁcient, approximate counting  over large vocabularies in a streaming context. Motivated by the existence of data  structures of this type, we explore the computation of associativity scores, other-  wise known as pointwise mutual information (PMI), in a streaming context. We  give theoretical bounds showing the impracticality of perfect online PMI compu-  tation, and detail an algorithm with high expected accuracy. Experiments on news  articles show our approach gives high accuracy on real world data.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">36</span></div><div id="author">Authors:<span id="author">Hongjing Lu, Matthew Weiden, Alan L. Yuille</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/1e48c4420b7073bc11916c6c1de226bb-Paper.pdf">Modeling the spacing effect in sequential category learning</a></div><div id="abs">Abstract: <span id="abs">We develop a Bayesian sequential model for category learning. The sequential model updates two category parameters, the mean and the variance, over time. We define conjugate temporal priors to enable closed form solutions to be obtained. This model can be easily extended to supervised and unsupervised learning involving multiple categories. To model the spacing effect, we introduce a generic prior in the temporal updating stage to capture a learning preference, namely, less change for repetition and more change for variation. Finally, we show how this approach can be generalized to efficiently performmodel selection to decide whether observations are from one or multiple categories.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">117</span></div><div id="author">Authors:<span id="author">Vinod Nair, Geoffrey E. Hinton</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/6e7b33fdea3adc80ebd648fffb665bb8-Paper.pdf">3D Object Recognition with Deep Belief Nets</a></div><div id="abs">Abstract: <span id="abs">We introduce a new type of Deep Belief Net and evaluate it on a 3D object recognition task. The top-level model is a third-order Boltzmann machine, trained using a hybrid algorithm that combines both generative and discriminative gradients. Performance is evaluated on the NORB database(normalized-uniform version), which contains stereo-pair images of objects under different lighting conditions and viewpoints. Our model achieves 6.5% error on the test set, which is close to the best published result for NORB (5.9%) using a convolutional neural net that has built-in knowledge of translation invariance. It substantially outperforms shallow models such as SVMs (11.6%). DBNs are especially suited for semi-supervised learning, and to demonstrate this we consider a modified version of the NORB recognition task in which additional unlabeled images are created by applying small translations to the images in the database. With the extra unlabeled data (and the same amount of labeled data as before), our model achieves 5.2% error, making it the current best result for NORB.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">19</span></div><div id="author">Authors:<span id="author">Andreas Bartels, Matthew Blaschko, Jacquelyn A. Shelton</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/0f840be9b8db4d3fbd5ba2ce59211f55-Paper.pdf">Augmenting Feature-driven fMRI Analyses: Semi-supervised learning and resting state activity</a></div><div id="abs">Abstract: <span id="abs">Resting state activity is brain activation that arises in the absence of any task, and is usually measured in awake subjects during prolonged fMRI scanning sessions where the only instruction given is to close the eyes and do nothing.  It has been recognized in recent years that resting state activity is implicated in a wide variety of brain function.  While certain networks of brain areas have different levels of activation at rest and during a task, there is nevertheless significant similarity between activations in the two cases.  This suggests that recordings of resting state activity can be used as a source of unlabeled data to augment discriminative regression techniques in a semi-supervised setting.  We evaluate this setting empirically yielding three main results: (i) regression tends to be improved by the use of Laplacian regularization even when no additional unlabeled data are available, (ii) resting state data may have a similar marginal distribution to that recorded during the execution of a visual processing task reinforcing the hypothesis that these conditions have similar types of activation, and (iii) this source of information can be broadly exploited to improve the robustness of empirical inference in fMRI studies, an inherently data poor domain.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">26</span></div><div id="author">Authors:<span id="author">Mark Palatucci, Dean Pomerleau, Geoffrey E. Hinton, Tom M. Mitchell</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/1543843a4723ed2ab08e18053ae6dc5b-Paper.pdf">Zero-shot Learning with Semantic Output Codes</a></div><div id="abs">Abstract: <span id="abs">We consider the problem of zero-shot learning, where the goal is to learn a classifier $f: X \rightarrow Y$ that must predict novel values of $Y$ that were omitted from the training set. To achieve this, we define the notion of a semantic output code classifier (SOC) which utilizes a knowledge base of semantic properties of $Y$ to extrapolate to novel classes. We provide a formalism for this type of classifier and study its theoretical properties in a PAC framework, showing conditions under which the classifier can accurately predict novel classes.  As a case study, we build a SOC classifier for a neural decoding task and show that it can often predict words that people are thinking about from functional magnetic resonance images (fMRI) of their neural activity, even without training examples for those words.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">41</span></div><div id="author">Authors:<span id="author">Zhirong Yang, Irwin King, Zenglin Xu, Erkki Oja</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/2291d2ec3b3048d1a6f86c2c4591b7e0-Paper.pdf">Heavy-Tailed Symmetric Stochastic Neighbor Embedding</a></div><div id="abs">Abstract: <span id="abs">Stochastic Neighbor Embedding (SNE) has shown to be quite promising for data visualization.  Currently, the most popular implementation, t-SNE, is restricted to a particular Student t-distribution as its embedding distribution. Moreover, it uses a gradient descent algorithm that may require users to tune parameters such as the learning step size, momentum, etc., in finding its optimum. In this paper, we propose the Heavy-tailed Symmetric Stochastic Neighbor Embedding (HSSNE) method, which is a generalization of the t-SNE to accommodate various heavy-tailed embedding similarity functions. With this generalization, we are presented with two difficulties.  The first is how to select the best embedding similarity among all heavy-tailed functions and the second is how to optimize the objective function once the heave-tailed function has been selected. Our contributions then are: (1) we point out that various heavy-tailed embedding similarities can be characterized by their negative score functions. Based on this finding, we present a parameterized subset of similarity functions for choosing the best tail-heaviness for HSSNE; (2) we present a fixed-point optimization algorithm that can be applied to all heavy-tailed functions and does not require the user to set any parameters; and (3) we present two empirical studies, one for unsupervised visualization showing that our optimization algorithm runs as fast and as good as the best known t-SNE implementation and the other for semi-supervised visualization showing quantitative superiority using the homogeneity measure as well as qualitative advantage in cluster separation over t-SNE.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">116</span></div><div id="author">Authors:<span id="author">Dimitris Margaritis</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Paper.pdf">Toward Provably Correct Feature Selection in Arbitrary Domains</a></div><div id="abs">Abstract: <span id="abs">In this paper we address the problem of provably correct feature selection in arbitrary domains.  An optimal solution to the problem is a Markov boundary, which is a minimal set of features that make the probability distribution of a target variable conditionally invariant to the state of all other features in the domain.  While numerous algorithms for this problem have been proposed, their theoretical correctness and practical behavior under arbitrary probability distributions is unclear.  We address this by introducing the Markov Boundary Theorem that precisely characterizes the properties of an ideal Markov boundary, and use it to develop algorithms that learn a more general boundary that can capture complex interactions that only appear when the values of multiple features are considered together.  We introduce two algorithms: an exact, provably correct one as well a more practical randomized anytime version, and show that they perform well on artificial as well as benchmark and real-world data sets.  Throughout the paper we make minimal assumptions that consist of only a general set of axioms that hold for every probability distribution, which gives these algorithms universal applicability.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">53</span></div><div id="author">Authors:<span id="author">Feng Zhou, Fernando Torre</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Paper.pdf">Canonical Time Warping for Alignment of Human Behavior</a></div><div id="abs">Abstract: <span id="abs">Alignment of time series is an important problem to solve in many scientific disciplines. In particular, temporal alignment of two or more subjects performing similar activities is a challenging problem due to the large temporal scale difference between human actions as well as the inter/intra subject variability. In this paper we present canonical time warping (CTW), an extension of canonical correlation analysis (CCA) for spatio-temporal alignment of the behavior between two subjects. CTW extends previous work on CCA in two ways: (i) it combines CCA with dynamic time warping for temporal alignment; and (ii) it extends CCA to allow local spatial deformations. We show CTWs effectiveness in three experiments: alignment of synthetic data, alignment of motion capture data of two subjects performing similar actions, and alignment of two people with similar facial expressions. Our results demonstrate that CTW provides both visually and qualitatively better alignment than state-of-the-art techniques based on dynamic time warping.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">104</span></div><div id="author">Authors:<span id="author">Kevin Briggman, Winfried Denk, Sebastian Seung, Moritz N. Helmstaedter, Srinivas C. Turaga</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/68d30a9594728bc39aa24be94b319d21-Paper.pdf">Maximin affinity learning of image segmentation</a></div><div id="abs">Abstract: <span id="abs">Images can be segmented by first using a classifier to predict an affinity graph that reflects the degree to which image pixels must be grouped together and then partitioning the graph to yield a segmentation. Machine learning has been applied to the affinity classifier to produce affinity graphs that are good in the sense of minimizing edge misclassification rates. However, this error measure is only indirectly related to the quality of segmentations produced by ultimately partitioning the affinity graph. We present the first machine learning algorithm for training a classifier to produce affinity graphs that are good in the sense of producing segmentations that directly minimize the Rand index, a well known segmentation performance measure. The Rand index measures segmentation performance by quantifying the classification of the connectivity of image pixel pairs after segmentation. By using the simple graph partitioning algorithm of finding the connected components of the thresholded affinity graph, we are able to train an affinity classifier to directly minimize the Rand index of segmentations resulting from the graph partitioning. Our learning algorithm corresponds to the learning of maximin affinities between image pixel pairs, which are predictive of the pixel-pair connectivity.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">113</span></div><div id="author">Authors:<span id="author">Robert Wilson, Leif Finkel</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/6d0f846348a856321729a2f36734d1a7-Paper.pdf">A Neural Implementation of the Kalman Filter</a></div><div id="abs">Abstract: <span id="abs">There is a growing body of experimental evidence to suggest that the brain is capable of approximating optimal Bayesian inference in the face of noisy input stimuli.  Despite this progress, the neural underpinnings of this computation are still poorly  understood.  In this paper we focus on the problem of Bayesian filtering of stochastic time series.  In particular we introduce a novel neural network, derived from a line attractor architecture, whose dynamics map directly onto those of the Kalman Filter in the limit where the prediction error is small.  When the prediction error is large we show that the network responds robustly to change-points in a way that is qualitatively compatible with the optimal Bayesian model.  The model suggests ways in which probability distributions are encoded in the brain and makes a number of testable experimental predictions.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">119</span></div><div id="author">Authors:<span id="author">Novi Quadrianto, John Lim, Dale Schuurmans, Tibério S. Caetano</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/7250eb93b3c18cc9daa29cf58af7a004-Paper.pdf">Convex Relaxation of Mixture Regression with Efficient Algorithms</a></div><div id="abs">Abstract: <span id="abs">We develop a convex relaxation of maximum a posteriori estimation of a mixture of regression models. Although our relaxation involves a semidefinite matrix variable, we reformulate the problem to eliminate the need for general semidefinite programming. In particular, we provide two reformulations that admit fast algorithms. The first is a max-min spectral reformulation exploiting quasi-Newton descent. The second is a min-min reformulation consisting of fast alternating steps of closed-form updates. We evaluate the methods against Expectation-Maximization in a real problem of motion segmentation from video data.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">108</span></div><div id="author">Authors:<span id="author">Harold Pashler, Nicholas Cepeda, Robert Lindsey, Ed Vul, Michael Mozer</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/6bc24fc1ab650b25b4114e93a98f1eba-Paper.pdf">Predicting the Optimal Spacing of Study: A Multiscale Context Model of Memory</a></div><div id="abs">Abstract: <span id="abs">When individuals learn facts (e.g., foreign language vocabulary) over multiple study sessions, the temporal spacing of study has a significant impact on memory retention.  Behavioral experiments have shown a nonmonotonic relationship between spacing and retention:  short or long intervals between study sessions yield lower cued-recall accuracy than intermediate intervals.  Appropriate spacing of study can double retention on educationally relevant time scales.  We introduce a Multiscale Context Model (MCM) that is able to predict the influence of a particular study schedule on retention for specific material.  MCMs prediction is based on empirical data characterizing forgetting of the material following a single study session.  MCM is a synthesis of two existing memory models (Staddon, Chelaru, &amp; Higa, 2002; Raaijmakers, 2003).  On the surface, these  models are unrelated and incompatible, but we show they share a core feature  that allows them to be integrated.  MCM can determine study schedules that  maximize the durability of learning, and has implications for education  and training.  MCM can be cast either as a neural network with inputs that  fluctuate over time, or as a cascade of leaky integrators.  MCM is  intriguingly similar to a Bayesian multiscale model of memory (Kording, Tenenbaum, Shadmehr, 2007), yet MCM is better able to account for human  declarative memory.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">115</span></div><div id="author">Authors:<span id="author">Matthias Seeger</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/6d70cb65d15211726dcce4c0e971e21c-Paper.pdf">Speeding up Magnetic Resonance Image Acquisition by Bayesian Multi-Slice Adaptive Compressed Sensing</a></div><div id="abs">Abstract: <span id="abs">We show how to sequentially optimize magnetic resonance imaging measurement designs over stacks of neighbouring image slices, by performing convex variational inference on a large scale non-Gaussian linear dynamical system, tracking dominating directions of posterior covariance without imposing any factorization constraints. Our approach can be scaled up to high-resolution images by reductions to numerical mathematics primitives and parallelization on several levels. In a first study, designs are found that improve significantly on others chosen independently for each slice or drawn at random.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">87</span></div><div id="author">Authors:<span id="author">Tomer Ullman, Chris Baker, Owen Macindoe, Owain Evans, Noah Goodman, Joshua B. Tenenbaum</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/52292e0c763fd027c6eba6b8f494d2eb-Paper.pdf">Help or Hinder: Bayesian Models of Social Goal Inference</a></div><div id="abs">Abstract: <span id="abs">Everyday social interactions are heavily influenced by our snap judgments about others goals.  Even young infants can infer the goals of intentional agents from observing how they interact with objects and other agents in their environment: e.g., that one agent ishelping orhindering anothers attempt to get up a hill or open a box. We propose a model for how people can infer these social goals from actions, based on inverse planning in multiagent Markov decision problems (MDPs).  The model infers the goal most likely to be driving an agents behavior by assuming the agent acts approximately rationally given environmental constraints and its model of other agents present.  We also present behavioral evidence in support of this model over a simpler, perceptual cue-based alternative.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">110</span></div><div id="author">Authors:<span id="author">Shuang-hong Yang, Hongyuan Zha, Bao-gang Hu</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/6c8349cc7260ae62e3b1396831a8398f-Paper.pdf">Dirichlet-Bernoulli Alignment: A Generative Model for Multi-Class Multi-Label Multi-Instance Corpora</a></div><div id="abs">Abstract: <span id="abs">We propose Dirichlet-Bernoulli Alignment (DBA), a generative model for corpora in which each pattern (e.g., a document) contains a set of instances (e.g., paragraphs in the document) and belongs to multiple classes. By casting predefined classes as latent Dirichlet variables (i.e., instance level labels), and modeling the multi-label of each pattern as Bernoulli variables conditioned on the weighted empirical average of topic assignments, DBA automatically aligns the latent topics discovered from data to human-defined classes. DBA is useful for both pattern classification and instance disambiguation, which are tested on text classification and named entity disambiguation for web search queries respectively.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">11</span></div><div id="author">Authors:<span id="author">Amarnag Subramanya, Jeff A. Bilmes</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/0768281a05da9f27df178b5c39a51263-Paper.pdf">Entropic Graph Regularization in Non-Parametric Semi-Supervised Classification</a></div><div id="abs">Abstract: <span id="abs">We prove certain theoretical properties of a graph-regularized transductive learning objective that is based on minimizing a Kullback-Leibler divergence based loss. These include showing that the iterative alternating minimization procedure used to minimize the objective converges to the correct solution and deriving a test for convergence. We also propose a graph node ordering algorithm that is cache cognizant and leads to a linear speedup in parallel computations. This ensures that the algorithm scales to large data sets. By making use of empirical evaluation on the TIMIT and Switchboard I corpora, we show this approach is able to out-perform other state-of-the-art SSL approaches. In one instance, we solve a problem on a 120 million node graph.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">105</span></div><div id="author">Authors:<span id="author">Samuel Gershman, Ed Vul, Joshua B. Tenenbaum</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/692f93be8c7a41525c0baf2076aecfb4-Paper.pdf">Perceptual Multistability as Markov Chain Monte Carlo Inference</a></div><div id="abs">Abstract: <span id="abs">While many perceptual and cognitive phenomena are well described in terms of Bayesian inference, the necessary computations are intractable at the scale of real-world tasks, and it remains unclear how the human mind approximates Bayesian inference algorithmically. We explore the proposal that for some tasks, humans use a form of Markov Chain Monte Carlo to approximate the posterior distribution over hidden variables.  As a case study, we show how several phenomena of perceptual multistability can be explained as MCMC inference in simple graphical models for low-level vision.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">107</span></div><div id="author">Authors:<span id="author">Umar Syed, Aleksandrs Slivkins, Nina Mishra</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/6aab1270668d8cac7cef2566a1c5f569-Paper.pdf">Adapting to the Shifting Intent of Search Queries</a></div><div id="abs">Abstract: <span id="abs">Search engines today present results that are often oblivious to recent shifts in intent. For example, the meaning of the query independence day shifts in early July to a US holiday and to a movie around the time of the box office release.  While no studies exactly quantify the magnitude of intent-shifting traffic, studies suggest that news events, seasonal topics, pop culture, etc account for 1/2 the search queries.  This paper shows that the signals a search engine receives can be used to both determine that a shift in intent happened, as well as find a result that is now more relevant. We present a meta-algorithm that marries a classifier with a bandit algorithm to achieve regret that depends logarithmically on the number of query impressions, under certain assumptions.  We provide strong evidence that this regret is close to the best achievable. Finally, via a series of experiments, we demonstrate that our algorithm outperforms prior approaches, particularly as the amount of intent-shifting traffic increases.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">109</span></div><div id="author">Authors:<span id="author">Samuel R. Bulò, Marcello Pelillo</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/6c3cf77d52820cd0fe646d38bc2145ca-Paper.pdf">A Game-Theoretic Approach to Hypergraph Clustering</a></div><div id="abs">Abstract: <span id="abs">Hypergraph clustering refers to the process of extracting maximally coherent groups from a set of objects using high-order (rather than pairwise) similarities. Traditional approaches to this problem are based on the idea of partitioning the input data into a user-defined number of classes, thereby obtaining the clusters as a by-product of the partitioning process. In this paper, we provide a radically different perspective to the problem. In contrast to the classical approach, we attempt to provide a meaningful formalization of the very notion of a cluster and we show that game theory offers an attractive and unexplored perspective that serves well our purpose. Specifically, we show that the hypergraph clustering problem can be naturally cast into a non-cooperative multi-player ``clustering game, whereby the notion of a cluster is equivalent to a classical game-theoretic equilibrium concept. From the computational viewpoint, we show that the problem of finding the equilibria of our clustering game is equivalent to locally optimizing a polynomial function over the standard simplex, and we provide a discrete-time dynamics to perform this optimization. Experiments are presented which show the superiority of our approach over state-of-the-art hypergraph clustering techniques.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">15</span></div><div id="author">Authors:<span id="author">Nino Shervashidze, Karsten Borgwardt</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/0a49e3c3a03ebde64f85c0bacd8a08e2-Paper.pdf">Fast subtree kernels on graphs</a></div><div id="abs">Abstract: <span id="abs">In this article, we propose fast subtree kernels on graphs. On graphs with n nodes and m edges and maximum degree d, these kernels comparing subtrees of height h can be computed in O(mh), whereas the classic subtree kernel by Ramon &amp; G¨artner scales as O(n24dh). Key to this efﬁciency is the observation that the Weisfeiler-Lehman test of isomorphism from graph theory elegantly computes a subtree kernel as a byproduct. Our fast subtree kernels can deal with labeled graphs, scale up easily to large graphs and outperform state-of-the-art graph ker- nels on several classiﬁcation benchmark datasets in terms of accuracy and runtime.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">118</span></div><div id="author">Authors:<span id="author">Guy Shani, Christopher Meek</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/70c639df5e30bdee440e4cdf599fec2b-Paper.pdf">Improving Existing Fault Recovery Policies</a></div><div id="abs">Abstract: <span id="abs">Automated recovery from failures is a key component in the management of large data centers. Such systems typically employ a hand-made controller created by an expert. While such controllers capture many important aspects of the recovery process, they are often not systematically optimized to reduce costs such as server downtime. In this paper we explain how to use data gathered from the interactions of the hand-made controller with the system, to create an optimized controller. We suggest learning an indefinite horizon Partially Observable Markov Decision Process, a model for decision making under uncertainty, and solve it using a point-based algorithm. We describe the complete process, starting with data gathering, model learning, model checking procedures, and computing a policy. While our paper focuses on a specific domain, our method is applicable to other systems that use a hand-coded, imperfect controllers.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">120</span></div><div id="author">Authors:<span id="author">Adam Sanborn, Nick Chater, Katherine A. Heller</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/731c83db8d2ff01bdc000083fd3c3740-Paper.pdf">Hierarchical Learning of Dimensional Biases in Human Categorization</a></div><div id="abs">Abstract: <span id="abs">Existing models of categorization typically represent to-be-classified items as points in a multidimensional space. While from a mathematical point of view, an infinite number of basis sets can be used to represent points in this space, the choice of basis set is psychologically crucial. People generally choose the same basis dimensions, and have a strong preference to generalize along the axes of these dimensions, but not diagonally". What makes some choices of dimension special? We explore the idea that the dimensions used by people echo the natural variation in the environment. Specifically, we present a rational model that does not assume dimensions, but learns the same type of dimensional generalizations that people display. This bias is shaped by exposing the model to many categories with a structure hypothesized to be like those which children encounter. Our model can be viewed as a type of transformed Dirichlet process mixture model, where it is the learning of the base distribution of the Dirichlet process which allows dimensional generalization.The learning behaviour of our model captures the developmental shift from roughly "isotropic" for children to the axis-aligned generalization that adults show."</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">3</span></div><div id="author">Authors:<span id="author">Odalric Maillard, Rémi Munos</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/01882513d5fa7c329e940dda99b12147-Paper.pdf">Compressed Least-Squares Regression</a></div><div id="abs">Abstract: <span id="abs">We consider the problem of learning, from K input data, a regression function in a function space of high dimension N using projections onto a random subspace of lower dimension M. From any linear approximation algorithm using empirical risk minimization (possibly penalized), we provide bounds on the excess risk of the estimate computed in the projected subspace (compressed domain) in terms of the excess risk of the estimate built in the high-dimensional space (initial domain). We apply the analysis to the ordinary Least-Squares regression and show that by choosing M=O(\sqrt{K}), the estimation error (for the quadratic loss) of the ``Compressed Least Squares Regression is O(1/\sqrt{K}) up to logarithmic factors. We also discuss the numerical complexity of several algorithms (both in initial and compressed domains) as a function of N, K, and M.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">114</span></div><div id="author">Authors:<span id="author">Menachem Fromer, Amir Globerson</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/6d3a2d24eb109dddf78374fe5d0ee067-Paper.pdf">An LP View of the M-best MAP problem</a></div><div id="abs">Abstract: <span id="abs">(1 - di )i (zi ) + ijE</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">111</span></div><div id="author">Authors:<span id="author">Daniel Cavagnaro, Jay Myung, Mark A. Pitt</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/6cd67d9b6f0150c77bda2eda01ae484c-Paper.pdf">Adaptive Design Optimization in Experiments with People</a></div><div id="abs">Abstract: <span id="abs">In cognitive science, empirical data collected from participants are the arbiters in model selection. Model discrimination thus depends on designing maximally informative experiments.  It has been shown that adaptive design optimization (ADO) allows one to discriminate models as efficiently as possible in simulation experiments.  In this paper we use ADO in a series of experiments with people to discriminate the Power, Exponential, and Hyperbolic models of memory retention, which has been a long-standing problem in cognitive science, providing an ideal setting in which to test the application of ADO for addressing questions about human cognition. Using an optimality criterion based on mutual information, ADO is able to find designs that are maximally likely to increase our certainty about the true model upon observation of the experiment outcomes.  Results demonstrate the usefulness of ADO and also reveal some challenges in its implementation.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">130</span></div><div id="author">Authors:<span id="author">Raman Arora</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/82cec96096d4281b7c95cd7e74623496-Paper.pdf">On Learning Rotations</a></div><div id="abs">Abstract: <span id="abs">An algorithm is presented for online learning of rotations. The proposed algorithm involves matrix exponentiated gradient updates and is motivated by the Von Neumann divergence. The additive updates are skew-symmetric matrices with trace zero which comprise the Lie algebra of the rotation group. The orthogonality and unit determinant of the matrix  parameter are preserved using matrix logarithms and exponentials and the algorithm lends itself to interesting interpretations in terms of the computational topology of the compact Lie groups. The stability and the computational complexity of the algorithm are discussed.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">121</span></div><div id="author">Authors:<span id="author">Bing Bai, Jason Weston, David Grangier, Ronan Collobert, Kunihiko Sadamasa, Yanjun Qi, Corinna Cortes, Mehryar Mohri</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/7504adad8bb96320eb3afdd4df6e1f60-Paper.pdf">Polynomial Semantic Indexing</a></div><div id="abs">Abstract: <span id="abs">We present a class of nonlinear (polynomial) models that are discriminatively trained to directly map from the word content in a query-document or document-document pair to a ranking score. Dealing with polynomial models on word features is computationally challenging. We propose a low rank (but diagonal preserving) representation of our polynomial models to induce feasible memory and computation requirements. We provide an empirical study on retrieval tasks based on Wikipedia documents, where we obtain state-of-the-art performance while providing realistically scalable methods.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">131</span></div><div id="author">Authors:<span id="author">Jacob Goldberger, Amir Leshem</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/839ab46820b524afda05122893c2fe8e-Paper.pdf">A Gaussian Tree Approximation for Integer Least-Squares</a></div><div id="abs">Abstract: <span id="abs">This paper proposes a new algorithm for the linear least squares problem where the unknown variables are constrained to be in a finite set.  The factor graph that corresponds to this problem is very loopy; in fact, it is a complete graph. Hence, applying the Belief Propagation (BP) algorithm yields very poor results. The algorithm described here is based on  an optimal  tree approximation of the Gaussian density of the unconstrained linear system. It is shown that even though the approximation is not directly applied to the exact discrete distribution, applying the BP algorithm to the modified factor graph outperforms current methods in terms of both performance and complexity. The improved performance of the proposed algorithm is demonstrated  on the problem of MIMO detection.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">133</span></div><div id="author">Authors:<span id="author">Andrew McCallum, Karl Schultz, Sameer Singh</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/847cc55b7032108eee6dd897f3bca8a5-Paper.pdf">FACTORIE: Probabilistic Programming via Imperatively Defined Factor Graphs</a></div><div id="abs">Abstract: <span id="abs">Discriminatively trained undirected graphical models have had wide empirical success, and there has been increasing interest in toolkits that ease their application to complex relational data.  The power in relational models is in their repeated structure and tied parameters; at issue is how to define these structures in a powerful and flexible way. Rather than using a declarative language, such as SQL or first-order logic, we advocate using an imperative language to express various aspects of model structure, inference, and learning.  By combining the traditional, declarative, statistical semantics of factor graphs with imperative definitions of their construction and operation, we allow the user to mix declarative and procedural domain knowledge, and also gain significant efficiencies.  We have implemented such imperatively defined factor graphs in a system we call Factorie, a software library for an object-oriented, strongly-typed, functional language.  In experimental comparisons to Markov Logic Networks on joint segmentation and coreference, we find our approach to be 3-15 times faster while reducing error by 20-25%-achieving a new state of the art.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">123</span></div><div id="author">Authors:<span id="author">Irina Rish, Benjamin Thyreau, Bertrand Thirion, Marion Plaze, Marie-laure Paillere-martinot, Catherine Martelli, Jean-luc Martinot, Jean-baptiste Poline, Guillermo A. Cecchi</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/788d986905533aba051261497ecffcbb-Paper.pdf">Discriminative Network Models of Schizophrenia</a></div><div id="abs">Abstract: <span id="abs">Schizophrenia is a complex psychiatric disorder that has eluded a characterization in terms of local abnormalities of brain activity, and is hypothesized to affect the collective, ``emergent working of the brain. We propose a novel data-driven approach to capture emergent features using functional brain networks [Eguiluzet al] extracted from fMRI data, and demonstrate its advantage over traditional region-of-interest (ROI) and local, task-specific linear activation analyzes. Our results suggest that schizophrenia is indeed associated with disruption of global, emergent brain properties related to its functioning as a network, which cannot be explained by alteration of local activation patterns. Moreover, further exploitation of interactions by sparse Markov Random Field classifiers shows clear gain over linear methods, such as Gaussian Naive Bayes and SVM, allowing to reach 86% accuracy (over 50% baseline - random guess), which is quite remarkable given that it is based on a single fMRI experiment using a simple auditory task.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">112</span></div><div id="author">Authors:<span id="author">Jonathan Huang, Carlos Guestrin</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/6cfe0e6127fa25df2a0ef2ae1067d915-Paper.pdf">Riffled Independence for Ranked Data</a></div><div id="abs">Abstract: <span id="abs">Representing distributions over permutations can be a daunting task due to the fact that the number of permutations of n objects scales factorially in n. One recent way that has been used to reduce storage complexity has been to exploit probabilistic independence, but as we argue, full independence assumptions impose strong sparsity constraints on distributions and are unsuitable for modeling rankings. We identify a novel class of independence structures, called riffled independence, which encompasses a more expressive family of distributions while retaining many of the properties necessary for performing efficient inference and reducing sample complexity. In riffled independence, one draws two permutations independently, then performs the riffle shuffle, common in card games, to combine the two permutations to form a single permutation. In ranking, riffled independence corresponds to ranking disjoint sets of objects independently, then interleaving those rankings. We provide a formal introduction and present algorithms for using riffled independence within Fourier-theoretic frameworks which have been explored by a number of recent papers.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">128</span></div><div id="author">Authors:<span id="author">Barry Chai, Dirk Walther, Diane Beck, Li Fei-fei</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/8248a99e81e752cb9b41da3fc43fbe7f-Paper.pdf">Exploring Functional Connectivities of the Human Brain using Multivariate Information Analysis</a></div><div id="abs">Abstract: <span id="abs">In this study, we present a method for estimating the mutual information for a localized pattern of fMRI data. We show that taking a multivariate information approach to voxel selection leads to a decoding accuracy that surpasses an univariate inforamtion approach and other standard voxel selection methods. Furthermore,we extend the multivariate mutual information theory to measure the functional connectivity between distributed brain regions. By jointly estimating the information shared by two sets of voxels we can reliably map out the connectivities in the human brain during experiment conditions. We validated our approach on a 6-way scene categorization fMRI experiment. The multivariate information analysis is able to ﬁnd strong information ﬂow between PPA and RSC, which conﬁrms existing neuroscience studies on scenes. Furthermore, by exploring over the whole brain, our method identifies other interesting ROIs that share information with the PPA, RSC scene network,suggesting interesting future work for neuroscientists.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">135</span></div><div id="author">Authors:<span id="author">Yicong Meng, Bertram E. Shi</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/884d247c6f65a96a7da4d1105d584ddd-Paper.pdf">Extending Phase Mechanism to Differential Motion Opponency for Motion Pop-out</a></div><div id="abs">Abstract: <span id="abs">We extend the concept of phase tuning, a ubiquitous mechanism in sensory neurons including motion and disparity detection neurons, to the motion contrast detection.  We demonstrate that motion contrast can be detected by phase shifts between motion neuronal responses in different spatial regions. By constructing the differential motion opponency in response to motions in two different spatial regions, varying motion contrasts can be detected, where similar motion is detected by zero phase shifts and differences in motion by non-zero phase shifts.  The model can exhibit either enhancement or suppression of responses by either different or similar motion in the surrounding.  A primary advantage of the model is that the responses are selective to relative motion instead of absolute motion, which could model neurons found in neurophysiological experiments responsible for motion pop-out detection.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">129</span></div><div id="author">Authors:<span id="author">Kaushik Sinha, Mikhail Belkin</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/82b8a3434904411a9fdc43ca87cee70c-Paper.pdf">Semi-supervised Learning using Sparse Eigenfunction Bases</a></div><div id="abs">Abstract: <span id="abs">We present a new framework for semi-supervised learning with sparse eigenfunction bases of kernel matrices.  It turns out that  when the \emph{cluster assumption} holds, that is, when the high density regions are sufficiently separated by  low density valleys, each high density area corresponds to a unique representative eigenvector. Linear combination of such eigenvectors (or, more precisely, of their Nystrom extensions) provide good candidates for good classification functions. By first choosing an appropriate basis of these eigenvectors from unlabeled data and then using labeled data  with Lasso to select a classifier in the span of these eigenvectors, we obtain a classifier, which has a very sparse representation in this basis. Importantly, the sparsity appears naturally from the  cluster assumption. Experimental results on a number  of real-world data-sets show that our method is competitive with the state of the art semi-supervised learning algorithms and outperforms the natural base-line algorithm (Lasso in the Kernel PCA basis).</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">136</span></div><div id="author">Authors:<span id="author">Chun-nan Hsu, Yu-ming Chang, Hanshen Huang, Yuh-jye Lee</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/884d79963bd8bc0ae9b13a1aa71add73-Paper.pdf">Periodic Step Size Adaptation for Single Pass On-line Learning</a></div><div id="abs">Abstract: <span id="abs">It has been established that the second-order stochastic gradient descent (2SGD) method can potentially achieve generalization performance as well as empirical optimum in a single pass (i.e., epoch) through the training examples. However, 2SGD requires computing the inverse of the Hessian matrix of the loss function, which is prohibitively expensive. This paper presents Periodic Step-size Adaptation (PSA), which approximates the Jacobian matrix of the mapping function and explores a linear relation between the Jacobian and Hessian to approximate the Hessian periodically and achieve near-optimal results in experiments on a wide variety of models and tasks.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">132</span></div><div id="author">Authors:<span id="author">Arthur Gretton, Peter Spirtes, Robert E. Tillman</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/83fa5a432ae55c253d0e60dbfa716723-Paper.pdf">Nonlinear directed acyclic structure learning with weakly additive noise models</a></div><div id="abs">Abstract: <span id="abs">The recently proposed \emph{additive noise model} has advantages over previous structure learning algorithms, when attempting to recover some true data generating mechanism, since it (i) does not assume linearity or Gaussianity and (ii) can recover a unique DAG rather than an equivalence class. However, its original extension to the multivariate case required enumerating all possible DAGs, and for some special distributions, e.g. linear Gaussian, the model is invertible and thus cannot be used for structure learning. We present a new approach which combines a PC style search using recent advances in kernel measures of conditional dependence with local searches for additive noise models in substructures of the equivalence class. This results in a more computationally efficient approach that is useful for arbitrary distributions even when additive noise models are invertible. Experiments with synthetic and real data show that this method is more accurate than previous methods when data are nonlinear and/or non-Gaussian.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">141</span></div><div id="author">Authors:<span id="author">Sundeep Rangan, Alyson K. Fletcher</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/8d6dc35e506fc23349dd10ee68dabb64-Paper.pdf">Orthogonal Matching Pursuit From Noisy Random Measurements: A New Analysis</a></div><div id="abs">Abstract: <span id="abs">Orthogonal matching pursuit (OMP) is a widely used greedy algorithm for recovering sparse vectors from linear measurements.  A well-known analysis of Tropp and Gilbert shows that OMP can recover a k-sparse n-dimensional real vector from m = 4k log(n) noise-free random linear measurements with a probability that goes to one as n goes to infinity. This work shows strengthens this result by showing that a lower number of measurements, m = 2k log(n-k), is in fact sufficient for asymptotic recovery. Moreover, this number of measurements is also sufficient for detection of the sparsity pattern (support) of the vector with measurement errors provided the signal-to-noise ratio (SNR) scales to infinity. The scaling m = 2k log(n-k) exactly matches the number of measurements required by the more complex lasso for signal recovery.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">139</span></div><div id="author">Authors:<span id="author">Siamac Fazli, Cristian Grozea, Marton Danoczy, Benjamin Blankertz, Florin Popescu, Klaus-Robert Müller</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/8a1e808b55fde9455cb3d8857ed88389-Paper.pdf">Subject independent EEG-based BCI decoding</a></div><div id="abs">Abstract: <span id="abs">In the quest to make Brain Computer Interfacing (BCI) more usable, dry electrodes have emerged that get rid of the initial 30 minutes required for placing an electrode cap.  Another time consuming step is the required individualized adaptation to the BCI user, which involves another 30 minutes calibration for assessing a subjects brain signature. In this paper we aim to also remove this calibration proceedure from BCI setup time by means of machine learning. In particular, we harvest a large database of EEG BCI motor imagination recordings (83 subjects) for constructing a library of subject-specific spatio-temporal filters and derive a subject independent BCI classifier. Our offline results indicate that BCI-na{i}ve users could start real-time BCI use with no prior calibration at only a very moderate performance loss."</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">137</span></div><div id="author">Authors:<span id="author">Peter Orbanz</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/892c91e0a653ba19df81a90f89d99bcd-Paper.pdf">Construction of Nonparametric Bayesian Models from Parametric Bayes Equations</a></div><div id="abs">Abstract: <span id="abs">We consider the general problem of constructing nonparametric   Bayesian models on infinite-dimensional random objects, such   as functions, infinite graphs or infinite permutations.   The problem has generated much interest in machine learning,   where it is treated heuristically, but has not been    studied in full generality in nonparametric Bayesian statistics, which tends to   focus on models over probability distributions.    Our approach applies a standard tool of stochastic process   theory, the construction of stochastic processes from their   finite-dimensional marginal distributions.    The main contribution of the paper is a generalization   of the classic Kolmogorov extension theorem to conditional   probabilities.   This extension allows a rigorous construction of nonparametric Bayesian models   from systems of finite-dimensional, parametric Bayes equations.   Using this approach, we show (i)   how existence of a conjugate posterior for    the nonparametric model can be guaranteed by choosing   conjugate finite-dimensional models in the construction, (ii) how the   mapping to the posterior parameters of the nonparametric   model can be explicitly determined, and (iii) that   the construction of conjugate models in essence requires the   finite-dimensional models to be in the exponential family.   As an application of our constructive framework,    we derive a model on infinite   permutations, the nonparametric Bayesian analogue of a model   recently proposed for the analysis of rank data.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">138</span></div><div id="author">Authors:<span id="author">Zenglin Xu, Rong Jin, Jianke Zhu, Irwin King, Michael Lyu, Zhirong Yang</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/8a0e1141fd37fa5b98d5bb769ba1a7cc-Paper.pdf">Adaptive Regularization for Transductive Support Vector Machine</a></div><div id="abs">Abstract: <span id="abs">We discuss the framework of Transductive Support Vector Machine (TSVM) from the perspective of the regularization strength induced by the unlabeled data. In this framework, SVM and TSVM can be regarded as a learning machine without regularization and one with full regularization from the unlabeled data, respectively. Therefore, to supplement this framework of the regularization strength, it is necessary to introduce data-dependant partial regularization. To this end, we reformulate TSVM into a form with controllable regularization strength, which includes SVM and TSVM as special cases. Furthermore, we introduce a method of adaptive regularization that is data dependant and is based on the smoothness assumption. Experiments on a set of benchmark data sets indicate the promising results of the proposed work compared with state-of-the-art TSVM algorithms.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">140</span></div><div id="author">Authors:<span id="author">Gert R. Lanckriet, Bharath K. Sriperumbudur</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/8b5040a8a5baf3e0e67386c2e3a9b903-Paper.pdf">On the Convergence of the Concave-Convex Procedure</a></div><div id="abs">Abstract: <span id="abs">The concave-convex procedure (CCCP) is a majorization-minimization algorithm that solves d.c. (difference of convex functions) programs as a sequence of convex programs. In machine learning, CCCP is extensively used in many learning algorithms like sparse support vector machines (SVMs), transductive SVMs, sparse principal component analysis, etc. Though widely used in many applications, the convergence behavior of CCCP has not gotten a lot of specific attention. Yuille and Rangarajan analyzed its convergence in their original paper, however, we believe the analysis is not complete. Although the convergence of CCCP can be derived from the convergence of the d.c. algorithm (DCA), their proof is more specialized and technical than actually required for the specific case of CCCP. In this paper, we follow a different reasoning and show how Zangwills global convergence theory of iterative algorithms provides a natural framework to prove the convergence of CCCP, allowing a more elegant and simple proof. This underlines Zangwills theory as a powerful and general framework to deal with the convergence issues of iterative algorithms, after also being used to prove the convergence of algorithms like expectation-maximization, generalized alternating minimization, etc. In this paper, we provide a rigorous analysis of the convergence of CCCP by addressing these questions: (i) When does CCCP find a local minimum or a stationary point of the d.c. program under consideration? (ii) When does the sequence generated by CCCP converge? We also present an open problem on the issue of local convergence of CCCP.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">147</span></div><div id="author">Authors:<span id="author">Arthur Gretton, Kenji Fukumizu, Zaïd Harchaoui, Bharath K. Sriperumbudur</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/9246444d94f081e3549803b928260f56-Paper.pdf">A Fast, Consistent Kernel Two-Sample Test</a></div><div id="abs">Abstract: <span id="abs">A kernel embedding of probability distributions into reproducing kernel Hilbert spaces (RKHS) has recently been proposed, which  allows the comparison of two probability measures P and Q based on the distance between their respective embeddings: for a sufficiently rich RKHS, this distance is zero if and only if P and Q coincide. In using this distance as a statistic for a  test of whether two samples are from different distributions, a major difficulty arises in computing the significance threshold, since the empirical statistic has as its null distribution (where P=Q)  an infinite weighted sum of $\chi^2$ random variables. The main result of the present work is a  novel, consistent estimate of this null distribution, computed from  the eigenspectrum of the Gram matrix on the aggregate sample from P and Q. This estimate may be computed faster than a previous consistent estimate based on the bootstrap.  Another prior approach was to compute the null distribution based on fitting a parametric family with the low order moments of the test statistic: unlike the present work, this heuristic has no guarantee of being accurate or consistent. We verify the performance of our null distribution estimate on both   an artificial example and on high dimensional multivariate data.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">134</span></div><div id="author">Authors:<span id="author">James Petterson, Jin Yu, Julian J. Mcauley, Tibério S. Caetano</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/85d8ce590ad8981ca2c8286f79f59954-Paper.pdf">Exponential Family Graph Matching and Ranking</a></div><div id="abs">Abstract: <span id="abs">We present a method for learning max-weight matching predictors in bipartite graphs. The method consists of performing maximum a posteriori estimation in exponential families with sufficient statistics that encode permutations and data features. Although inference is in general hard, we show that for one very relevant application - document ranking - exact inference is efficient. For general model instances, an appropriate sampler is readily available. Contrary to existing max-margin matching models, our approach is statistically consistent and, in addition, experiments with increasing sample sizes indicate superior improvement over such models. We apply the method to graph matching in computer vision as well as to a standard benchmark dataset for learning document ranking, in which we obtain state-of-the-art results, in particular improving on max-margin variants. The drawback of this method with respect to max-margin alternatives is its runtime for large graphs, which is high comparatively.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">166</span></div><div id="author">Authors:<span id="author">Sanjiv Kumar, Mehryar Mohri, Ameet Talwalkar</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/a49e9411d64ff53eccfdd09ad10a15b3-Paper.pdf">Ensemble Nystrom Method</a></div><div id="abs">Abstract: <span id="abs">A crucial technique for scaling kernel methods to very large data sets reaching or exceeding millions of instances is based on low-rank approximation of kernel matrices. We introduce a new family of algorithms based on mixtures of Nystrom approximations, ensemble Nystrom algorithms, that yield more accurate low-rank approximations than the standard Nystrom method. We give a detailed study of multiple variants of these algorithms based on simple averaging, an exponential weight method, or regression-based methods. We also present a theoretical analysis of these algorithms, including novel error bounds guaranteeing a better convergence rate than the standard Nystrom method. Finally, we report the results of extensive experiments with several data sets containing up to 1M points demonstrating the signiﬁcant performance improvements gained over the standard Nystrom approximation.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">144</span></div><div id="author">Authors:<span id="author">Kuzman Ganchev, Ben Taskar, Fernando Pereira, João Gama</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/8f1d43620bc6bb580df6e80b0dc05c48-Paper.pdf">Posterior vs Parameter Sparsity in Latent Variable Models</a></div><div id="abs">Abstract: <span id="abs">In this paper we explore the problem of biasing unsupervised models to favor sparsity. We extend the posterior regularization framework [8] to encourage the  model to achieve posterior sparsity on the unlabeled training data. We apply this new method to learn ﬁrst-order HMMs for unsupervised part-of-speech (POS) tagging, and show that HMMs learned this way consistently and signiﬁcantly out-performs both EM-trained HMMs, and HMMs with a sparsity-inducing Dirichlet prior trained by variational EM. We evaluate these HMMs on three languages — English, Bulgarian and Portuguese — under four conditions. We ﬁnd that our  method always improves performance with respect to both baselines, while variational Bayes actually degrades performance in most cases. We increase accuracy with respect to EM by 2.5%-8.7% absolute and we see improvements even in a semisupervised condition where a limited dictionary is provided.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">13</span></div><div id="author">Authors:<span id="author">Alessandro Perina, Marco Cristani, Umberto Castellani, Vittorio Murino, Nebojsa Jojic</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf">Free energy score space</a></div><div id="abs">Abstract: <span id="abs">Score functions induced by generative models extract fixed-dimension feature vectors from different-length data observations by subsuming the process of data generation, projecting them in highly informative spaces called score spaces. In this way, standard discriminative classifiers are proved to achieve higher performances than a solely generative or discriminative approach. In this paper, we present a novel score space that exploits the free energy associated to a generative model through a score function. This function aims at capturing both the uncertainty of the model learning and ``local compliance  of data observations with respect to the generative process. Theoretical justifications and convincing comparative classification results on various generative models prove the goodness of the proposed strategy.</span></div>d for the special case of sparse online learning using L1-regularization.</p><p id="section"><div id="pid">Paperid:<span id="pid">124</span></div><div id="author">Authors:<span id="author">Mladen Kolar, Le Song, Eric P. Xing</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/7bcdf75ad237b8e02e301f4091fb6bc8-Paper.pdf">Sparsistent Learning of Varying-coefficient Models with Structural Changes</a></div><div id="abs">Abstract: <span id="abs">To estimate the changing structure of a varying-coefficient   varying-structure (VCVS) model remains an important and open problem   in dynamic system modelling, which includes learning trajectories of   stock prices, or uncovering the topology of an evolving gene   network. In this paper, we investigate sparsistent learning of a   sub-family of this model --- piecewise constant VCVS models. We   analyze two main issues in this problem: inferring time points where   structural changes occur and estimating model structure (i.e., model   selection) on each of the constant segments. We propose a two-stage   adaptive procedure, which first identifies jump points of structural   changes and then identifies relevant covariates to a response on   each of the segments. We provide an asymptotic analysis of the   procedure, showing that with the increasing sample size, number of   structural changes, and number of variables, the true model can be   consistently selected. We demonstrate the performance of the method   on synthetic data and apply it to the brain computer interface   dataset. We also consider how this applies to structure estimation   of time-varying probabilistic graphical models.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">179</span></div><div id="author">Authors:<span id="author">Raghunandan Keshavan, Andrea Montanari, Sewoong Oh</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/aa942ab2bfa6ebda4840e7360ce6e7ef-Paper.pdf">Matrix Completion from Noisy Entries</a></div><div id="abs">Abstract: <span id="abs">Given a matrix M of low-rank, we consider the problem of reconstructing it from noisy observations of a small, random subset of its entries. The problem arises in a variety of applications, from collaborative filtering (the ‘Netflix problem’) to structure-from-motion and positioning. We study a low complexity algorithm introduced in [1], based on a combination of spectral techniques and manifold optimization, that we call here OPTSPACE. We prove performance guarantees that are order-optimal in a number of circumstances.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">148</span></div><div id="author">Authors:<span id="author">Matthias Hein</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/92977ae4d2ba21425a59afb269c2a14e-Paper.pdf">Robust Nonparametric Regression with Metric-Space Valued Output</a></div><div id="abs">Abstract: <span id="abs">Motivated by recent developments in manifold-valued regression we propose a family of nonparametric kernel-smoothing estimators with metric-space valued output including a robust median type estimator and the classical Frechet mean. Depending on the choice of the output space and the chosen metric the estimator reduces to partially well-known procedures for multi-class classification, multivariate regression in Euclidean space, regression with manifold-valued output and even some cases of structured output learning. In this paper we focus on the case of regression with manifold-valued input and output. We show pointwise and Bayes consistency for all estimators in the family for the case of manifold-valued output and illustrate the robustness properties of the estimator with experiments.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">126</span></div><div id="author">Authors:<span id="author">Yiming Ying, Colin Campbell, Mark Girolami</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/7f1de29e6da19d22b51c68001e7e0e54-Paper.pdf">Analysis of SVM with Indefinite Kernels</a></div><div id="abs">Abstract: <span id="abs">The recent introduction of indefinite SVM by Luss and dAspremont [15] has effectively demonstrated SVM classification with a non-positive semi-definite  kernel (indefinite kernel).  This paper studies the properties of the objective function introduced there. In particular, we show that the objective function  is continuously differentiable and its gradient can be explicitly computed. Indeed, we further show that its gradient is Lipschitz continuous.  The main idea behind our analysis is that the objective function is smoothed by the penalty term, in its saddle (min-max) representation, measuring the distance between the indefinite kernel matrix and the proxy positive semi-definite one. Our elementary result greatly facilitates the application of gradient-based algorithms. Based on our analysis,  we further develop  Nesterovs smooth optimization approach [16,17] for indefinite SVM which has an optimal convergence rate for smooth problems. Experiments on various benchmark datasets validate our analysis and demonstrate the efficiency of our proposed algorithms.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">122</span></div><div id="author">Authors:<span id="author">Lei Shi, Thomas L. Griffiths</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/754dda4b1ba34c6fa89716b85d68532b-Paper.pdf">Neural Implementation of Hierarchical Bayesian Inference by Importance Sampling</a></div><div id="abs">Abstract: <span id="abs">The goal of perception is to infer the hidden states in the hierarchical process by which sensory data are generated. Human behavior is consistent with the optimal statistical solution to this problem in many tasks, including cue combination and orientation detection. Understanding the neural mechanisms underlying this behavior is of particular importance, since probabilistic computations are notoriously challenging. Here we propose a simple mechanism for Bayesian inference which involves averaging over a few feature detection neurons which fire at a rate determined by their similarity to a sensory stimulus. This mechanism is based on a Monte Carlo method known as importance sampling, commonly used in computer science and statistics. Moreover, a simple extension to recursive importance sampling can be used to perform hierarchical Bayesian inference. We identify a scheme for implementing importance sampling with spiking neurons, and show that this scheme can account for human behavior in cue combination and oblique effect.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">127</span></div><div id="author">Authors:<span id="author">Charles Kemp</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/82161242827b703e6acf9c726942a1e4-Paper.pdf">Quantification and the language of thought</a></div><div id="abs">Abstract: <span id="abs">Many researchers have suggested that the psychological complexity of a concept is related to the length of its representation in a language of thought.  As yet, however, there are few concrete proposals about the nature of this language. This paper makes one such proposal: the language of thought allows first order quantification (quantification over objects) more readily than second-order quantification (quantification over features). To support this proposal we present behavioral results from a concept learning study inspired by the work of Shepard, Hovland and Jenkins."</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">149</span></div><div id="author">Authors:<span id="author">Peter Sollich, Matthew Urry, Camille Coti</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/92cc227532d17e56e07902b254dfad10-Paper.pdf">Kernels and learning curves for Gaussian process regression on random graphs</a></div><div id="abs">Abstract: <span id="abs">We investigate how well Gaussian process regression can learn functions defined on graphs, using large regular random graphs as a paradigmatic example. Random-walk based kernels are shown to have some surprising properties: within the standard approximation of a locally tree-like graph structure, the kernel does not become constant, i.e.neighbouring function values do not become fully correlated, when the lengthscale $\sigma$ of the kernel is made large. Instead the kernel attains a non-trivial limiting form, which we calculate. The fully correlated limit is reached only once loops become relevant, and we estimate where the crossover to this regime occurs. Our main subject are learning curves of Bayes error versus training set size. We show that these are qualitatively well predicted by a simple approximation using only the spectrum of a large tree as input, and generically scale with $n/V$, the number of training examples per vertex. We also explore how this behaviour changes once kernel lengthscales are large enough for loops to become important.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">145</span></div><div id="author">Authors:<span id="author">Tao Hu, Anthony Leonardo, Dmitri B. Chklovskii</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/8f7d807e1f53eff5f9efbe5cb81090fb-Paper.pdf">Reconstruction of Sparse Circuits Using Multi-neuronal Excitation (RESCUME)</a></div><div id="abs">Abstract: <span id="abs">One of the central problems in neuroscience is reconstructing synaptic connectivity in neural circuits. Synapses onto a neuron can be probed by sequentially stimulating potentially pre-synaptic neurons while monitoring the membrane voltage of the post-synaptic neuron. Reconstructing a large neural circuit using such a “brute force” approach is rather time-consuming and inefficient because the connectivity in neural circuits is sparse. Instead, we propose to measure a post-synaptic neuron’s voltage while stimulating simultaneously multiple randomly chosen potentially pre-synaptic neurons. To extract the weights of individual synaptic connections we apply a decoding algorithm recently developed for compressive sensing. Compared to the brute force approach, our method promises significant time savings that grow with the size of the circuit. We use computer simulations to find optimal stimulation parameters and explore the feasibility of our reconstruction method under realistic experimental conditions including noise and non-linear synaptic integration. Multiple-neuron stimulation allows reconstructing synaptic connectivity just from the spiking activity of post-synaptic neurons, even when sub-threshold voltage is unavailable. By using calcium indicators, voltage-sensitive dyes, or multi-electrode arrays one could monitor activity of multiple post-synaptic neurons simultaneously, thus mapping their synaptic inputs in parallel, potentially reconstructing a complete neural circuit.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">171</span></div><div id="author">Authors:<span id="author">Le Song, Mladen Kolar, Eric P. Xing</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/a67f096809415ca1c9f112d96d27689b-Paper.pdf">Time-Varying Dynamic Bayesian Networks</a></div><div id="abs">Abstract: <span id="abs">Directed graphical models such as Bayesian networks are a favored formalism to model the dependency structures in complex multivariate systems such as those encountered in biology and neural sciences. When the system is undergoing dynamic transformation, often a temporally rewiring network is needed for capturing the dynamic causal influences between covariates. In this paper, we propose a time-varying dynamic Bayesian network (TV-DBN) for modeling the structurally varying directed dependency structures underlying non-stationary biological/neural time series. This is a challenging problem due the non-stationarity and sample scarcity of the time series. We present a kernel reweighted $\ell_1$ regularized auto-regressive procedure for learning the TV-DBN model. Our method enjoys nice properties such as computational efficiency and provable asymptotic consistency. Applying TV-DBN to time series measurements during yeast cell cycle and brain response to visual stimuli reveals interesting dynamics underlying the respective biological systems.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">151</span></div><div id="author">Authors:<span id="author">Tomasz Malisiewicz, Alyosha Efros</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/93db85ed909c13838ff95ccfa94cebd9-Paper.pdf">Beyond Categories: The Visual Memex Model for Reasoning About Object Relationships</a></div><div id="abs">Abstract: <span id="abs">The use of context is critical for scene understanding in computer vision, where the recognition of an object is driven by both local appearance and the objects relationship to other elements of the scene (context).  Most current approaches rely on modeling the relationships between object categories as a source of context. In this paper we seek to move beyond categories to provide a richer appearance-based model of context.  We present an exemplar-based model of objects and their relationships, the Visual Memex, that encodes both local appearance and 2D spatial context between object instances. We evaluate our model on Torralbas proposed Context Challenge against a baseline category-based system. Our experiments suggest that moving beyond categories for context modeling appears to be quite beneficial, and may be the critical missing ingredient in scene understanding systems.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">153</span></div><div id="author">Authors:<span id="author">Charles Kemp, Alan Jern</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/96da2f590cd7246bbde0051047b0d6f7-Paper.pdf">Abstraction and Relational learning</a></div><div id="abs">Abstract: <span id="abs">Many categories are better described by providing relational information than listing characteristic features.  We present a hierarchical generative model that helps to explain how relational categories are learned and used. Our model learns abstract schemata that specify the relational similarities shared by members of a category, and our emphasis on abstraction departs from previous theoretical proposals that focus instead on comparison of concrete instances. Our first experiment suggests that our abstraction-based account can address some of the tasks that have previously been used to support comparison-based approaches. Our second experiment focuses on one-shot schema learning, a problem that raises challenges for comparison-based approaches but is handled naturally by our abstraction-based account.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">183</span></div><div id="author">Authors:<span id="author">Garvesh Raskutti, Bin Yu, Martin J. Wainwright</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/b1563a78ec59337587f6ab6397699afc-Paper.pdf">Lower bounds on minimax rates for nonparametric regression with additive sparsity and smoothness</a></div><div id="abs">Abstract: <span id="abs">This paper uses information-theoretic techniques to determine minimax rates for estimating nonparametric sparse additive regression models under high-dimensional scaling.  We assume an additive decomposition of the form $f^*(X_1, \ldots, X_p) = \sum_{j \in S} h_j(X_j)$, where each component function $h_j$ lies in some Hilbert Space $\Hilb$ and $S \subset \{1, \ldots, \pdim \}$ is an unknown subset with cardinality $\s = |S$.  Given $\numobs$ i.i.d. observations of $f^*(X)$ corrupted with white Gaussian noise where the covariate vectors $(X_1, X_2, X_3,...,X_{\pdim})$ are drawn with i.i.d. components from some distribution $\mP$, we determine tight lower bounds on the minimax rate for estimating the regression function with respect to squared $\LTP$ error. The main result shows that the minimax rates are $\max{\big(\frac{\s \log \pdim / \s}{n}, \LowerRateSq \big)}$.  The first term reflects the difficulty of performing \emph{subset selection} and is independent of the Hilbert space $\Hilb$; the second term $\LowerRateSq$ is an \emph{\s-dimensional estimation} term, depending only on the low dimension $\s$ but not the ambient dimension $\pdim$, that captures the difficulty of estimating a sum of $\s$ univariate functions in the Hilbert space $\Hilb$.  As a special case, if $\Hilb$ corresponds to the $\m$-th order Sobolev space $\SobM$ of functions that are $m$-times differentiable, the $\s$-dimensional estimation term takes the form $\LowerRateSq \asymp \s \; n^{-2\m/(2\m+1)}$. The minimax rates are compared with rates achieved by an $\ell_1$-penalty based approach, it can be shown that a certain $\ell_1$-based approach achieves the minimax optimal rate.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">193</span></div><div id="author">Authors:<span id="author">Sundeep Rangan, Vivek Goyal, Alyson K. Fletcher</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/bdb106a0560c4e46ccc488ef010af787-Paper.pdf">Asymptotic Analysis of MAP Estimation via the Replica Method and Compressed Sensing</a></div><div id="abs">Abstract: <span id="abs">The replica method is a non-rigorous but widely-used technique from statistical physics used in the asymptotic analysis of many large random nonlinear problems.  This paper applies the replica method to non-Gaussian MAP estimation.  It is shown that with large random linear measurements and Gaussian noise, the asymptotic behavior of the MAP estimate of an n-dimensional vector ``decouples as n scalar MAP estimators.  The result is a counterpart to Guo and Verdus replica analysis on MMSE estimation. The replica MAP analysis can be readily applied to many estimators used in compressed sensing, including basis pursuit, lasso, linear estimation with thresholding and zero-norm estimation.  In the case of lasso estimation, the scalar estimator reduces to a soft-thresholding operator and for zero-norm estimation it reduces to a hard-threshold.  Among other benefits, the replica method provides a computationally tractable method for exactly computing various performance metrics including MSE and sparsity recovery.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">165</span></div><div id="author">Authors:<span id="author">Keith Bush, Joelle Pineau</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/a2557a7b2e94197ff767970b67041697-Paper.pdf">Manifold Embeddings for Model-Based Reinforcement Learning under Partial Observability</a></div><div id="abs">Abstract: <span id="abs">Interesting real-world datasets often exhibit nonlinear, noisy, continuous-valued states that are unexplorable, are poorly described by first principles, and are only partially observable. If partial observability can be overcome, these constraints suggest the use of model-based reinforcement learning.  We experiment with manifold embeddings as the reconstructed observable state-space of an off-line, model-based reinforcement learning approach to control. We demonstrate the embedding of a system changes as a result of learning and that the best performing embeddings well-represent the dynamics of both the uncontrolled and adaptively controlled system.  We apply this approach in simulation to learn a neurostimulation policy that is more efficient in treating epilepsy than conventional policies.  We then demonstrate the learned policy completely suppressing seizures in real-world neurostimulation experiments on actual animal brain slices.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">182</span></div><div id="author">Authors:<span id="author">Theodore J. Perkins</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/afda332245e2af431fb7b672a68b659d-Paper.pdf">Maximum likelihood trajectories for continuous-time Markov chains</a></div><div id="abs">Abstract: <span id="abs">Continuous-time Markov chains are used to model systems in which transitions between states as well as the time the system spends in each state are random.  Many computational problems related to such chains have been solved, including determining state distributions as a function of time, parameter estimation, and control.  However, the problem of inferring most likely trajectories, where a trajectory is a sequence of states as well as the amount of time spent in each state, appears unsolved.  We study three versions of this problem: (i) an initial value problem, in which an initial state is given and we seek the most likely trajectory until a given final time, (ii) a boundary value problem, in which initial and final states and times are given, and we seek the most likely trajectory connecting them, and (iii) trajectory inference under partial observability, analogous to finding maximum likelihood trajectories for hidden Markov models.  We show that maximum likelihood trajectories are not always well-defined, and describe a polynomial time test for well-definedness.  When well-definedness holds, we show that each of the three problems can be solved in polynomial time, and we develop efficient dynamic programming algorithms for doing so.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">154</span></div><div id="author">Authors:<span id="author">Xiao-ming Wu, Anthony M. So, Zhenguo Li, Shuo-yen R. Li</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/96ea64f3a1aa2fd00c72faacf0cb8ac9-Paper.pdf">Fast Graph Laplacian Regularized Kernel Learning via Semidefinite–Quadratic–Linear Programming</a></div><div id="abs">Abstract: <span id="abs">Kernel learning is a powerful framework for nonlinear data modeling. Using the kernel trick, a number of problems have been formulated as semidefinite programs (SDPs). These include Maximum Variance Unfolding (MVU) (Weinberger et al., 2004) in nonlinear dimensionality reduction, and Pairwise Constraint Propagation (PCP) (Li et al., 2008) in constrained clustering. Although in theory SDPs can be efficiently solved, the high computational complexity incurred in numerically processing the huge linear matrix inequality constraints has rendered the SDP approach unscalable. In this paper, we show that a large class of kernel learning problems can be reformulated as semidefinite-quadratic-linear programs (SQLPs), which only contain a simple positive semidefinite constraint, a second-order cone constraint and a number of linear constraints. These constraints are much easier to process numerically, and the gain in speedup over previous approaches is at least of the order $m^{2.5}$, where m is the matrix dimension. Experimental results are also presented to show the superb computational efficiency of our approach.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">161</span></div><div id="author">Authors:<span id="author">Bryan Conroy, Ben Singer, James Haxby, Peter J. Ramadge</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/9f396fe44e7c05c16873b05ec425cbad-Paper.pdf">fMRI-Based Inter-Subject Cortical Alignment Using Functional Connectivity</a></div><div id="abs">Abstract: <span id="abs">The inter-subject alignment of functional MRI (fMRI) data is important for improving the statistical power of fMRI group analyses. In contrast to existing anatomically-based methods, we propose a novel multi-subject algorithm that derives a functional correspondence by aligning spatial patterns of functional connectivity across a set of subjects. We test our method on fMRI data collected during a movie viewing experiment. By cross-validating the results of our algorithm, we show that the correspondence successfully generalizes to a secondary movie dataset not used to derive the alignment.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">155</span></div><div id="author">Authors:<span id="author">Vivek Farias, Srikanth Jagabathula, Devavrat Shah</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/98b297950041a42470269d56260243a1-Paper.pdf">A Data-Driven Approach to Modeling Choice</a></div><div id="abs">Abstract: <span id="abs">We visit the following fundamental problem: For a `generic model of consumer choice (namely, distributions over preference lists) and a limited amount of data on how consumers actually make decisions (such as marginal preference information), how may one predict revenues from offering a particular assortment of choices? This problem is central to areas within operations research, marketing and econometrics. We present a framework to answer such questions and design a number of tractable algorithms (from a data and computational standpoint) for the same.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">194</span></div><div id="author">Authors:<span id="author">Ruben Coen-cagli, Peter Dayan, Odelia Schwartz</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/be3159ad04564bfb90db9e32851ebf9c-Paper.pdf">Statistical Models of Linear and Nonlinear Contextual Interactions in Early Visual Processing</a></div><div id="abs">Abstract: <span id="abs">A central hypothesis about early visual processing is that it represents inputs in a coordinate system matched to the statistics of natural scenes. Simple versions of this lead to Gabor-like receptive fields and divisive gain modulation from local surrounds; these have led to influential neural and psychological models of visual processing. However, these accounts are based on an incomplete view of the visual context surrounding each point. Here, we consider an approximate model of linear and non-linear correlations between the responses of spatially distributed Gabor-like receptive fields, which, when trained on an ensemble of natural scenes, unifies a range of spatial context effects. The full model accounts for neural surround data in primary visual cortex (V1), provides a statistical foundation for perceptual phenomena associated with Lis (2002) hypothesis that V1 builds a saliency map, and fits data on the tilt illusion.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">184</span></div><div id="author">Authors:<span id="author">Liwei Wang</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/b1d10e7bafa4421218a51b1e1f1b0ba2-Paper.pdf">Sufficient Conditions for Agnostic Active Learnable</a></div><div id="abs">Abstract: <span id="abs">We study pool-based active learning in the presence of noise, i.e. the agnostic setting. Previous works have shown that the effectiveness of agnostic active learning depends on the learning problem and the hypothesis space. Although there are many cases on which active learning is very useful, it is also easy to construct examples that no active learning algorithm can have advantage. In this paper, we propose intuitively reasonable sufficient conditions under which agnostic active learning algorithm is strictly superior to passive supervised learning. We show that under some noise condition, if the classification boundary and the underlying distribution are smooth to a finite order, active learning achieves polynomial improvement in the label complexity; if the boundary and the distribution are infinitely smooth, the improvement is exponential.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">199</span></div><div id="author">Authors:<span id="author">John Wright, Arvind Ganesh, Shankar Rao, Yigang Peng, Yi Ma</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/c45147dee729311ef5b5c3003946c48f-Paper.pdf">Robust Principal Component Analysis: Exact Recovery of Corrupted Low-Rank Matrices via Convex Optimization</a></div><div id="abs">Abstract: <span id="abs">Principal component analysis is a fundamental operation in computational data analysis, with myriad applications ranging from web search to bioinformatics to computer vision and image analysis. However, its performance and applicability in real scenarios are limited by a lack of robustness to outlying or corrupted observations. This paper considers the idealized “robust principal component analysis” problem of recovering a low rank matrix A from corrupted observations D = A + E. Here, the error entries E can be arbitrarily large (modeling grossly corrupted observations common in visual and bioinformatic data), but are assumed to be sparse. We prove that most matrices A can be efficiently and exactly recovered from most error sign-and-support patterns, by solving a simple convex program. Our result holds even when the rank of A grows nearly proportionally (up to a logarithmic factor) to the dimensionality of the observation space and the number of errors E grows in proportion to the total number of entries in the matrix. A by-product of our analysis is the first proportional growth results for the related problem of completing a low-rank matrix from a small fraction of its entries. Simulations and real-data examples corroborate the theoretical results, and suggest potential applications in computer vision.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">176</span></div><div id="author">Authors:<span id="author">Ingo Steinwart, Andreas Christmann</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/a89cf525e1d9f04d16ce31165e139a4b-Paper.pdf">Fast Learning from Non-i.i.d. Observations</a></div><div id="abs">Abstract: <span id="abs">We prove an oracle inequality for generic regularized empirical risk minimization algorithms learning from  $\a$-mixing processes. To illustrate this oracle inequality, we use it to derive learning rates for some learning methods including least squares SVMs. Since the proof of the oracle inequality uses recent localization ideas developed for independent and identically distributed (i.i.d.) processes, it turns out that these learning rates are close to the optimal rates known in the i.i.d. case.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">192</span></div><div id="author">Authors:<span id="author">Zahi Karam, Douglas Sturim, William M. Campbell</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/bac9162b47c56fc8a4d2a519803d51b3-Paper.pdf">Speaker Comparison with Inner Product Discriminant Functions</a></div><div id="abs">Abstract: <span id="abs">Speaker comparison, the process of finding the speaker similarity between two speech signals, occupies a central role in a variety of applications---speaker verification, clustering, and identification. Speaker comparison can be placed in a geometric framework by casting the problem as a model comparison process.  For a given speech signal, feature vectors are produced and used to adapt a Gaussian mixture model (GMM).  Speaker comparison can then be viewed as the process of compensating and finding metrics on the space of adapted models.  We propose a framework, inner product discriminant functions (IPDFs), which extends many common techniques for speaker comparison: support vector machines, joint factor analysis, and linear scoring.  The framework uses inner products between the parameter vectors of GMM models motivated by several statistical methods.  Compensation of nuisances is performed via linear transforms on GMM parameter vectors.  Using the IPDF framework, we show that many current techniques are simple variations of each other.  We demonstrate, on a 2006 NIST speaker recognition evaluation task, new scoring methods using IPDFs which produce excellent error rates and require significantly less computation than current techniques.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">156</span></div><div id="author">Authors:<span id="author">Manqi Zhao, Venkatesh Saligrama</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/996a7fa078cc36c46d02f9af3bef918b-Paper.pdf">Anomaly Detection with Score functions based on Nearest Neighbor Graphs</a></div><div id="abs">Abstract: <span id="abs">We propose a novel non-parametric adaptive anomaly detection algorithm for high dimensional data based on score functions derived from nearest neighbor graphs on n-point nominal data. Anomalies are declared whenever the score of a test sample falls below q, which is supposed to be the desired false alarm level. The resulting anomaly detector is shown to be asymptotically optimal in that it is uniformly most powerful for the specified false alarm level, q, for the case when the anomaly density is a mixture of the nominal and a known density. Our algorithm is computationally efficient, being linear in dimension and quadratic in data size. It does not require choosing complicated tuning parameters or function approximation classes and it can adapt to local structure such as local change in dimensionality. We demonstrate the algorithm on both artificial and real data sets in high dimensional feature spaces.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">189</span></div><div id="author">Authors:<span id="author">Yusuke Watanabe, Kenji Fukumizu</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/b6f0479ae87d244975439c6124592772-Paper.pdf">Graph Zeta Function in the Bethe Free Energy and Loopy Belief Propagation</a></div><div id="abs">Abstract: <span id="abs">We propose a new approach to the analysis of Loopy Belief Propagation (LBP) by establishing a formula that connects the Hessian of the Bethe free energy with the edge zeta function. The formula has a number of theoretical implications on LBP. It is applied to give a sufficient condition that the Hessian of the Bethe free energy is positive definite, which shows non-convexity for graphs with multiple cycles. The formula clarifies the relation between the local stability of a fixed point of LBP and local minima of the Bethe free energy. We also propose a new approach to the uniqueness of LBP fixed point, and show various conditions of uniqueness.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">163</span></div><div id="author">Authors:<span id="author">Baback Moghaddam, Mohammad Emtiyaz Khan, Kevin P. Murphy, Benjamin M. Marlin</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/a1519de5b5d44b31a01de013b9b51a80-Paper.pdf">Accelerating Bayesian Structural Inference for Non-Decomposable Gaussian Graphical Models</a></div><div id="abs">Abstract: <span id="abs">In this paper we make several contributions towards accelerating approximate Bayesian structural inference for non-decomposable GGMs. Our first contribution is to show how to efficiently compute a BIC or Laplace approximation to the marginal likelihood of non-decomposable graphs using convex methods for precision matrix estimation. This optimization technique can be used as a fast scoring function inside standard Stochastic Local Search (SLS) for generating posterior samples. Our second contribution is a novel framework for efficiently generating large sets of high-quality graph topologies without performing local search. This graph proposal method, which we call Neighborhood Fusion" (NF), samples candidate Markov blankets at each node using sparse regression techniques. Our final contribution is a hybrid method combining the complementary strengths of NF and SLS. Experimental results in structural recovery and prediction tasks demonstrate that NF and hybrid NF/SLS out-perform state-of-the-art local search methods, on both synthetic and real-world datasets, when realistic computational limits are imposed."</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">169</span></div><div id="author">Authors:<span id="author">Maxim Raginsky, Svetlana Lazebnik</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/a5e00132373a7031000fd987a3c9f87b-Paper.pdf">Locality-sensitive binary codes from shift-invariant kernels</a></div><div id="abs">Abstract: <span id="abs">This paper addresses the problem of designing binary codes for high-dimensional data such that vectors that are similar in the original space map to similar binary strings. We introduce a simple distribution-free encoding scheme based on random projections, such that the expected Hamming distance between the binary codes of two vectors is related to the value of a shift-invariant kernel (e.g., a Gaussian kernel) between the vectors. We present a full theoretical analysis of the convergence properties of the proposed scheme, and report favorable experimental performance as compared to a recent state-of-the-art method, spectral hashing.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">175</span></div><div id="author">Authors:<span id="author">Gunhee Kim, Antonio Torralba</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/a87ff679a2f3e71d9181a67b7542122c-Paper.pdf">Unsupervised Detection of Regions of Interest Using Iterative Link Analysis</a></div><div id="abs">Abstract: <span id="abs">This paper proposes a fast and scalable alternating optimization technique to detect regions of interest (ROIs) in cluttered Web images without labels. The proposed approach discovers highly probable regions of  object instances by iteratively repeating the following two functions: (1) choose the exemplar set (i.e. small number of high ranked reference ROIs) across the dataset and (2) refine the ROIs of each image with respect to the exemplar set. These two subproblems are formulated as ranking in two different similarity networks of ROI hypotheses by link analysis. The experiments with the PASCAL 06 dataset show that our unsupervised localization performance is better than one of state-of-the-art techniques and comparable to supervised methods. Also, we test the scalability of our approach with five objects in Flickr dataset consisting of more than 200,000 images.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">152</span></div><div id="author">Authors:<span id="author">Nan Ye, Wee S. Lee, Hai L. Chieu, Dan Wu</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/94f6d7e04a4d452035300f18b984988c-Paper.pdf">Conditional Random Fields with High-Order Features for Sequence Labeling</a></div><div id="abs">Abstract: <span id="abs">Dependencies among neighbouring labels in a sequence is an important source of information for sequence labeling problems. However, only dependencies between adjacent labels are commonly exploited in practice because of the high computational complexity of typical inference algorithms when longer distance dependencies are taken into account. In this paper, we show that it is possible to design efficient inference algorithms for a conditional random field using features that depend on long consecutive label sequences (high-order features), as long as the number of distinct label sequences in the features used is small. This leads to efficient learning algorithms for these conditional random fields. We show experimentally that exploiting dependencies using high-order features can lead to substantial performance improvements for some problems and discuss conditions under which high-order features can be effective.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">150</span></div><div id="author">Authors:<span id="author">Shuheng Zhou</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/92fb0c6d1758261f10d052e6e2c1123c-Paper.pdf">Thresholding Procedures for High Dimensional Variable Selection and Statistical Estimation</a></div><div id="abs">Abstract: <span id="abs">Given $n$ noisy samples with $p$ dimensions, where $n \ll p$, we show that the multi-stage thresholding procedures can accurately estimate a sparse vector $\beta \in \R^p$ in a linear model, under the restricted eigenvalue conditions (Bickel-Ritov-Tsybakov 09). Thus our conditions for model selection consistency are considerably weaker than what has been achieved in previous works. More importantly, this method allows very significant values of $s$, which is the number of non-zero elements in the true parameter $\beta$. For example, it works for cases where the ordinary Lasso would have failed. Finally, we show that if $X$ obeys a uniform uncertainty principle and if the true parameter is sufficiently sparse, the Gauss-Dantzig selector (Cand\{e}s-Tao 07) achieves the $\ell_2$ loss within a logarithmic factor of the ideal mean square error one would achieve with an oracle which would supply perfect information about which coordinates are non-zero and which are above the noise level, while selecting a sufficiently sparse model.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">157</span></div><div id="author">Authors:<span id="author">Yoshinobu Kawahara, Kiyohito Nagano, Koji Tsuda, Jeff A. Bilmes</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/9ad6aaed513b73148b7d49f70afcfb32-Paper.pdf">Submodularity Cuts and Applications</a></div><div id="abs">Abstract: <span id="abs">Several key problems in machine learning, such as feature selection and active learning, can be formulated as submodular set function maximization.  We present herein a novel algorithm for maximizing a submodular set function under a cardinality constraint --- the algorithm is based on a cutting-plane method and is implemented as an iterative small-scale binary-integer linear programming procedure. It is well known that this problem is NP-hard, and the approximation factor achieved by the greedy algorithm is the theoretical limit for polynomial time. As for (non-polynomial time) exact algorithms that perform reasonably in practice, there has been very little in the literature although the problem is quite important for many applications. Our algorithm is guaranteed to find the exact solution in finite iterations, and it converges fast in practice due to the efficiency of the cutting-plane mechanism. Moreover, we also provide a method that produces successively decreasing upper-bounds of the optimal solution, while our algorithm provides successively increasing lower-bounds.  Thus, the accuracy of the current solution can be estimated at any point, and the algorithm can be stopped early once a desired degree of tolerance is met.  We evaluate our algorithm on sensor placement and feature selection applications showing good performance.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">185</span></div><div id="author">Authors:<span id="author">Tat-jun Chin, Hanzi Wang, David Suter</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/b337e84de8752b27eda3a12363109e80-Paper.pdf">The Ordered Residual Kernel for Robust Motion Subspace Clustering</a></div><div id="abs">Abstract: <span id="abs">We present a novel and highly effective approach for multi-body motion segmentation. Drawing inspiration from robust statistical model fitting, we estimate putative subspace hypotheses from the data. However, instead of ranking them we encapsulate the hypotheses in a novel Mercer kernel which elicits the potential of two point trajectories to have emerged from the same subspace. The kernel permits the application of well-established statistical learning methods for effective outlier rejection, automatic recovery of the number of motions and accurate segmentation of the point trajectories. The method operates well under severe outliers arising from spurious trajectories or mistracks. Detailed experiments on a recent benchmark dataset (Hopkins 155) show that our method is superior to other state-of-the-art approaches in terms of recovering the number of motions, segmentation accuracy, robustness against gross outliers and computational efficiency.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">190</span></div><div id="author">Authors:<span id="author">Ruslan Salakhutdinov</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/b7ee6f5f9aa5cd17ca1aea43ce848496-Paper.pdf">Learning in Markov Random Fields using Tempered Transitions</a></div><div id="abs">Abstract: <span id="abs">Markov random fields (MRFs), or undirected graphical models, provide a powerful framework for modeling complex dependencies among random variables. Maximum likelihood learning in MRFs is hard due to the presence of the global normalizing constant. In this paper we consider a class of stochastic approximation algorithms of Robbins-Monro type that uses Markov chain Monte Carlo to do approximate maximum likelihood learning. We show that using MCMC operators based on tempered transitions enables the stochastic approximation algorithm to better explore highly multimodal distributions, which considerably improves parameter estimates in large densely-connected MRFs. Our results on MNIST and NORB datasets demonstrate that we can successfully learn good generative models of high-dimensional, richly structured data and perform well on digit and object recognition tasks.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">164</span></div><div id="author">Authors:<span id="author">Benjamin Culpepper, Bruno A. Olshausen</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/a1d50185e7426cbb0acad1e6ca74b9aa-Paper.pdf">Learning transport operators for image manifolds</a></div><div id="abs">Abstract: <span id="abs">We describe a method for learning a group of continuous transformation operators  to traverse smooth nonlinear manifolds. The method is applied to model how  natural images change over time and scale. The group of continuous transform  operators is represented by a basis that is adapted to the statistics of the data so  that the inﬁnitesimal generator for a measurement orbit can be produced by a  linear combination of a few basis elements. We illustrate how the method can be  used to efﬁciently code time-varying images by describing changes across time  and scale in terms of the learned operators.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">177</span></div><div id="author">Authors:<span id="author">Sanja Fidler, Marko Boben, Ales Leonardis</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/a8baa56554f96369ab93e4f3bb068c22-Paper.pdf">Evaluating multi-class learning strategies in a generative hierarchical framework for object detection</a></div><div id="abs">Abstract: <span id="abs">Multiple object class learning and detection is a challenging problem due to the large number of object classes and their high visual variability. Specialized detectors usually excel in performance, while joint representations optimize sharing and reduce inference time --- but are complex to train. Conveniently, sequential learning of categories cuts down training time by transferring existing knowledge to novel classes, but cannot fully exploit the richness of shareability and might depend on ordering in learning. In hierarchical frameworks these issues have been little explored. In this paper, we show how different types of multi-class learning can be done within one generative hierarchical framework and provide a rigorous experimental analysis of various object class learning strategies as the number of classes grows. Specifically, we propose, evaluate and compare three important types of multi-class learning: 1.) independent training of individual categories, 2.) joint training of classes, 3.) sequential learning of classes. We explore and compare their computational behavior (space and time) and detection performance as a function of the number of learned classes on several recognition data sets.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">167</span></div><div id="author">Authors:<span id="author">Wei Bian, Dacheng Tao</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/a516a87cfcaef229b342c437fe2b95f7-Paper.pdf">Manifold Regularization for SIR with Rate Root-n Convergence</a></div><div id="abs">Abstract: <span id="abs">In this paper, we study the manifold regularization for the Sliced Inverse Regression (SIR). The manifold regularization improves the standard SIR in two aspects: 1) it encodes the local geometry for SIR and 2) it enables SIR to deal with transductive and semi-supervised learning problems. We prove that the proposed graph Laplacian based regularization is convergent at rate root-n. The projection directions of the regularized SIR are optimized by using a conjugate gradient method on the Grassmann manifold. Experimental results support our theory.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">191</span></div><div id="author">Authors:<span id="author">Fen Xia, Tie-yan Liu, Hang Li</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/ba2fd310dcaa8781a9a652a31baf3c68-Paper.pdf">Statistical Consistency of Top-k Ranking</a></div><div id="abs">Abstract: <span id="abs">This paper is concerned with the consistency analysis on listwise ranking methods. Among various ranking methods, the listwise methods have competitive performances on benchmark datasets and are regarded as one of the state-of-the-art approaches. Most listwise ranking methods manage to optimize ranking on the whole list (permutation) of objects, however, in practical applications such as information retrieval, correct ranking at the top k positions is much more important. This paper aims to analyze whether existing listwise ranking methods are statistically consistent in the top-k setting. For this purpose, we define a top-k ranking framework, where the true loss (and thus the risks) are defined on the basis of top-k subgroup of permutations. This framework can include the permutation-level ranking framework proposed in previous work as a special case. Based on the new framework, we derive sufficient conditions for a listwise ranking method to be consistent with the top-k true loss, and show an effective way of modifying the surrogate loss functions in existing methods to satisfy these conditions. Experimental results show that after the modifications, the methods can work significantly better than their original versions.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">186</span></div><div id="author">Authors:<span id="author">Hamed Valizadegan, Rong Jin, Ruofei Zhang, Jianchang Mao</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/b3967a0e938dc2a6340e258630febd5a-Paper.pdf">Learning to Rank by Optimizing NDCG Measure</a></div><div id="abs">Abstract: <span id="abs">Learning to rank is a relatively new field of study, aiming to learn a ranking function from a set of training data with relevancy labels. The ranking algorithms are often evaluated using Information Retrieval measures, such as Normalized Discounted Cumulative Gain [1] and Mean Average Precision [2]. Until recently, most learning to rank algorithms were not using a loss function related to the above mentioned evaluation measures. The main difficulty in direct optimization of these measures is that they depend on the ranks of documents, not the numerical values output by the ranking function. We propose a probabilistic framework that addresses this challenge by optimizing the expectation of NDCG over all the possible permutations of documents. A relaxation strategy is used to approximate the average of NDCG over the space of permutation, and a bound optimization approach is proposed to make the computation efficient. Extensive experiments show that the proposed algorithm outperforms state-of-the-art ranking algorithms on several benchmark data sets.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">159</span></div><div id="author">Authors:<span id="author">Paris Smaragdis, Madhusudana Shashanka, Bhiksha Raj</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/9c3b1830513cc3b8fc4b76635d32e692-Paper.pdf">A Sparse Non-Parametric Approach for Single Channel Separation of Known Sounds</a></div><div id="abs">Abstract: <span id="abs">In this paper we present an algorithm for separating mixed sounds from  a monophonic recording. Our approach makes use of training data which  allows us to learn representations of the types of sounds that compose the  mixture. In contrast to popular methods that attempt to extract com-  pact generalizable models for each sound from training data, we employ  the training data itself as a representation of the sources in the mixture.  We show that mixtures of known sounds can be described as sparse com-  binations of the training data itself, and in doing so produce signiﬁcantly  better separation results as compared to similar systems based on compact  statistical models.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">170</span></div><div id="author">Authors:<span id="author">Rong Jin, Shijun Wang, Yang Zhou</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/a666587afda6e89aec274a3657558a27-Paper.pdf">Regularized Distance Metric Learning:Theory and Algorithm</a></div><div id="abs">Abstract: <span id="abs">In this paper, we examine the generalization error of regularized distance metric learning. We show that with appropriate constraints, the generalization error of regularized distance metric learning could be independent from the dimensionality, making it suitable for handling high dimensional data. In addition, we present an efficient online learning algorithm for regularized distance metric learning. Our empirical studies with data classification and face recognition show that the proposed algorithm is (i) effective for distance metric learning when compared to the state-of-the-art methods, and (ii) efficient and robust for high dimensional data.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">198</span></div><div id="author">Authors:<span id="author">Michael Brückner, Tobias Scheffer</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">Nash Equilibria of Static Prediction Games</a></div><div id="abs">Abstract: <span id="abs">The standard assumption of identically distributed training and test data can be violated when an adversary can exercise some control over the generation of the test data. In a prediction game, a learner produces a predictive model while an adversary may alter the distribution of input data. We study single-shot prediction games in which the cost functions of learner and adversary are not necessarily antagonistic. We identify conditions under which the prediction game has a unique Nash equilibrium, and derive algorithms that will find the equilibrial prediction models. In a case study, we explore properties of Nash-equilibrial prediction models for email spam filtering empirically.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">168</span></div><div id="author">Authors:<span id="author">Bernhard Nessler, Michael Pfeiffer, Wolfgang Maass</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/a5cdd4aa0048b187f7182f1b9ce7a6a7-Paper.pdf">STDP enables spiking neurons to detect hidden causes of their inputs</a></div><div id="abs">Abstract: <span id="abs">The principles by which spiking neurons contribute to the astounding computational power of generic cortical microcircuits, and how spike-timing-dependent plasticity (STDP) of synaptic weights could generate and maintain  this computational function, are unknown. We show here that STDP, in conjunction with a stochastic soft winner-take-all (WTA) circuit, induces spiking neurons to generate through their synaptic weights implicit internal models for subclasses (or causes") of the high-dimensional spike patterns of hundreds of pre-synaptic neurons. Hence these  neurons will fire after learning whenever the current input best matches their internal model. The resulting computational function of soft WTA circuits, a common network motif of cortical microcircuits, could therefore be a drastic dimensionality reduction of information streams, together with the autonomous creation of internal models for the probability distributions of their input patterns. We show that the autonomous generation and maintenance of this computational function can be explained on the basis of rigorous mathematical principles. In particular, we show that STDP is able to approximate a stochastic online Expectation-Maximization (EM) algorithm for modeling the input data. A corresponding result is shown for Hebbian learning in artificial neural networks."</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">178</span></div><div id="author">Authors:<span id="author">Zhihua Zhang, Guang Dai</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/a8e864d04c95572d1aece099af852d0a-Paper.pdf">Optimal Scoring for Unsupervised Learning</a></div><div id="abs">Abstract: <span id="abs">We are often interested in casting classification and clustering problems in a regression framework, because it is feasible to achieve some statistical properties in this framework by imposing some penalty criteria. In this paper we illustrate optimal scoring, which was originally proposed for performing Fisher linear discriminant analysis by regression, in the application of unsupervised learning. In particular, we devise a novel clustering algorithm that we call optimal discriminant clustering (ODC). We associate our algorithm with the existing unsupervised learning algorithms such as spectral clustering, discriminative clustering and sparse principal component analysis. Thus, our work shows that optimal scoring  provides a new approach to the implementation of  unsupervised learning. This approach facilitates the development of new unsupervised learning algorithms.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">209</span></div><div id="author">Authors:<span id="author">Jörg Lücke, Richard Turner, Maneesh Sahani, Marc Henniges</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/d67d8ab4f4c10bf22aa353e27879133c-Paper.pdf">Occlusive Components Analysis</a></div><div id="abs">Abstract: <span id="abs">We study unsupervised learning in a probabilistic generative model for occlusion. The model uses two types of latent variables: one indicates which objects are present in the image, and the other how they are ordered in depth. This depth order then determines how the positions and appearances of the objects present, specified in the model parameters, combine to form the image. We show that the object parameters can be learnt from an unlabelled set of images in which objects occlude one another. Exact maximum-likelihood learning is intractable. However, we show that tractable approximations to Expectation Maximization (EM) can be found if the training images each contain only a small number of objects on average. In numerical experiments it is shown that these approximations recover the correct set of object parameters. Experiments on a novel version of the bars test using colored bars, and experiments on more realistic data, show that the algorithm performs well in extracting the generating causes. Experiments based on the standard bars benchmark test for object learning show that the algorithm performs well in comparison to other recent component extraction approaches. The model and the learning algorithm thus connect research on occlusion with the research field of multiple-cause component extraction methods.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">174</span></div><div id="author">Authors:<span id="author">Bangpeng Yao, Dirk Walther, Diane Beck, Li Fei-fei</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/a86c450b76fb8c371afead6410d55534-Paper.pdf">Hierarchical Mixture of Classification Experts Uncovers Interactions between Brain Regions</a></div><div id="abs">Abstract: <span id="abs">The human brain can be described as containing a number of functional regions. For a given task, these regions, as well as the connections between them, play a key role in information processing in the brain. However, most existing multi-voxel pattern analysis approaches either treat multiple functional regions as one large uniform region or several independent regions, ignoring the connections between regions. In this paper, we propose to model such connections in an Hidden Conditional Random Field (HCRF) framework, where the classifier of one region of interest (ROI) makes predictions based on not only its voxels but also the classifier predictions from ROIs that it connects to. Furthermore, we propose a structural learning method in the HCRF framework to automatically uncover the connections between ROIs. Experiments on fMRI data acquired while human subjects viewing images of natural scenes show that our model can improve the top-level (the classifier combining information from all ROIs) and ROI-level prediction accuracy, as well as uncover some meaningful connections between ROIs.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">160</span></div><div id="author">Authors:<span id="author">Sylvain Arlot, Francis R. Bach</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/9c838d2e45b2ad1094d42f4ef36764f6-Paper.pdf">Data-driven calibration of linear estimators with minimal penalties</a></div><div id="abs">Abstract: <span id="abs">This paper tackles the problem of selecting among several linear estimators in non-parametric regression; this includes model selection for linear regression, the choice of a regularization parameter in kernel ridge regression or spline smoothing, and the choice of a kernel in multiple kernel learning. We propose a new algorithm which first estimates consistently the variance of the noise, based upon the concept of minimal penalty which was previously introduced in the context of model selection. Then, plugging our variance estimate in Mallows $C_L$ penalty is proved to lead to an algorithm satisfying an oracle inequality. Simulation experiments with kernel ridge regression and multiple kernel learning show that the proposed algorithm often improves significantly existing calibration procedures such as 10-fold cross-validation or generalized cross-validation.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">195</span></div><div id="author">Authors:<span id="author">Sebastian Gerwinn, Philipp Berens, Matthias Bethge</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/be83ab3ecd0db773eb2dc1b0a17836a1-Paper.pdf">A joint maximum-entropy model for binary neural population patterns and continuous signals</a></div><div id="abs">Abstract: <span id="abs">Second-order maximum-entropy models have recently gained much interest for describing the statistics of binary spike trains. Here, we extend this approach to take continuous stimuli into account as well. By constraining  the joint second-order statistics, we obtain a joint Gaussian-Boltzmann distribution of continuous stimuli and binary neural firing patterns, for which we also compute marginal and conditional distributions. This model has the same computational complexity as pure binary models and fitting it to data is a convex problem. We show that the model can be seen as an extension to the classical spike-triggered average/covariance analysis and can be used as a non-linear method for extracting features which a neural population is sensitive to. Further, by calculating the posterior distribution of stimuli given an observed neural response, the model can be used to decode stimuli and yields a natural spike-train metric. Therefore, extending the framework of maximum-entropy models to continuous variables allows us to gain novel insights into the relationship between the firing patterns of neural ensembles and the stimuli they are processing.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">173</span></div><div id="author">Authors:<span id="author">Stephen Gould, Tianshi Gao, Daphne Koller</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/a7aeed74714116f3b292a982238f83d2-Paper.pdf">Region-based Segmentation and Object Detection</a></div><div id="abs">Abstract: <span id="abs">Object detection and multi-class image segmentation are two closely related tasks that can be greatly improved when solved jointly by feeding information from one task to the other. However, current state-of-the-art models use a separate representation for each task making joint inference clumsy and leaving classification of many parts of the scene ambiguous. In this work, we propose a hierarchical region-based approach to joint object detection and image segmentation. Our approach reasons about pixels, regions and objects in a coherent probabilistic model. Importantly, our model gives a single unified description of the scene. We explain every pixel in the image and enforce global consistency between all variables in our model. We run experiments on challenging vision datasets and show significant improvement over state-of-the-art object detection accuracy.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">172</span></div><div id="author">Authors:<span id="author">Yao-liang Yu, Yuxi Li, Dale Schuurmans, Csaba Szepesvári</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/a684eceee76fc522773286a895bc8436-Paper.pdf">A General Projection Property for Distribution Families</a></div><div id="abs">Abstract: <span id="abs">We prove that linear projections between distribution families with fixed first and second moments are surjective, regardless of dimension. We further extend this result to families that respect additional constraints, such as symmetry, unimodality and log-concavity. By combining our results with classic univariate inequalities, we provide new worst-case analyses for natural risk criteria arising in different fields. One discovery is that portfolio selection under the worst-case value-at-risk and conditional value-at-risk criteria yields identical portfolios.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">162</span></div><div id="author">Authors:<span id="author">Honglak Lee, Peter Pham, Yan Largman, Andrew Y. Ng</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/a113c1ecd3cace2237256f4c712f61b5-Paper.pdf">Unsupervised feature learning for audio classification using convolutional deep belief networks</a></div><div id="abs">Abstract: <span id="abs">In recent years, deep learning approaches have gained significant interest as a way of building hierarchical representations from unlabeled data. However, to our knowledge, these deep learning approaches have not been extensively studied for auditory data. In this paper, we apply convolutional deep belief networks to audio data and empirically evaluate them on various audio classification tasks. For the case of speech data, we show that the learned features correspond to phones/phonemes. In addition, our feature representations trained from unlabeled audio data show very good performance for multiple audio classification tasks. We hope that this paper will inspire more research on deep learning approaches applied to a wide range of audio recognition tasks.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">201</span></div><div id="author">Authors:<span id="author">Hengshuai Yao, Shalabh Bhatnagar, Dongcui Diao, Richard S. Sutton, Csaba Szepesvári</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/c52f1bd66cc19d05628bd8bf27af3ad6-Paper.pdf">Multi-Step Dyna Planning for Policy Evaluation and Control</a></div><div id="abs">Abstract: <span id="abs">We extend Dyna planning architecture for policy evaluation and control in two significant aspects. First, we introduce a multi-step Dyna planning that projects the simulated state/feature many steps into the future. Our multi-step Dyna is based on a multi-step model, which we call the {\em $\lambda$-model}. The $\lambda$-model interpolates between the one-step model and an infinite-step model, and can be learned efficiently online. Second, we use for Dyna control a dynamic multi-step model that is able to predict the results of a sequence of greedy actions and track the optimal policy in the long run. Experimental results show that Dyna using the multi-step model evaluates a policy faster than using single-step models; Dyna control algorithms using the dynamic tracking model are much faster than model-free algorithms; further, multi-step Dyna control algorithms enable the policy and value function to converge much faster to their optima than single-step Dyna algorithms.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">203</span></div><div id="author">Authors:<span id="author">Sennay Ghebreab, Steven Scholte, Victor Lamme, Arnold Smeulders</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/ccb0989662211f61edae2e26d58ea92f-Paper.pdf">A Biologically Plausible Model for Rapid Natural Scene Identification</a></div><div id="abs">Abstract: <span id="abs">Contrast statistics of the majority of natural images conform to a Weibull distribution. This property of natural images may facilitate efficient and very rapid extraction of a scenes visual gist. Here we investigate whether a neural response model based on the Weibull contrast distribution captures visual information that humans use to rapidly identify natural scenes. In a learning phase, we measure EEG activity  of 32 subjects viewing brief flashes of 800 natural scenes. From these neural measurements and the contrast statistics of the natural image stimuli,  we  derive an across subject  Weibull response model. We use this model to predict the responses to a large set of new scenes and estimate which scene the subject viewed by finding the best match between the model predictions and the observed EEG responses. In almost 90 percent of the cases our model accurately predicts the observed scene. Moreover, in most failed cases, the scene mistaken for the observed scene is visually similar to the observed scene itself. These results suggest that Weibull contrast statistics of natural images contain a considerable amount of scene gist information to warrant rapid identification of natural images.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">196</span></div><div id="author">Authors:<span id="author">Matthias Bethge, Eero P. Simoncelli, Fabian H. Sinz</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/c0e190d8267e36708f955d7ab048990d-Paper.pdf">Hierarchical Modeling of Local Image Features through $L_p$-Nested Symmetric Distributions</a></div><div id="abs">Abstract: <span id="abs">We introduce a new family of distributions, called $L_p${\em -nested symmetric distributions}, whose densities access the data exclusively through a hierarchical cascade of $L_p$-norms. This class generalizes the family of spherically and $L_p$-spherically symmetric distributions which have recently been successfully used for natural image modeling. Similar to those distributions it allows for a nonlinear mechanism to reduce the dependencies between its variables. With suitable choices of the parameters and norms, this family also includes the Independent Subspace Analysis (ISA) model, which has been proposed as a means of deriving filters that mimic complex cells found in mammalian primary visual cortex. $L_p$-nested distributions are easy to estimate and allow us to explore the variety of models between ISA and the $L_p$-spherically symmetric models. Our main findings are that, without a preprocessing step of contrast gain control, the independent subspaces of ISA are in fact more dependent than the individual filter coefficients within a subspace and, with contrast gain control, where ISA finds more than one subspace, the filter responses were almost independent anyway.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">228</span></div><div id="author">Authors:<span id="author">Yang Wang, Gholamreza Haffari, Shaojun Wang, Greg Mori</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/e5841df2166dd424a57127423d276bbe-Paper.pdf">A Rate Distortion Approach for Semi-Supervised Conditional Random Fields</a></div><div id="abs">Abstract: <span id="abs">We propose a novel information theoretic approach for semi-supervised learning of conditional random fields. Our approach defines a training objective that combines the conditional likelihood on labeled data and the mutual information on unlabeled data. Different from previous minimum conditional entropy semi-supervised discriminative learning methods, our approach can be naturally cast into the rate distortion theory framework in information theory. We analyze the tractability of the framework for structured prediction and present a convergent variational training algorithm to defy the combinatorial explosion of terms in the sum over label configurations. Our experimental results show that the rate distortion approach outperforms standard $l_2$ regularization and minimum conditional entropy regularization on both multi-class classification and sequence labeling problems.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">232</span></div><div id="author">Authors:<span id="author">Corinna Cortes, Mehryar Mohri, Afshin Rostamizadeh</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/e7f8a7fb0b77bcb3b283af5be021448f-Paper.pdf">Learning Non-Linear Combinations of Kernels</a></div><div id="abs">Abstract: <span id="abs">This paper studies the general problem of learning kernels based on a polynomial combination of base kernels. It analyzes this problem in the case of regression and the kernel ridge regression algorithm. It examines the corresponding learning kernel optimization problem, shows how that minimax problem can be reduced to a simpler minimization problem, and proves that the global solution of this problem always lies on the boundary. We give a projection-based gradient descent algorithm for solving the optimization problem, shown empirically to converge in few iterations. Finally, we report the results of extensive experiments with this algorithm using several publicly available datasets demonstrating the effectiveness of our technique.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">230</span></div><div id="author">Authors:<span id="author">Masayuki Karasuyama, Ichiro Takeuchi</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/e6b4b2a746ed40e1af829d1fa82daa10-Paper.pdf">Multiple Incremental Decremental Learning of Support Vector Machines</a></div><div id="abs">Abstract: <span id="abs">We propose a multiple incremental decremental algorithm  of Support Vector Machine (SVM). Conventional single  cremental decremental SVM can update the trained model  efficiently when single data point is added to or removed  from the training set. When we add and/or remove multiple  data points, this algorithm is time-consuming because we  need to repeatedly apply it to each data point. The roposed  algorithm is computationally more efficient when multiple  data points are added and/or removed simultaneously. The  single incremental decremental algorithm is built on an  optimization technique called parametric programming.  We extend the idea and introduce multi-parametric  programming for developing the proposed algorithm.  Experimental results on synthetic and real data sets indicate that the proposed algorithm can significantly  reduce the computational cost of multiple incremental  decremental operation. Our approach is especially useful  for online SVM learning in which we need to remove old  data points and add new data points in a short amount of  time.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">229</span></div><div id="author">Authors:<span id="author">Chenghui Cai, Xuejun Liao, Lawrence Carin</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/e58cc5ca94270acaceed13bc82dfedf7-Paper.pdf">Learning to Explore and Exploit in POMDPs</a></div><div id="abs">Abstract: <span id="abs">A fundamental objective in reinforcement learning is the maintenance of a proper balance between exploration and exploitation. This problem becomes more challenging when the agent can only partially observe the states of its environment. In this paper we propose a dual-policy method for jointly learning the agent behavior and the balance between exploration exploitation, in partially observable environments. The method subsumes traditional exploration, in which the agent takes actions to gather information about the environment, and active learning, in which the agent queries an oracle for optimal actions (with an associated cost for employing the oracle). The form of the employed exploration is dictated by the specific problem. Theoretical guarantees are provided concerning the optimality of the balancing of exploration and exploitation. The effectiveness of the method is demonstrated by experimental results on benchmark problems.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">211</span></div><div id="author">Authors:<span id="author">Vijay Desai, Vivek Farias, Ciamac C. Moallemi</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Paper.pdf">A Smoothed Approximate Linear Program</a></div><div id="abs">Abstract: <span id="abs">We present a novel linear program for the approximation of the dynamic programming cost-to-go function in high-dimensional stochastic control problems. LP approaches to approximate DP naturally restrict attention to approximations that are lower bounds to the optimal cost-to-go function. Our program -- the `smoothed approximate linear program -- relaxes this restriction in an appropriate fashion while remaining computationally tractable. Doing so appears to have several advantages: First, we demonstrate superior bounds on the quality of approximation to the optimal cost-to-go function afforded by our approach. Second, experiments with our approach on a challenging problem (the game of Tetris) show that the approach outperforms the existing LP approach (which has previously been shown to be competitive with several ADP algorithms) by an order of magnitude.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">256</span></div><div id="author">Authors:<span id="author">Jonathan Chang, Sean Gerrish, Chong Wang, Jordan L. Boyd-graber, David M. Blei</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/f92586a25bb3145facd64ab20fd554ff-Paper.pdf">Reading Tea Leaves: How Humans Interpret Topic Models</a></div><div id="abs">Abstract: <span id="abs">Probabilistic topic models are a popular tool for the unsupervised analysis of text, providing both a predictive model of future text and a latent topic representation of the corpus. Practitioners typically assume that the latent space is semantically meaningful. It is used to check models, summarize the corpus, and guide exploration of its contents. However, whether the latent space is interpretable is in need of quantitative evaluation. In this paper, we present new quantitative methods for measuring semantic meaning in inferred topics. We back these measures with large-scale user studies, showing that they capture aspects of the model that are undetected by previous measures of model quality based on held-out likelihood. Surprisingly, topic models which perform better on held-out likelihood may infer less semantically meaningful topics.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">236</span></div><div id="author">Authors:<span id="author">Finale Doshi-velez</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/ebd9629fc3ae5e9f6611e2ee05a31cef-Paper.pdf">The Infinite Partially Observable Markov Decision Process</a></div><div id="abs">Abstract: <span id="abs">The Partially Observable Markov Decision Process (POMDP) framework has proven useful in planning domains that require balancing actions that increase an agents knowledge and actions that increase an agents reward.  Unfortunately, most POMDPs are complex structures with a large number of parameters.  In many realworld problems, both the structure and the parameters are difficult to specify from domain knowledge alone.  Recent work in Bayesian reinforcement learning has made headway in learning POMDP models; however, this work has largely focused on learning the parameters of the POMDP model.  We define an infinite POMDP (iPOMDP) model that does not require knowledge of the size of the state space; instead, it assumes that the number of visited states will grow as the agent explores its world and explicitly models only visited states.  We demonstrate the iPOMDP utility on several standard problems.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">231</span></div><div id="author">Authors:<span id="author">Marcel V. Gerven, Botond Cseke, Robert Oostenveld, Tom Heskes</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Paper.pdf">Bayesian Source Localization with the Multivariate Laplace Prior</a></div><div id="abs">Abstract: <span id="abs">We introduce a novel multivariate Laplace (MVL) distribution as a sparsity promoting prior for Bayesian source localization that allows the specification of constraints between and within sources. We represent the MVL distribution as a scale mixture that induces a coupling between source variances instead of their means. Approximation of the posterior marginals using expectation propagation is shown to be very efficient due to properties of the scale mixture representation. The computational bottleneck amounts to computing the diagonal elements of a sparse matrix inverse. Our approach is illustrated using a mismatch negativity paradigm for which MEG data and a structural MRI have been acquired. We show that spatial coupling leads to sources which are active over larger cortical areas as compared with an uncoupled prior.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">216</span></div><div id="author">Authors:<span id="author">Yusuke Fujiwara, Yoichi Miyawaki, Yukiyasu Kamitani</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/dc5689792e08eb2e219dce49e64c885b-Paper.pdf">Estimating image bases for visual image reconstruction from human brain activity</a></div><div id="abs">Abstract: <span id="abs">Image representation based on image bases provides a framework for understanding neural representation of visual perception. A recent fMRI study has shown that arbitrary contrast-defined visual images can be reconstructed from fMRI activity patterns using a combination of multi-scale local image bases. In the reconstruction model, the mapping from an fMRI activity pattern to the contrasts of the image bases was learned from measured fMRI responses to visual images. But the shapes of the images bases were fixed, and thus may not be optimal for reconstruction. Here, we propose a method to build a reconstruction model in which image bases are automatically extracted from the measured data. We constructed a probabilistic model that relates the fMRI activity space to the visual image space via a set of latent variables. The mapping from the latent variables to the visual image space can be regarded as a set of image bases. We found that spatially localized, multi-scale image bases were estimated near the fovea, and that the model using the estimated image bases was able to accurately reconstruct novel visual images. The proposed method provides a means to discover a novel functional mapping between stimuli and brain activity patterns.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">237</span></div><div id="author">Authors:<span id="author">Chonghai Hu, Weike Pan, James T. Kwok</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/ec5aa0b7846082a2415f0902f0da88f2-Paper.pdf">Accelerated Gradient Methods for Stochastic Optimization and Online Learning</a></div><div id="abs">Abstract: <span id="abs">Regularized risk minimization often involves non-smooth optimization, either because of the loss function (e.g., hinge loss) or the regularizer (e.g., $\ell_1$-regularizer). Gradient descent methods, though highly scalable and easy to implement, are known to converge slowly on these problems. In this paper, we develop novel accelerated gradient methods for stochastic optimization while still preserving their computational simplicity and scalability. The proposed algorithm, called SAGE (Stochastic Accelerated GradiEnt), exhibits fast convergence rates on stochastic optimization with both convex and strongly convex objectives. Experimental results show that SAGE is faster than recent (sub)gradient methods including FOLOS, SMIDAS and SCD. Moreover, SAGE can also be extended for online learning, resulting in a simple but powerful algorithm.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">207</span></div><div id="author">Authors:<span id="author">Marcus Hutter</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/d3d9446802a44259755d38e6d163e820-Paper.pdf">Discrete MDL Predicts in Total Variation</a></div><div id="abs">Abstract: <span id="abs">The Minimum Description Length (MDL) principle selects the model that has the shortest code for data plus model. We show that for a countable class of models, MDL predictions are close to the true distribution in a strong sense. The result is completely general. No independence, ergodicity, stationarity, identifiability, or other assumption on the model class need to be made. More formally, we show that for any countable class of models, the distributions selected by MDL (or MAP) asymptotically predict (merge with) the true measure in the class in total variation distance. Implications for non-i.i.d. domains like time-series forecasting, discriminative learning, and reinforcement learning are discussed.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">222</span></div><div id="author">Authors:<span id="author">Kevin Waugh, Nolan Bard, Michael Bowling</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/e0ec453e28e061cc58ac43f91dc2f3f0-Paper.pdf">Strategy Grafting in Extensive Games</a></div><div id="abs">Abstract: <span id="abs">Extensive games are often used to model the interactions of multiple agents within an environment.  Much recent work has focused on increasing the size of an extensive game that can be feasibly solved.  Despite these improvements, many interesting games are still too large for such techniques.  A common approach for computing strategies in these large games is to first employ an abstraction technique to reduce the original game to an abstract game that is of a manageable size.  This abstract game is then solved and the resulting strategy is used in the original game.  Most top programs in recent AAAI Computer Poker Competitions use this approach.  The trend in this competition has been that strategies found in larger abstract games tend to beat strategies found in smaller abstract games.  These larger abstract games have more expressive strategy spaces and therefore contain better strategies.  In this paper we present a new method for computing strategies in large games.  This method allows us to compute more expressive strategies without increasing the size of abstract games that we are required to solve.  We demonstrate the power of the approach experimentally in both small and large games, while also providing a theoretical justification for the resulting improvement.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">249</span></div><div id="author">Authors:<span id="author">Xiaojin Zhu, Bryan R. Gibson, Timothy T. Rogers</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/f7664060cc52bc6f3d620bcedc94a4b6-Paper.pdf">Human Rademacher Complexity</a></div><div id="abs">Abstract: <span id="abs">We propose to use Rademacher complexity, originally developed in computational learning theory, as a measure of human learning capacity.  Rademacher complexity measures a learners ability to fit random data, and can be used to bound the learners true error based on the observed training sample error.  We first review the definition of Rademacher complexity and its generalization bound.  We then describe a learning the noise" procedure to experimentally measure human Rademacher complexities.  The results from empirical studies showed that: (i) human Rademacher complexity can be successfully measured, (ii) the complexity depends on the domain and training sample size in intuitive ways, (iii) human learning respects the generalization bounds, (iv) the bounds can be useful in predicting the danger of overfitting in human learning.  Finally, we discuss the potential applications of human Rademacher complexity in cognitive science."</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">235</span></div><div id="author">Authors:<span id="author">Steven Chase, Andrew Schwartz, Wolfgang Maass, Robert A. Legenstein</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/eb163727917cbba1eea208541a643e74-Paper.pdf">Functional network reorganization in motor cortex can be explained by reward-modulated Hebbian learning</a></div><div id="abs">Abstract: <span id="abs">The control of neuroprosthetic devices from the activity of motor cortex neurons benefits from learning effects where the function of these neurons is adapted to the control task. It was recently shown that tuning properties of neurons in monkey motor cortex are adapted selectively in order to compensate for an erroneous interpretation of their activity. In particular, it was shown that the tuning curves of those neurons whose preferred directions had been misinterpreted changed more than those of other neurons. In this article, we show that the experimentally observed self-tuning properties of the system can be explained on the basis of a simple learning rule. This learning rule utilizes neuronal noise for exploration and performs Hebbian weight updates that are modulated by a global reward signal. In contrast to most previously proposed reward-modulated Hebbian learning rules, this rule does not require extraneous knowledge about what is noise and what is signal. The learning rule is able to optimize the performance of the model system within biologically realistic periods of time and under high noise levels. When the neuronal noise is fitted to experimental data, the model produces learning effects similar to those found in monkey experiments.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">255</span></div><div id="author">Authors:<span id="author">Jacob Whitehill, Ting-fan Wu, Jacob Bergsma, Javier R. Movellan, Paul L. Ruvolo</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/f899139df5e1059396431415e770c6dd-Paper.pdf">Whose Vote Should Count More: Optimal Integration of Labels from Labelers of Unknown Expertise</a></div><div id="abs">Abstract: <span id="abs">Modern machine learning-based approaches to computer vision require very large databases of labeled images. Some contemporary vision systems already require on the order of millions of images for training (e.g., Omron face detector). While the collection of these large databases is becoming a bottleneck, new Internet-based services that allow labelers from around the world to be easily hired and managed provide a promising solution.  However, using these services to label large databases brings with it new theoretical and practical challenges: (1) The labelers may have wide ranging levels of expertise which are unknown a priori, and in some cases may be adversarial; (2) images may vary in their level of difficulty; and (3) multiple labels for the same image must be combined to provide an estimate of the actual label of the image. Probabilistic approaches provide a principled way to approach these problems. In this paper we present a probabilistic model and use it to simultaneously infer the label of each image, the expertise of each labeler, and the difficulty of each image. On both simulated and real data, we demonstrate that the model outperforms the commonly used ``Majority Vote heuristic for inferring image labels, and is robust to both adversarial and noisy labelers.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">260</span></div><div id="author">Authors:<span id="author">Marius Leordeanu, Martial Hebert, Rahul Sukthankar</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/fc2c7c47b918d0c2d792a719dfb602ef-Paper.pdf">An Integer Projected Fixed Point Method for Graph Matching and MAP Inference</a></div><div id="abs">Abstract: <span id="abs">Graph matching and MAP inference are essential problems in computer vision and machine learning. We introduce a novel algorithm that can accommodate both problems and solve them efficiently. Recent graph matching algorithms are based on a general quadratic programming formulation, that takes in consideration both unary and second-order terms reflecting the similarities in local appearance as well as in the pairwise geometric relationships between the matched features. In this case the problem is NP-hard and a lot of effort has been spent in finding efficiently approximate solutions by relaxing the constraints of the original problem. Most algorithms find optimal continuous solutions of the modified problem, ignoring during the optimization the original discrete constraints. The continuous solution is quickly binarized at the end, but very little attention is put into this final discretization step. In this paper we argue that the stage in which a discrete solution is found is crucial for good performance. We propose an efficient algorithm, with climbing and convergence properties, that optimizes in the discrete domain the quadratic score, and it gives excellent results either by itself or by starting from the solution returned by any graph matching algorithm. In practice it outperforms state-or-the art algorithms and it also significantly improves their performance if used in combination. When applied to MAP inference, the algorithm is a parallel extension of Iterated Conditional Modes (ICM) with climbing and convergence properties that make it a compelling alternative to the sequential ICM. In our experiments on MAP inference our algorithm proved its effectiveness by outperforming ICM and Max-Product Belief Propagation.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">34</span></div><div id="author">Authors:<span id="author">Ye Chen, Michael Kapralov, John Canny, Dmitry Y. Pavlov</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/1bb91f73e9d31ea2830a5e73ce3ed328-Paper.pdf">Factor Modeling for Advertisement Targeting</a></div><div id="abs">Abstract: <span id="abs">We adapt a probabilistic latent variable model, namely GaP (Gamma-Poisson), to ad targeting in the contexts of sponsored search (SS) and behaviorally targeted (BT) display advertising. We also approach the important problem of ad positional bias by formulating a one-latent-dimension GaP factorization. Learning from click-through data is intrinsically large scale, even more so for ads. We scale up the algorithm to terabytes of real-world SS and BT data that contains hundreds of millions of users and hundreds of thousands of features, by leveraging the scalability characteristics of the algorithm and the inherent structure of the problem including data sparsity and locality. Specifically, we demonstrate two somewhat orthogonal philosophies of scaling algorithms to large-scale problems, through the SS and BT implementations, respectively. Finally, we report the experimental results using Yahoos vast datasets, and show that our approach substantially outperform the state-of-the-art methods in prediction accuracy. For BT in particular, the ROC area achieved by GaP is exceeding 0.95, while one prior approach using Poisson regression yielded 0.83. For computational performance, we compare a single-node sparse implementation with a parallel implementation using Hadoop MapReduce, the results are counterintuitive yet quite interesting. We therefore provide insights into the underlying principles of large-scale learning.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">143</span></div><div id="author">Authors:<span id="author">Koby Crammer, Alex Kulesza, Mark Dredze</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/8ebda540cbcc4d7336496819a46a1b68-Paper.pdf">Adaptive Regularization of Weight Vectors</a></div><div id="abs">Abstract: <span id="abs">We present AROW, a new online learning algorithm that combines several properties of successful : large margin training, confidence weighting, and the capacity to handle non-separable data. AROW performs adaptive regularization of the prediction function upon seeing each new instance, allowing it to perform especially well in the presence of label noise.  We derive a mistake bound, similar in form to the second order perceptron bound, which does not assume separability. We also relate our algorithm to recent confidence-weighted online learning techniques and empirically show that AROW achieves state-of-the-art performance and notable robustness in the case of non-separable data.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">221</span></div><div id="author">Authors:<span id="author">George Konidaris, Andrew G. Barto</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/e0cf1f47118daebc5b16269099ad7347-Paper.pdf">Skill Discovery in Continuous Reinforcement Learning Domains using Skill Chaining</a></div><div id="abs">Abstract: <span id="abs">We introduce skill chaining, a skill discovery method for reinforcement learning agents in continuous domains, that builds chains of skills leading to an end-of-task reward. We demonstrate experimentally that it creates skills that result in performance benefits in a challenging continuous domain.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">218</span></div><div id="author">Authors:<span id="author">Grzegorz Swirszcz, Naoki Abe, Aurelie C. Lozano</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/dd45045f8c68db9f54e70c67048d32e8-Paper.pdf">Grouped Orthogonal Matching Pursuit for Variable Selection and Prediction</a></div><div id="abs">Abstract: <span id="abs">We consider the problem of variable group selection for least squares regression, namely, that of selecting groups of variables for best regression performance, leveraging and adhering to a natural grouping structure within the explanatory variables. We show that this problem can be efficiently addressed by using a certain greedy style algorithm. More precisely, we propose the Group Orthogonal Matching Pursuit algorithm (Group-OMP), which extends the standard OMP procedure (also referred to as ``forward greedy feature selection algorithm for least squares regression) to perform stage-wise group variable selection. We prove that under certain conditions Group-OMP can identify the correct (groups of) variables. We also provide an upperbound on the $l_\infty$ norm of the difference between the estimated regression coefficients and the true coefficients. Experimental results on simulated and real world datasets indicate that Group-OMP compares favorably to Group Lasso, OMP and Lasso, both in terms of variable selection and prediction accuracy.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">254</span></div><div id="author">Authors:<span id="author">Lan Du, Lu Ren, Lawrence Carin, David B. Dunson</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/f85454e8279be180185cac7d243c5eb3-Paper.pdf">A Bayesian Model for Simultaneous Image Clustering, Annotation and Object Segmentation</a></div><div id="abs">Abstract: <span id="abs">A non-parametric Bayesian model is proposed for processing multiple images. The analysis employs image features and, when present, the words associated with accompanying annotations. The model clusters the images into classes, and each image is segmented into a set of objects, also allowing the opportunity to assign a word to each object (localized labeling). Each object is assumed to be represented as a heterogeneous mix of components, with this realized via mixture models linking image features to object types. The number of image classes, number of object types, and the characteristics of the object-feature mixture models are inferred non-parametrically. To constitute spatially contiguous objects, a new logistic stick-breaking process is developed. Inference is performed efficiently via variational Bayesian analysis, with example results presented on two image databases.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">206</span></div><div id="author">Authors:<span id="author">Mingyuan Zhou, Haojun Chen, Lu Ren, Guillermo Sapiro, Lawrence Carin, John W. Paisley</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/cfecdb276f634854f3ef915e2e980c31-Paper.pdf">Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations</a></div><div id="abs">Abstract: <span id="abs">Non-parametric Bayesian techniques are considered for learning dictionaries for sparse image representations, with applications in denoising, inpainting and compressive sensing (CS). The beta process is employed as a prior for learning the dictionary, and this non-parametric method naturally infers an appropriate dictionary size. The Dirichlet process and a probit stick-breaking process are also considered to exploit structure within an image. The proposed method can learn a sparse dictionary in situ; training images may be exploited if available, but they are not required. Further, the noise variance need not be known, and can be non-stationary. Another virtue of the proposed method is that sequential inference can be readily employed, thereby allowing scaling to large images. Several example results are presented, using both Gibbs and variational Bayesian inference, with comparisons to other state-of-the-art approaches.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">83</span></div><div id="author">Authors:<span id="author">Jong K. Kim, Seungjin Choi</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/4e0928de075538c593fbdabb0c5ef2c3-Paper.pdf">Clustering sequence sets for motif discovery</a></div><div id="abs">Abstract: <span id="abs">Most of existing methods for DNA motif discovery consider only a single set of sequences to find an over-represented motif. In contrast, we consider multiple sets of sequences where we group sets associated with the same motif into a cluster, assuming that each set involves a single motif. Clustering sets of sequences yields clusters of coherent motifs, improving signal-to-noise ratio or enabling us to identify multiple motifs. We present a probabilistic model for DNA motif discovery where we identify multiple motifs through searching for patterns which are shared across multiple sets of sequences. Our model infers cluster-indicating latent variables and learns motifs simultaneously, where these two tasks interact with each other. We show that our model can handle various motif discovery problems, depending on how to construct multiple sets of sequences. Experiments on three different problems for discovering DNA motifs emphasize the useful behavior and confirm the substantial gains over existing methods where only single set of sequences is considered.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">204</span></div><div id="author">Authors:<span id="author">Tom Ouyang, Randall Davis</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/cf67355a3333e6e143439161adc2d82e-Paper.pdf">Learning from Neighboring Strokes: Combining Appearance and Context for Multi-Domain Sketch Recognition</a></div><div id="abs">Abstract: <span id="abs">We propose a new sketch recognition framework that combines a rich representation of low level visual appearance with a graphical model for capturing high level relationships between symbols. This joint model of appearance and context allows our framework to be less sensitive to noise and drawing variations, improving accuracy and robustness. The result is a recognizer that is better able to handle the wide range of drawing styles found in messy freehand sketches. We evaluate our work on two real-world domains, molecular diagrams and electrical circuit diagrams, and show that our combined approach significantly improves recognition performance.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">245</span></div><div id="author">Authors:<span id="author">Yongxin Xi, Uri Hasson, Peter J. Ramadge, Zhen J. Xiang</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/f2217062e9a397a1dca429e7d70bc6ca-Paper.pdf">Boosting with Spatial Regularization</a></div><div id="abs">Abstract: <span id="abs">By adding a spatial regularization kernel to a standard loss function formulation of the boosting problem, we develop a framework for spatially informed boosting.  From this regularized loss framework we derive an efficient boosting algorithm that uses additional weights/priors on the base classifiers.  We prove that the proposed algorithm exhibits a ``grouping effect, which encourages the selection of all spatially local, discriminative base classifiers.  The algorithms primary advantage is in applications where the trained classifier is used to identify the spatial pattern of discriminative information, e.g. the voxel selection problem in fMRI.  We demonstrate the algorithms performance on various data sets.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">224</span></div><div id="author">Authors:<span id="author">M. P. Kumar, Daphne Koller</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/e2a2dcc36a08a345332c751b2f2e476c-Paper.pdf">Learning a Small Mixture of Trees</a></div><div id="abs">Abstract: <span id="abs">The problem of approximating a given probability distribution using a simpler distribution plays an important role in several areas of machine learning, e.g. variational inference and classification. Within this context, we consider the task of learning a mixture of tree distributions. Although mixtures of trees can be learned by minimizing the KL-divergence using an EM algorithm, its success depends heavily on the initialization. We propose an efficient strategy for obtaining a good initial set of trees that attempts to cover the entire observed distribution by minimizing the $\alpha$-divergence with $\alpha = \infty$. We formulate the problem using the fractional covering framework and present a convergent sequential algorithm that only relies on solving a convex program at each iteration. Compared to previous methods, our approach results in a significantly smaller mixture of trees that provides similar or better accuracies. We demonstrate the usefulness of our approach by learning pictorial structures for face recognition.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">142</span></div><div id="author">Authors:<span id="author">Zhi Yang, Qi Zhao, Edward Keefer, Wentai Liu</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/8e296a067a37563370ded05f5a3bf3ec-Paper.pdf">Noise Characterization, Modeling, and Reduction for In Vivo Neural Recording</a></div><div id="abs">Abstract: <span id="abs">Studying signal and noise properties of recorded neural data is critical in developing more efficient algorithms to recover the encoded information. Important issues exist in this research including the variant spectrum spans of neural spikes that make it difficult to choose a global optimal bandpass filter. Also, multiple sources produce aggregated noise that deviates from the conventional white Gaussian noise. In this work, the spectrum variability of spikes is addressed, based on which the concept of adaptive bandpass filter that fits the spectrum of individual spikes is proposed. Multiple noise sources have been studied through analytical models as well as empirical measurements. The dominant noise source is identified as neuron noise followed by interface noise of the electrode. This suggests that major efforts to reduce noise from electronics are not well spent. The measured noise from in vivo experiments shows a family of 1/f^{x} (x=1.5\pm 0.5) spectrum that can be reduced using noise shaping techniques. In summary, the methods of adaptive bandpass filtering and noise shaping together result in several dB signal-to-noise ratio (SNR) enhancement.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">210</span></div><div id="author">Authors:<span id="author">Ofer Dekel</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/d707329bece455a462b58ce00d1194c9-Paper.pdf">Distribution-Calibrated Hierarchical Classification</a></div><div id="abs">Abstract: <span id="abs">While many advances have already been made on the topic of hierarchical classi-  ﬁcation learning, we take a step back and examine how a hierarchical classiﬁca-  tion problem should be formally deﬁned. We pay particular attention to the fact  that many arbitrary decisions go into the design of the the label taxonomy that  is provided with the training data, and that this taxonomy is often unbalanced.  We correct this problem by using the data distribution to calibrate the hierarchical  classiﬁcation loss function. This distribution-based correction must be done with  care, to avoid introducing unmanagable statstical dependencies into the learning  problem. This leads us off the beaten path of binomial-type estimation and into  the uncharted waters of geometric-type estimation. We present a new calibrated  deﬁnition of statistical risk for hierarchical classiﬁcation, an unbiased geometric  estimator for this risk, and a new algorithmic reduction from hierarchical classiﬁ-  cation to cost-sensitive classiﬁcation.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">243</span></div><div id="author">Authors:<span id="author">Ian Stevenson, Konrad Koerding</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/f197002b9a0853eca5e046d9ca4663d5-Paper.pdf">Structural inference affects depth perception in the context of potential occlusion</a></div><div id="abs">Abstract: <span id="abs">In many domains, humans appear to combine perceptual cues in a near-optimal, probabilistic fashion: two noisy pieces of information tend to be combined linearly with weights proportional to the precision of each cue. Here we present a case where structural information plays an important role. The presence of a background cue gives rise to the possibility of occlusion, and places a soft constraint on the location of a target – in effect propelling it forward. We present an ideal observer model of depth estimation for this situation where structural or ordinal information is important and then fit the model to human data from a stereo-matching task. To test whether subjects are truly using ordinal cues in a probabilistic manner we then vary the uncertainty of the task. We find that the model accurately predicts shifts in subject’s behavior. Our results indicate that the nervous system estimates depth ordering in a probabilistic fashion and estimates the structure of the visual scene during depth perception.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">208</span></div><div id="author">Authors:<span id="author">Marius Kloft, Ulf Brefeld, Pavel Laskov, Klaus-Robert Müller, Alexander Zien, Sören Sonnenburg</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/d516b13671a4179d9b7b458a6ebdeb92-Paper.pdf">Efficient and Accurate Lp-Norm Multiple Kernel Learning</a></div><div id="abs">Abstract: <span id="abs">Learning linear combinations of multiple kernels is an appealing strategy when the right choice of features is unknown. Previous approaches to multiple kernel learning (MKL) promote sparse kernel combinations and hence support interpretability. Unfortunately, L1-norm MKL is hardly observed to outperform trivial baselines in practical applications. To allow for robust kernel mixtures, we generalize MKL to arbitrary Lp-norms. We devise new insights on the connection between  several existing MKL formulations and develop two efficient interleaved optimization strategies for arbitrary p&gt;1. Empirically, we demonstrate that the interleaved optimization strategies are much faster compared to the traditionally used wrapper approaches. Finally, we apply Lp-norm MKL to real-world problems from computational biology, showing that non-sparse MKL achieves accuracies that go beyond the state-of-the-art.</span></div>, we show that our algorithm finds extremely good approximate solutions for various kinds of MRFs with geometry.</p><p id="section"><div id="pid">Paperid:<span id="pid">252</span></div><div id="author">Authors:<span id="author">Wolf Vanpaemel</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/f7cade80b7cc92b991cf4d2806d6bd78-Paper.pdf">Measuring model complexity with the prior predictive</a></div><div id="abs">Abstract: <span id="abs">In the last few decades, model complexity has received a lot of press. While many methods have been proposed that jointly measure a model’s descriptive adequacy and its complexity, few measures exist that measure complexity in itself. Moreover, existing measures ignore the parameter prior, which is an inherent part of the model and affects the complexity. This paper presents a stand alone measure for model complexity, that takes the number of parameters, the functional form, the range of the parameters and the parameter prior into account. This Prior Predictive Complexity (PPC) is an intuitive and easy to compute measure. It starts from the observation that model complexity is the property of the model that enables it to fit a wide range of outcomes. The PPC then measures how wide this range exactly is.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">250</span></div><div id="author">Authors:<span id="author">Elad Hazan, Satyen Kale</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/f770b62bc8f42a0b66751fe636fc6eb0-Paper.pdf">Beyond Convexity: Online Submodular Minimization</a></div><div id="abs">Abstract: <span id="abs">We consider an online decision problem over a discrete space in which the loss function is submodular. We give algorithms which are computationally efficient and are Hannan-consistent in both the full information and bandit settings.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">212</span></div><div id="author">Authors:<span id="author">Ed Vul, George Alvarez, Joshua B. Tenenbaum, Michael J. Black</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/d79aac075930c83c2f1e369a511148fe-Paper.pdf">Explaining human multiple object tracking as resource-constrained approximate inference in a dynamic probabilistic model</a></div><div id="abs">Abstract: <span id="abs">Multiple object tracking is a task commonly used to investigate the architecture of human visual attention. Human participants show a distinctive pattern of successes and failures in tracking experiments that is often attributed to limits on an object system, a tracking module, or other specialized cognitive structures. Here we use a computational analysis of the task of object tracking to ask which human failures arise from cognitive limitations and which are consequences of inevitable perceptual uncertainty in the tracking task. We find that many human performance phenomena, measured through novel behavioral experiments, are naturally produced by the operation of our ideal observer model (a Rao-Blackwelized particle filter). The tradeoff between the speed and number of objects being tracked, however, can only arise from the allocation of a flexible cognitive resource, which can be formalized as either memory or attention.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">217</span></div><div id="author">Authors:<span id="author">Sahand Negahban, Bin Yu, Martin J. Wainwright, Pradeep K. Ravikumar</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/dc58e3a306451c9d670adcd37004f48f-Paper.pdf">A unified framework for high-dimensional analysis of $M$-estimators with decomposable regularizers</a></div><div id="abs">Abstract: <span id="abs">The estimation of high-dimensional parametric models requires imposing some structure on the models, for instance that they be sparse, or that matrix structured parameters have low rank. A general approach for such structured parametric model estimation is to use regularized M-estimation procedures, which regularize a loss function that measures goodness of fit of the parameters to the data with some regularization function that encourages the assumed structure. In this paper, we aim to provide a unified analysis of such regularized M-estimation procedures. In particular, we report the convergence rates of such estimators in any metric norm. Using just our main theorem, we are able to rederive some of the many existing results, but also obtain a wide range of novel convergence rates results. Our analysis also identifies key properties of loss and regularization functions such as restricted strong convexity, and decomposability, that ensure the corresponding regularized M-estimators have good convergence rates.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">220</span></div><div id="author">Authors:<span id="author">Matthew Streeter, Daniel Golovin, Andreas Krause</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/e0c641195b27425bb056ac56f8953d24-Paper.pdf">Online Learning of Assignments</a></div><div id="abs">Abstract: <span id="abs">Which ads should we display in sponsored search in order to maximize our revenue?  How should we dynamically rank information sources to maximize value of information?  These applications exhibit strong diminishing returns: Selection of redundant ads and information sources decreases their marginal utility.  We show that these and other problems can be formalized as repeatedly selecting an assignment of items to positions to maximize a sequence of monotone submodular functions that arrive one by one.  We present an efficient algorithm for this general problem and analyze it in the no-regret model.  Our algorithm is equipped with strong theoretical guarantees, with a performance ratio that converges to the optimal constant of 1-1/e.  We empirically evaluate our algorithms on two real-world online optimization problems on the web: ad allocation with submodular utilities, and dynamically ranking blogs to detect information cascades.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">213</span></div><div id="author">Authors:<span id="author">Jing Gao, Feng Liang, Wei Fan, Yizhou Sun, Jiawei Han</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/d7a728a67d909e714c0774e22cb806f2-Paper.pdf">Graph-based Consensus Maximization among Multiple Supervised and Unsupervised Models</a></div><div id="abs">Abstract: <span id="abs">Little work has been done to directly combine the outputs of multiple supervised and unsupervised models. However, it can increase the accuracy and applicability of ensemble methods. First, we can boost the diversity of classification ensemble by incorporating multiple clustering outputs, each of which provides grouping constraints for the joint label predictions of a set of related objects. Secondly, ensemble of supervised models is limited in applications which have no access to raw data but to the meta-level model outputs. In this paper, we aim at calculating a consolidated classification solution for a set of objects by maximizing the consensus among both supervised predictions and unsupervised grouping constraints. We seek a global optimal label assignment for the target objects, which is different from the result of traditional majority voting and model combination approaches. We cast the problem into an optimization problem on a bipartite graph, where the objective function favors smoothness in the conditional probability estimates over the graph, as well as penalizes deviation from initial labeling of supervised models. We solve the problem through iterative propagation of conditional probability estimates among neighboring nodes, and interpret the method as conducting a constrained embedding in a transformed space, as well as a ranking on the graph. Experimental results on three real applications demonstrate the benefits of the proposed method over existing alternatives.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">261</span></div><div id="author">Authors:<span id="author">Lawrence Cayton</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/fe131d7f5a6b38b23cc967316c13dae2-Paper.pdf">Efficient Bregman Range Search</a></div><div id="abs">Abstract: <span id="abs">We develop an algorithm for efficient range search when the notion of dissimilarity is given by a Bregman divergence.  The range search task is to return all points in a potentially large database that are within some specified distance of a query.  It arises in many learning algorithms such as locally-weighted regression, kernel density estimation, neighborhood graph-based algorithms, and in tasks like outlier detection and information retrieval.  In metric spaces, efficient range search-like algorithms based on spatial data structures have been deployed on a variety of statistical tasks.  Here we describe the first algorithm for range search for an arbitrary Bregman divergence.   This broad class of dissimilarity measures includes the relative entropy,  Mahalanobis distance, Itakura-Saito divergence, and a variety of matrix divergences.  Metric methods cannot be directly applied since Bregman divergences do not in general satisfy the triangle inequality.  We derive geometric properties of Bregman divergences that yield an efficient algorithm for range search based on a recently proposed space decomposition for Bregman divergences.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">215</span></div><div id="author">Authors:<span id="author">Saketha N. Jagarlapudi, Dinesh G, Raman S, Chiranjib Bhattacharyya, Aharon Ben-tal, Ramakrishnan K.r.</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/d86ea612dec96096c5e0fcc8dd42ab6d-Paper.pdf">On the Algorithmics and Applications of a Mixed-norm based Kernel Learning Formulation</a></div><div id="abs">Abstract: <span id="abs">Motivated from real world problems, like object categorization, we study a particular mixed-norm regularization for Multiple Kernel Learning (MKL). It is assumed that the given set of kernels are grouped into distinct components where each component is crucial for the learning task at hand. The formulation hence employs $l_\infty$ regularization for promoting combinations at the component level and $l_1$ regularization for promoting sparsity among kernels in each component. While previous attempts have formulated this as a non-convex problem, the formulation given here is an instance of non-smooth convex optimization problem which admits an efficient Mirror-Descent (MD) based procedure. The MD procedure optimizes over product of simplexes, which is not a well-studied case in literature. Results on real-world datasets show that the new MKL formulation is well-suited for object categorization tasks and that the MD based algorithm outperforms state-of-the-art MKL solvers like \texttt{simpleMKL} in terms of computational effort.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">262</span></div><div id="author">Authors:<span id="author">Martin Allen, Shlomo Zilberstein</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/fec8d47d412bcbeece3d9128ae855a7a-Paper.pdf">Complexity of Decentralized Control: Special Cases</a></div><div id="abs">Abstract: <span id="abs">The worst-case complexity of general decentralized POMDPs, which are equivalent to partially observable stochastic games (POSGs) is very high, both for the cooperative and competitive cases.  Some reductions in complexity have been achieved by exploiting independence relations in some models.  We show that these results are somewhat limited:  when these independence assumptions are relaxed in very small ways, complexity returns to that of the general case.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">158</span></div><div id="author">Authors:<span id="author">Ricardo Henao, Ole Winther</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/9b698eb3105bd82528f23d0c92dedfc0-Paper.pdf">Bayesian Sparse Factor Models and DAGs Inference and Comparison</a></div><div id="abs">Abstract: <span id="abs">In this paper we present a novel approach to learn directed acyclic graphs (DAG) and factor models within the same framework while also allowing for model comparison between them. For this purpose, we exploit the connection between factor models and DAGs to propose Bayesian hierarchies based on spike and slab priors to promote sparsity, heavy-tailed priors to ensure identifiability and predictive densities to perform the model comparison. We require identifiability to be able to produce variable orderings leading to valid DAGs and sparsity to learn the structures. The effectiveness of our approach is demonstrated through extensive experiments on artificial and biological data showing that our approach outperform a number of state of the art methods.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">257</span></div><div id="author">Authors:<span id="author">Bryan Russell, Alyosha Efros, Josef Sivic, Bill Freeman, Andrew Zisserman</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/fba9d88164f3e2d9109ee770223212a0-Paper.pdf">Segmenting Scenes by Matching Image Composites</a></div><div id="abs">Abstract: <span id="abs">In this paper, we investigate how similar images sharing the same global description can help with unsupervised scene segmentation in an image.  In contrast to recent work in semantic alignment of scenes, we allow an input image to be explained by partial matches of similar scenes.  This allows for a better explanation of the input scenes.  We perform MRF-based segmentation that optimizes over matches, while respecting boundary information.  The recovered segments are then used to re-query a large database of images to retrieve better matches for the target region. We show improved performance in detecting occluding boundaries over previous methods on data gathered from the LabelMe database.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">258</span></div><div id="author">Authors:<span id="author">Kian M. Chai</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/fbd7939d674997cdb4692d34de8633c4-Paper.pdf">Generalization Errors and Learning Curves for Regression with Multi-task Gaussian Processes</a></div><div id="abs">Abstract: <span id="abs">We provide some insights into how task correlations in multi-task Gaussian process (GP) regression affect the generalization error and the learning curve.  We analyze the asymmetric two-task case, where a secondary task is to help the learning of a primary task. Within this setting, we give bounds on the generalization error and the learning curve of the primary task. Our approach admits intuitive understandings of the multi-task GP by relating it to single-task GPs. For the case of one-dimensional input-space under optimal sampling with data only for the secondary task, the limitations of multi-task GP can be quantified explicitly.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">234</span></div><div id="author">Authors:<span id="author">Gal Chechik, Uri Shalit, Varun Sharma, Samy Bengio</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/e97ee2054defb209c35fe4dc94599061-Paper.pdf">An Online Algorithm for Large Scale Image Similarity Learning</a></div><div id="abs">Abstract: <span id="abs">Learning a measure of similarity between pairs of objects is a fundamental problem in machine learning. It stands in the core of classification methods like kernel machines, and is particularly useful for applications like searching for images that are similar to a given image or finding videos that are relevant to a given video. In these tasks, users look for objects that are not only visually similar but also semantically related to a given object. Unfortunately, current approaches for learning similarity may not scale to large datasets with high dimensionality, especially when imposing metric constraints on the learned similarity. We describe OASIS, a method for learning pairwise similarity that is fast and scales linearly with the number of objects and the number of non-zero features. Scalability is achieved through online learning of a bilinear model over sparse representations using a large margin criterion and an efficient hinge loss cost. OASIS is accurate at a wide range of scales: on a standard benchmark with thousands of images, it is more precise than state-of-the-art methods, and faster by orders of magnitude. On 2 million images collected from the web, OASIS can be trained within 3 days on a single CPU. The non-metric similarities learned by OASIS can be transformed into metric similarities, achieving higher precisions than similarities that are learned as metrics in the first place. This suggests an approach for learning a metric from data that is larger by an order of magnitude than was handled before.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">242</span></div><div id="author">Authors:<span id="author">Richard Socher, Samuel Gershman, Per Sederberg, Kenneth Norman, Adler J. Perotte, David M. Blei</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/efe937780e95574250dabe07151bdc23-Paper.pdf">A Bayesian Analysis of Dynamics in Free Recall</a></div><div id="abs">Abstract: <span id="abs">We develop a probabilistic model of human memory performance in free recall experiments. In these experiments, a subject first studies a list of words and then tries to recall them.  To model these data, we draw on both previous psychological research and statistical topic models of text documents.  We assume that memories are formed by assimilating the semantic meaning of studied words (represented as a distribution over topics) into a slowly changing latent context (represented in the same space).  During recall, this context is reinstated and used as a cue for retrieving studied words.  By conceptualizing memory retrieval as a dynamic latent variable model, we are able to use Bayesian inference to represent uncertainty and reason about the cognitive processes underlying memory.  We present a particle filter algorithm for performing approximate posterior inference, and evaluate our model on the prediction of recalled words in experimental data. By specifying the model hierarchically, we are also able to capture inter-subject variability.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">225</span></div><div id="author">Authors:<span id="author">Mario Fritz, Gary Bradski, Sergey Karayev, Trevor Darrell, Michael J. Black</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/e46de7e1bcaaced9a54f1e9d0d2f800d-Paper.pdf">An Additive Latent Feature Model for Transparent Object Recognition</a></div><div id="abs">Abstract: <span id="abs">Existing methods for recognition of object instances and categories based on quantized local features can perform poorly when local features exist on transparent surfaces, such as glass or plastic objects. There are characteristic patterns to the local appearance of transparent objects, but they may not be well captured by distances to individual examples or by a local pattern codebook obtained by vector quantization.  The appearance of a transparent patch is determined in part by the refraction of a background pattern through a transparent medium: the energy from the background usually dominates the patch appearance.  We model transparent local patch appearance using an additive  model of latent factors: background factors due to scene content,  and factors which capture a local edge energy distribution characteristic of the refraction.  We implement our method using a novel LDA-SIFT formulation which performs LDA prior to any vector quantization step; we discover latent topics which are characteristic of particular transparent patches and quantize the SIFT space into transparent visual words according to the latent topic dimensions. No knowledge of the background scene is required at test time; we show examples recognizing transparent glasses in a domestic environment.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">20</span></div><div id="author">Authors:<span id="author">Anne Hsu, Thomas L. Griffiths</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/0f96613235062963ccde717b18f97592-Paper.pdf">Differential Use of Implicit Negative Evidence in Generative and Discriminative Language Learning</a></div><div id="abs">Abstract: <span id="abs">A classic debate in cognitive science revolves around understanding how children learn complex linguistic rules, such as those governing  restrictions on verb alternations, without negative  evidence. Traditionally, formal learnability arguments have been used to claim that such learning is impossible without the aid of innate  language-specific knowledge. However, recently, researchers have shown  that statistical models are capable of learning complex rules from only  positive evidence. These two kinds of learnability analyses differ in their assumptions about the role of the distribution from which linguistic  input is generated.  The former analyses assume that learners seek to identify grammatical sentences in a way that is robust to the distribution  from which the sentences are generated, analogous to discriminative  approaches in machine learning. The latter assume that learners are trying  to estimate a generative model, with sentences being sampled from that  model. We show that these two learning approaches differ in their use of implicit negative evidence -- the absence of a sentence -- when learning  verb alternations, and demonstrate that human learners can produce results  consistent with the predictions of both approaches, depending on the  context in which the learning problem is presented.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">253</span></div><div id="author">Authors:<span id="author">Long Zhu, Yuanahao Chen, Bill Freeman, Antonio Torralba</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/f7e6c85504ce6e82442c770f7c8606f0-Paper.pdf">Nonparametric Bayesian Texture Learning and Synthesis</a></div><div id="abs">Abstract: <span id="abs">We present a nonparametric Bayesian method for texture learning and synthesis.  A texture image is represented by a 2D-Hidden Markov Model (2D-HMM) where the hidden states correspond to the cluster labeling of textons and the transition matrix encodes their spatial layout (the compatibility between adjacent textons). 2D-HMM is coupled with the Hierarchical Dirichlet process (HDP) which allows the number of textons and the complexity of transition matrix grow as the input texture becomes irregular. The HDP makes use of Dirichlet process prior which favors regular textures by penalizing the model complexity. This framework (HDP-2D-HMM) learns the texton vocabulary and their spatial layout jointly and automatically.  The HDP-2D-HMM results in a compact  representation of textures which allows fast texture synthesis with comparable rendering quality over the state-of-the-art image-based rendering methods. We also show that HDP-2D-HMM can be applied to perform image segmentation and synthesis.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">239</span></div><div id="author">Authors:<span id="author">Feng Yan, Ningyi Xu, Yuan Qi</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/ed265bc903a5a097f61d3ec064d96d2e-Paper.pdf">Parallel Inference for Latent Dirichlet Allocation on Graphics Processing Units</a></div><div id="abs">Abstract: <span id="abs">The recent emergence of Graphics Processing Units (GPUs) as general-purpose parallel computing devices provides us with new opportunities to develop scalable learning methods for massive data. In this work, we consider the problem of parallelizing two inference methods on GPUs for latent Dirichlet Allocation (LDA) models, collapsed Gibbs sampling (CGS) and collapsed variational Bayesian (CVB). To address limited memory constraints on GPUs, we propose a novel data partitioning scheme that effectively reduces the memory cost. Furthermore, the partitioning scheme balances the computational cost on each multiprocessor and enables us to easily avoid memory access conflicts. We also use data streaming to handle extremely large datasets. Extensive experiments showed that our parallel inference methods consistently produced LDA models with the same predictive power as sequential training methods did but with 26x speedup for CGS and 196x speedup for CVB on a GPU with 30 multiprocessors; actually the speedup is almost linearly scalable with the number of multiprocessors available. The proposed partitioning scheme and data streaming can be easily ported to many other models in machine learning.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">226</span></div><div id="author">Authors:<span id="author">Jean-pascal Pfister, Peter Dayan, Máté Lengyel</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/e4bb4c5173c2ce17fd8fcd40041c068f-Paper.pdf">Know Thy Neighbour: A Normative Theory of Synaptic Depression</a></div><div id="abs">Abstract: <span id="abs">Synapses exhibit an extraordinary degree of short-term malleability, with release probabilities and effective synaptic strengths changing markedly over multiple timescales. From the perspective of a fixed computational operation in a network, this seems like a most unacceptable degree of added noise. We suggest an alternative theory according to which short term synaptic plasticity plays a normatively-justifiable role. This theory starts from the commonplace observation that the spiking of a neuron is an incomplete, digital, report of the analog quantity that contains all the critical information, namely its membrane potential. We suggest that one key task for a synapse is to solve the inverse problem of estimating the pre-synaptic membrane potential from the spikes it receives and prior  expectations, as in a recursive filter. We show that short-term synaptic depression has canonical dynamics which closely resemble those required for optimal estimation, and that it indeed supports high quality estimation. Under this account, the local postsynaptic potential and the level of synaptic resources track the (scaled) mean and variance of the estimated presynaptic membrane potential. We make  experimentally testable predictions for how the statistics of subthreshold membrane potential fluctuations and the form of spiking non-linearity should be related to the properties of short-term plasticity in any particular cell type.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">241</span></div><div id="author">Authors:<span id="author">Sebastian Gerwinn, Leonard White, Matthias Kaschube, Matthias Bethge, Jakob H. Macke</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/ef0d3930a7b6c95bd2b32ed45989c61f-Paper.pdf">Bayesian estimation of orientation preference maps</a></div><div id="abs">Abstract: <span id="abs">Imaging techniques such as optical imaging of intrinsic signals, 2-photon calcium imaging and voltage sensitive dye imaging can be used to measure the functional organization of visual cortex across different spatial scales. Here, we present Bayesian methods based on Gaussian processes for extracting topographic maps from functional imaging data.  In particular, we focus on the estimation of orientation preference maps (OPMs) from intrinsic signal imaging data.  We model the underlying map as a bivariate Gaussian process, with a prior covariance function that reflects known properties of OPMs, and a noise covariance adjusted to the data.  The posterior mean can be interpreted as an optimally smoothed estimate of the map, and can be used for model based interpolations of the map from sparse measurements.  By sampling from the posterior distribution, we can get error bars on statistical properties such as preferred orientations,  pinwheel locations or -counts.  Finally, the use of an explicit probabilistic model facilitates interpretation of parameters and provides the basis for decoding studies.  We demonstrate our model both on simulated data and on intrinsic signaling data from ferret visual cortex.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">238</span></div><div id="author">Authors:<span id="author">Marek Petrik, Shlomo Zilberstein</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/ec5decca5ed3d6b8079e2e7e7bacc9f2-Paper.pdf">Robust Value Function Approximation Using Bilinear Programming</a></div><div id="abs">Abstract: <span id="abs">Existing value function approximation methods have been successfully used in many applications, but they often lack useful a priori error bounds. We propose approximate bilinear programming, a new formulation of value function approximation that provides strong a priori guarantees. In particular, it provably finds an approximate value function that minimizes the Bellman residual. Solving a bilinear program optimally is NP hard, but this is unavoidable because the Bellman-residual minimization itself is NP hard. We, therefore, employ and analyze a common approximate algorithm for bilinear programs. The analysis shows that this algorithm offers a convergent generalization of approximate policy iteration. Finally, we demonstrate that the proposed approach can consistently minimize the Bellman residual on a simple benchmark problem.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">63</span></div><div id="author">Authors:<span id="author">Yiming Ying, Kaizhu Huang, Colin Campbell</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/37bc2f75bf1bcfe8450a1a41c200364c-Paper.pdf">Sparse Metric Learning via Smooth Optimization</a></div><div id="abs">Abstract: <span id="abs">In this paper we study the problem of learning a low-dimensional (sparse)  distance matrix.  We propose a novel metric learning model which can simultaneously conduct dimension reduction and learn a distance matrix. The sparse representation involves a mixed-norm regularization which is  non-convex.  We then show that it can be equivalently formulated  as a convex saddle (min-max) problem. From this saddle representation, we develop an efficient smooth optimization approach for sparse metric learning although the learning model is based on a non-differential loss function. This smooth optimization approach has an optimal convergence rate of $O(1 /\ell^2)$ for smooth problems where $\ell$ is the iteration number. Finally, we run experiments to validate the effectiveness and efficiency of our sparse metric learning model on various datasets.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">219</span></div><div id="author">Authors:<span id="author">Parikshit Ram, Dongryeol Lee, Hua Ouyang, Alexander G. Gray</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/ddb30680a691d157187ee1cf9e896d03-Paper.pdf">Rank-Approximate Nearest Neighbor Search: Retaining Meaning and Speed in High Dimensions</a></div><div id="abs">Abstract: <span id="abs">The long-standing problem of efficient nearest-neighbor (NN) search has ubiquitous applications ranging from astrophysics to MP3 fingerprinting to bioinformatics to movie recommendations.  As the dimensionality of the dataset increases, exact NN search becomes computationally prohibitive; (1+eps)-distance-approximate NN search can provide large speedups but risks losing the meaning of NN search present in the ranks (ordering) of the distances. This paper presents a simple, practical algorithm allowing the user to, for the first time, directly control the true accuracy of NN search (in terms of ranks) while still achieving the large speedups over exact NN.  Experiments with high-dimensional datasets show that it often achieves faster and more accurate results than the best-known distance-approximate method, with much more stable behavior.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">16</span></div><div id="author">Authors:<span id="author">Yi-hao Kao, Benjamin V. Roy, Xiang Yan</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Paper.pdf">Directed Regression</a></div><div id="abs">Abstract: <span id="abs">When used to guide decisions, linear regression analysis typically involves estimation of regression coefficients via ordinary least squares and their subsequent use to make decisions. When there are multiple response variables and features do not perfectly capture their relationships, it is beneficial to account for the decision objective when computing regression coefficients. Empirical optimization does so but sacrifices performance when features are well-chosen or training data are insufficient. We propose directed regression, an efficient algorithm that combines merits of ordinary least squares and empirical optimization. We demonstrate through a computational study that directed regression can generate significant performance gains over either alternative. We also develop a theory that motivates the algorithm.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">200</span></div><div id="author">Authors:<span id="author">Christos Boutsidis, Petros Drineas, Michael W. Mahoney</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/c51ce410c124a10e0db5e4b97fc2af39-Paper.pdf">Unsupervised Feature Selection for the $k$-means Clustering Problem</a></div><div id="abs">Abstract: <span id="abs">We present a novel feature selection algorithm for the $k$-means clustering problem. Our algorithm is randomized and, assuming an accuracy parameter $\epsilon \in (0,1)$, selects and appropriately rescales in an unsupervised manner $\Theta(k \log(k / \epsilon) / \epsilon^2)$ features from a dataset of arbitrary dimensions. We prove that, if we run any $\gamma$-approximate $k$-means algorithm ($\gamma \geq 1$) on the features selected using our method, we can find a $(1+(1+\epsilon)\gamma)$-approximate partition with high probability.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">187</span></div><div id="author">Authors:<span id="author">Martin Zinkevich, John Langford, Alex J. Smola</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/b55ec28c52d5f6205684a473a2193564-Paper.pdf">Slow Learners are Fast</a></div><div id="abs">Abstract: <span id="abs">Online learning algorithms have impressive convergence properties   when it comes to risk minimization and convex games on very large   problems. However, they are inherently sequential in their design   which prevents them from taking advantage of modern multi-core   architectures. In this paper we prove that online learning with   delayed updates converges well, thereby facilitating parallel online   learning.</span></div>nt, but does not scale well. BoostMetric is instead based on a key observation that any positive semidefinite matrix can be decomposed into a linear positive combination of trace-one rank-one matrices.  BoostMetric thus uses rank-one positive semidefinite matrices as weak learners within an efficient and scalable boosting-based learning process. The resulting method is easy to implement, does not require tuning, and can accommodate various types of constraints.  Experiments on various datasets show that the proposed algorithm compares favorably to those state-of-the-art methods in terms of classification accuracy and running time.</p><p id="section"><div id="pid">Paperid:<span id="pid">197</span></div><div id="author">Authors:<span id="author">Wenming Zheng, Zhouchen Lin</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/c16a5320fa475530d9583c34fd356ef5-Paper.pdf">Optimizing Multi-Class Spatio-Spectral Filters via Bayes Error Estimation for EEG Classification</a></div><div id="abs">Abstract: <span id="abs">The method of common spatio-spectral patterns (CSSPs) is an extension of common spatial patterns (CSPs) by utilizing the technique of delay embedding to alleviate the adverse effects of noises and artifacts on the electroencephalogram (EEG) classification. Although the CSSPs method has shown to be more powerful than the CSPs method in the EEG classification, this method is only suitable for two-class EEG classification problems. In this paper, we generalize the two-class CSSPs method to multi-class cases. To this end, we first develop a novel theory of multi-class Bayes error estimation and then present the multi-class CSSPs (MCSSPs) method based on this Bayes error theoretical framework. By minimizing the estimated closed-form Bayes error, we obtain the optimal spatio-spectral filters of MCSSPs. To demonstrate the effectiveness of the proposed method, we conduct extensive experiments on the data set of BCI competition 2005. The experimental results show that our method significantly outperforms the previous multi-class CSPs (MCSPs) methods in the EEG classification.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">202</span></div><div id="author">Authors:<span id="author">Chong Wang, David M. Blei</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/ca46c1b9512a7a8315fa3c5a946e8265-Paper.pdf">Variational Inference for the Nested Chinese Restaurant Process</a></div><div id="abs">Abstract: <span id="abs">The nested Chinese restaurant process (nCRP) is a powerful nonparametric Bayesian model for learning tree-based hierarchies from data. Since its posterior distribution is intractable, current inference methods have all relied on MCMC sampling. In this paper, we develop an alternative inference technique based on variational methods. To employ variational methods, we derive a tree-based stick-breaking construction of the nCRP mixture model, and a novel variational algorithm that efficiently explores a posterior over a large set of combinatorial structures. We demonstrate the use of this approach for text and hand written digits modeling, where we show we can adapt the nCRP to continuous data as well.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">146</span></div><div id="author">Authors:<span id="author">Andrew Guillory, Jeff A. Bilmes</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/90794e3b050f815354e3e29e977a88ab-Paper.pdf">Label Selection on Graphs</a></div><div id="abs">Abstract: <span id="abs">We investigate methods for selecting sets of labeled vertices for use in predicting the labels of vertices on a graph.  We specifically study methods which choose a single batch of labeled vertices (i.e. offline, non sequential methods).  In this setting, we find common graph smoothness assumptions directly motivate simple label selection methods with interesting theoretical guarantees. These methods bound prediction error in terms of the smoothness of the true labels with respect to the graph.  Some of these bounds give new motivations for previously proposed algorithms, and some suggest new algorithms which we evaluate.  We show improved performance over baseline methods on several real world data sets.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">61</span></div><div id="author">Authors:<span id="author">Mikkel Schmidt</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/371bce7dc83817b7893bcdeed13799b5-Paper.pdf">Linearly constrained Bayesian matrix factorization for blind source separation</a></div><div id="abs">Abstract: <span id="abs">We present a general Bayesian approach to probabilistic matrix factorization subject to linear constraints. The approach is based on a Gaussian observation model and Gaussian priors with bilinear equality and inequality constraints. We present an efficient Markov chain Monte Carlo inference procedure based on Gibbs sampling. Special cases of the proposed model are Bayesian formulations of non-negative matrix factorization and factor analysis.  The method is evaluated on a blind source separation problem. We demonstrate that our algorithm can be used to extract meaningful and interpretable features that are remarkably different from features extracted using existing related matrix factorization techniques.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">181</span></div><div id="author">Authors:<span id="author">Chunxiao Zhou, Huixia J. Wang, Yongmei M. Wang</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/afd4836712c5e77550897e25711e1d96-Paper.pdf">Efficient Moments-based Permutation Tests</a></div><div id="abs">Abstract: <span id="abs">In this paper, we develop an efficient moments-based permutation test approach to improve the system’s efficiency by approximating the permutation distribution of the test statistic with Pearson distribution series. This approach involves the calculation of the first four moments of the permutation distribution. We propose a novel recursive method to derive these moments theoretically and analytically without any permutation.  Experimental results using different test statistics are demonstrated using simulated data and real data. The proposed strategy takes advantage of nonparametric permutation tests and parametric Pearson distribution approximation to achieve both accuracy and efficiency.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">246</span></div><div id="author">Authors:<span id="author">Jonathan W. Pillow</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/f3f27a324736617f20abbf2ffd806f6d-Paper.pdf">Time-rescaling methods for the estimation and assessment of non-Poisson neural encoding models</a></div><div id="abs">Abstract: <span id="abs">Recent work on the statistical modeling of neural responses has focused on modulated renewal processes in which the spike rate is a function of the stimulus and recent spiking history. Typically, these models incorporate spike-history dependencies via either: (A) a conditionally-Poisson process with rate dependent on a linear projection of the spike train history (e.g., generalized linear model); or (B) a modulated non-Poisson renewal process (e.g., inhomogeneous gamma process). Here we show that the two approaches can be combined, resulting in a {\it conditional renewal} (CR) model for neural spike trains. This model captures both real and rescaled-time effects, and can be fit by maximum likelihood using a simple application of the time-rescaling theorem [1]. We show that for any modulated renewal process model, the log-likelihood is concave in the linear filter parameters only under certain restrictive conditions on the renewal density (ruling out many popular choices, e.g. gamma with $\kappa \neq1$), suggesting that real-time history effects are easier to estimate than non-Poisson renewal properties. Moreover, we show that goodness-of-fit tests based on the time-rescaling theorem [1] quantify relative-time effects, but do not reliably assess accuracy in spike prediction or stimulus-response modeling. We illustrate the CR model with applications to both real and simulated neural data.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">214</span></div><div id="author">Authors:<span id="author">Ryan Mcdonald, Mehryar Mohri, Nathan Silberman, Dan Walker, Gideon S. Mann</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/d81f9c1be2e08964bf9f24b15f0e4900-Paper.pdf">Efficient Large-Scale Distributed Training of Conditional Maximum Entropy Models</a></div><div id="abs">Abstract: <span id="abs">Training conditional maximum entropy models on massive data requires significant time and computational resources. In this paper, we investigate three common distributed training strategies: distributed gradient, majority voting ensembles, and parameter mixtures. We analyze the worst-case runtime and resource costs of each and present a theoretical foundation for the convergence of parameters under parameter mixtures, the most efficient strategy. We present large-scale experiments comparing the different strategies and demonstrate that parameter mixtures over independent models use fewer resources and achieve comparable loss as compared to standard approaches.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">247</span></div><div id="author">Authors:<span id="author">Kwang I. Kim, Florian Steinke, Matthias Hein</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/f4552671f8909587cf485ea990207f3b-Paper.pdf">Semi-supervised Regression using Hessian energy with an application to semi-supervised dimensionality reduction</a></div><div id="abs">Abstract: <span id="abs">Semi-supervised regression based on the graph Laplacian suffers from the fact that the solution is biased towards a constant and the lack of extrapolating power. Outgoing from these observations we propose to use the second-order Hessian energy for semi-supervised regression which overcomes both of these problems, in particular, if the data lies on or close to a low-dimensional submanifold in the feature space, the Hessian energy prefers functions which vary ``linearly with respect to the natural parameters in the data. This property makes it also particularly suited for the task of semi-supervised dimensionality reduction where the goal is to find the natural parameters in the data based on a few labeled points. The experimental result suggest that our method is superior to semi-supervised regression using Laplacian regularization and standard supervised methods and is particularly suited for semi-supervised dimensionality reduction.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">180</span></div><div id="author">Authors:<span id="author">Tetsuro Morimura, Eiji Uchibe, Junichiro Yoshimoto, Kenji Doya</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/acf4b89d3d503d8252c9c4ba75ddbf6d-Paper.pdf">A Generalized Natural Actor-Critic Algorithm</a></div><div id="abs">Abstract: <span id="abs">Policy gradient Reinforcement Learning (RL) algorithms have received much attention in seeking stochastic policies that maximize the average rewards.  In addition, extensions based on the concept of the Natural Gradient (NG) show promising learning efficiency because these regard metrics for the task. Though there are two candidate metrics, Kakades Fisher Information Matrix (FIM) and Morimuras FIM, all RL algorithms with NG have followed the Kakades approach. In this paper, we describe a generalized Natural Gradient (gNG) by linearly interpolating the two FIMs and propose an efficient implementation for the gNG learning based on a theory of the estimating function, generalized Natural Actor-Critic (gNAC). The gNAC algorithm involves a near optimal auxiliary function to reduce the variance of the gNG estimates. Interestingly, the gNAC can be regarded as a natural extension of the current state-of-the-art NAC algorithm, as long as the interpolating parameter is appropriately selected. Numerical experiments showed that the proposed gNAC algorithm can estimate gNG efficiently and outperformed the NAC algorithm.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">233</span></div><div id="author">Authors:<span id="author">Jian Peng, Liefeng Bo, Jinbo Xu</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/e820a45f1dfc7b95282d10b6087e11c0-Paper.pdf">Conditional Neural Fields</a></div><div id="abs">Abstract: <span id="abs">Conditional random fields (CRF) are quite successful on sequence labeling tasks such as natural language processing and biological sequence analysis. CRF models use linear potential functions to represent the relationship between input features and outputs. However, in many real-world applications such as protein structure prediction and handwriting recognition, the relationship between input features and outputs is highly complex and nonlinear, which cannot be accurately modeled by a linear function. To model the nonlinear relationship between input features and outputs we propose Conditional Neural Fields (CNF), a new conditional probabilistic graphical model for sequence labeling. Our CNF model extends CRF by adding one (or possibly several) middle layer between input features and outputs. The middle layer consists of a number of hidden parameterized gates, each acting as a local neural network node or feature extractor to capture the nonlinear relationship between input features and outputs. Therefore, conceptually this CNF model is much more expressive than the linear CRF model. To better control the complexity of the CNF model, we also present a hyperparameter optimization procedure within the evidence framework. Experiments on two widely-used benchmarks indicate that this CNF model performs significantly better than a number of popular methods. In particular, our CNF model is the best among about ten machine learning methods for protein secondary tructure prediction and also among a few of the best methods for handwriting recognition.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">251</span></div><div id="author">Authors:<span id="author">Massih R. Amini, Nicolas Usunier, Cyril Goutte</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/f79921bbae40a577928b76d2fc3edc2a-Paper.pdf">Learning from Multiple Partially Observed Views - an Application to Multilingual Text Categorization</a></div><div id="abs">Abstract: <span id="abs">We address the problem of learning classifiers when observations have multiple views, some of which may not be observed for all examples.  We assume the existence of view generating functions which may complete the missing views in an approximate way.  This situation corresponds for example to learning text classifiers from multilingual collections where documents are not available in all languages.  In that case, Machine Translation (MT) systems may be used to translate each document in the missing languages.  We derive a generalization error bound for classifiers learned on examples with multiple artificially created views. Our result uncovers a trade-off between the size of the training set, the number of views, and the quality of the view generating functions. As a consequence, we identify situations where it is more interesting to use multiple views for learning instead of classical single view learning.  An extension of this framework is a natural way to leverage unlabeled multi-view data in semi-supervised learning.  Experimental results on a subset of the Reuters RCV1/RCV2 collections support our findings by showing that additional views obtained from MT may significantly improve the classification performance in the cases identified by our trade-off.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">248</span></div><div id="author">Authors:<span id="author">Laura Dietz, Valentin Dallmeier, Andreas Zeller, Tobias Scheffer</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/f64eac11f2cd8f0efa196f8ad173178e-Paper.pdf">Localizing Bugs in Program Executions with Graphical Models</a></div><div id="abs">Abstract: <span id="abs">We devise a graphical model that supports the process of debugging software by guiding developers to code that is likely to contain defects. The model is trained using execution traces of passing test runs; it reflects the distribution over transitional patterns of code positions. Given a failing test case, the model determines the least likely transitional pattern in the execution trace. The model is designed such that Bayesian inference has a closed-form solution. We evaluate the  Bernoulli graph model on data of the software projects AspectJ and Rhino.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">23</span></div><div id="author">Authors:<span id="author">Varun Kanade, Adam Kalai</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/13f9896df61279c928f19721878fac41-Paper.pdf">Potential-Based Agnostic Boosting</a></div><div id="abs">Abstract: <span id="abs">We prove strong noise-tolerance properties of a potential-based boosting algorithm, similar to MadaBoost (Domingo and Watanabe, 2000) and SmoothBoost (Servedio, 2003). Our analysis is in the agnostic framework of Kearns, Schapire and Sellie (1994), giving polynomial-time guarantees in presence of arbitrary noise. A remarkable feature of our algorithm is that it can be implemented without reweighting examples, by randomly relabeling them instead. Our boosting theorem gives, as easy corollaries, alternative derivations of two recent non-trivial results in computational learning theory: agnostically learning decision trees (Gopalan et al, 2008) and agnostically learning halfspaces (Kalai et al, 2005). Experiments suggest that the algorithm performs similarly to Madaboost.</span></div></p><p id="section"><div id="pid">Paperid:<span id="pid">223</span></div><div id="author">Authors:<span id="author">Khashayar Rohanimanesh, Sameer Singh, Andrew McCallum, Michael J. Black</span></div><div id="title">Title: <a href="https://proceedings.neurips.cc/paper_files/paper/2009/file/e205ee2a5de471a70c1fd1b46033a75f-Paper.pdf">Training Factor Graphs with Reinforcement Learning for Efficient MAP Inference</a></div><div id="abs">Abstract: <span id="abs">Large, relational factor graphs with structure defined by   first-order logic or other languages give rise to notoriously   difficult inference problems.  Because unrolling the structure   necessary to represent distributions over all hypotheses has   exponential blow-up, solutions are often derived from MCMC. However,   because of limitations in the design and parameterization of the   jump function, these sampling-based methods suffer from local   minima|the system must transition through lower-scoring   configurations before arriving at a better MAP solution. This paper   presents a new method of explicitly selecting fruitful downward   jumps by leveraging reinforcement learning (RL). Rather than setting   parameters to maximize the likelihood of the training data,   parameters of the factor graph are treated as a log-linear function   approximator and learned with temporal difference (TD); MAP   inference is performed by executing the resulting policy on held out   test data. Our method allows efficient gradient updates since only   factors in the neighborhood of variables affected by an action need   to be computed|we bypass the need to compute marginals entirely.   Our method provides dramatic empirical success, producing new   state-of-the-art results on a complex joint model of ontology   alignment, with a 48\% reduction in error over state-of-the-art in   that domain.</span></div></p></body></html>